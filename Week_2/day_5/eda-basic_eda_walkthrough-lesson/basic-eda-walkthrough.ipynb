{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Walkthrough of Standard EDA Procedure\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "This lesson uses a boston housing market dataset to walk through a basic exploratory data analysis procedure, starting from the very beginning with loading the data. \n",
    "\n",
    "Though in many if not most cases the EDA procedure will be considerably more involved, this should give you an idea of the basic workflow a data scientist would go through when taking a look at a new dataset.\n",
    "\n",
    "Note: this lesson is strictly exploratory. We will not be formulating any hypotheses about the data or testing them. In many cases you may have formulated a hypothesis before even looking at your data, which could considerably affect your focus and choices in what to investigate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Description of the Boston Housing Data](#data_description)\n",
    "- [Loading the data](#load_data)\n",
    "- [Drop unwanted columns](#drop)\n",
    "- [Clean corrupted data](#clean)\n",
    "- [Count null values and drop rows](#drop_nulls)\n",
    "- [Rename columns](#rename)\n",
    "- [Investigate potential outliers with boxplots](#boxplots)\n",
    "- [Plot all variables together](#plot_all)\n",
    "- [Standardization of variables](#standardization)\n",
    "- [Plot the standardized variables together](#plot_all_rescaled)\n",
    "- [Look at the covariance or correlation between variables](#cov_cor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_description'></a>\n",
    "\n",
    "### Description of the Boston Housing Data columns\n",
    "\n",
    "---\n",
    "\n",
    "The columns of the dataset are coded. The corresponding descriptions are:\n",
    "\n",
    "    CRIM: per capita crime rate by town \n",
    "    ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "    INDUS: proportion of non-retail business acres per town \n",
    "    CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "    NOX: nitric oxides concentration (parts per 10 million) \n",
    "    RM: average number of rooms per dwelling \n",
    "    AGE: proportion of owner-occupied units built prior to 1940 \n",
    "    DIS: weighted distances to five Boston employment centres \n",
    "    RAD: index of accessibility to radial highways \n",
    "    TAX: full-value property-tax rate per 10000 dollars\n",
    "    PTRATIO: pupil-teacher ratio by town \n",
    "    B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "    LSTAT: % lower status of the population \n",
    "    MEDV: Median value of owner-occupied homes in 1000's of dollars\n",
    "    \n",
    "Each row in the dataset represents a different suburb of Boston.\n",
    "\n",
    "These descriptions of shortened or coded variables are often called \"codebooks\" or data dictionaries. They are typically found with datasets you might find online in a separate file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_data'></a>\n",
    "\n",
    "### 1. Load the data\n",
    "\n",
    "---\n",
    "\n",
    "Import the csv into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_file = 'datasets/housing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4,09</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4,9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4,9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6,0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6,0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS RAD  \\\n",
       "0           0  0.00632  18.0   2.31     0  0.538  6.575  65.2    4,09   1   \n",
       "1           1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4,9671   2   \n",
       "2           2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4,9671   2   \n",
       "3           3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6,0622   3   \n",
       "4           4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6,0622   3   \n",
       "\n",
       "   TAX  PTRATIO       B  LSTAT  MEDV  \n",
       "0  296     15.3  396.90   4.98  24.0  \n",
       "1  242     17.8  396.90   9.14  21.6  \n",
       "2  242     17.8  392.83   4.03  34.7  \n",
       "3  222     18.7  394.63   2.94  33.4  \n",
       "4  222     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "boston_df = pd.read_csv(boston_file)\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='header'></a>\n",
    "\n",
    "### 2. Describe the basic format of the data and the columns\n",
    "\n",
    "---\n",
    "\n",
    "Use the `.head()` function (and optionally pass in an integer for the number of rows you want to see) to examine what the loaded data looks like. This is a good initial step to get a feel for what is in the csv and what problems may be present.\n",
    "\n",
    "The `.dtypes` attribute tells you the data type for each of your columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first 8 rows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dtypes of the columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop'></a>\n",
    "\n",
    "### 2. Drop unwanted columns\n",
    "\n",
    "---\n",
    "\n",
    "There is a column labeled `Unnamed: 0` which appears to simply number the rows. We already have the number id of the rows in the DataFrame's index and so we don't need this column.\n",
    "\n",
    "The `.drop()` built-in function can be used to get rid of a column. When removing a column, we need to specify `axis=1` to the function.\n",
    "\n",
    "For the record, the `.index` attribute holds the row indices. This is the the sister attribute to the `.columns` attribute that we work with more often.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the index object and the first 20 items in the DataFrame's index \n",
    "# to see that we have these row numbers already:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the unneccesary column:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "\n",
    "### 3. Clean corrupted columns\n",
    "\n",
    "---\n",
    "\n",
    "You may have noticed when we examined the `dtypes` attribute that two of the columns were of type \"object\", indicating that they were string. However, we know from the data description above (and we can infer from the header of the data) that `DIS` and `RAD` should in fact be numeric.\n",
    "\n",
    "It is pretty common to have numeric columns represented as strings in your data if some of the observations are corrupted. It is important to always check the data types of your columns.\n",
    "\n",
    "**3.A What is causing the `DIS` column to be encoded as a string? Figure out a way to make sure the column is numeric while preserving information.**\n",
    "\n",
    "*Tip: The `.map()` built-in function on a column will apply a function to each element of the column.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.B What is causing the `RAD` column to be encoded as string? Figure out a way to make sure the column is numeric while preserving information.**\n",
    "\n",
    "*Tip: You can put `np.nan` values in place of corrupted observations, which are numeric \"Null\" values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop_nulls'></a>\n",
    "\n",
    "### 4. Determine how many observations are missing\n",
    "\n",
    "---\n",
    "\n",
    "Having replaced the question marks with `np.nan` values, we know that there are some missing observations for the `RAD` column. \n",
    "\n",
    "When we start to build models with data, null values in observations are (almost) never allowed. It is important to always see how many observations are missing and for which columns.\n",
    "\n",
    "A handy way to look at how many null values there are per column with pandas is:\n",
    "\n",
    "```python\n",
    "boston.isnull().sum()\n",
    "```\n",
    "\n",
    "The `.isull()` built-in function will convert the columns to boolean `True` and `False` values (returning a new dataframe) where null values are indicated by `True`. \n",
    "\n",
    "The `.sum()` function tacked on to the back of that will then sum these boolean columns, and the total number of null values per column will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the null values.** \n",
    "\n",
    "In this case, lets keep it simple and just drop the rows from the dataset that contain null values. If a column has a ton of null values it often makes more sense to drop the column entirely instead of the rows with null values. In this case, we will just drop the rows.\n",
    "\n",
    "The `.dropna()` function will drop any rows that have _**ANY**_ null values for you.  Use this carefully as you could drop many more rows than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rename'></a>\n",
    "\n",
    "### 5. Make the column names more descriptive\n",
    "\n",
    "---\n",
    "\n",
    "Often it is annoying to have to memorize what the codes mean for columns, or reference the codebook whenever I want to know the meaning of a variable. It often makes sense to rename columns that are not descriptive.\n",
    "\n",
    "There is more than one way to do this, but one easy way is to use the `.rename()` function.\n",
    "\n",
    "Here are the column names and their descriptions again for reference:\n",
    "\n",
    "    CRIM: per capita crime rate by town \n",
    "    ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "    INDUS: proportion of non-retail business acres per town \n",
    "    CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "    NOX: nitric oxides concentration (parts per 10 million) \n",
    "    RM: average number of rooms per dwelling \n",
    "    AGE: proportion of owner-occupied units built prior to 1940 \n",
    "    DIS: weighted distances to five Boston employment centres \n",
    "    RAD: index of accessibility to radial highways \n",
    "    TAX: full-value property-tax rate per 10000 dollars\n",
    "    PTRATIO: pupil-teacher ratio by town \n",
    "    B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "    LSTAT: % lower status of the population \n",
    "    MEDV: Median value of owner-occupied homes in 1000's of dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There two popular methods to rename dataframe columns.\n",
    "1. Using a _dictionary substitution_, which is very useful if you only want to rename a few of the columns. This method uses the `.rename()` function.\n",
    "2. Using a _list replacement_, which is quicker than writing out a dictionary, but requires a full list of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List Replacement Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='describe'></a>\n",
    "\n",
    "### 6. Describe the summary statistics for the columns\n",
    "\n",
    "---\n",
    "\n",
    "The `.describe()` function gives summary statistics for each of your variables. What are some, if any, oddities you notice about the variables based on this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boxplots'></a>\n",
    "\n",
    "### 7. Plot variables with potential outliers using boxplots.\n",
    "\n",
    "---\n",
    "\n",
    "Here we will use the seaborn package to plot boxplots of the variables we have identified as potentially having outliers.\n",
    "\n",
    "Some notes on seaborn's boxplot keyword argument options:\n",
    "\n",
    "    orient: can be 'v' or 'h' for vertical and horizontal, respectively\n",
    "    fliersize: the size of the outlier points (pixels I think)\n",
    "    linewidth: the width of line outlining the boxplot\n",
    "    notch: show the confidence interval for the median (calculated by seaborn/plt.boxplot)\n",
    "    saturation: saturate the colors to an extent\n",
    "\n",
    "There are more keyword arguments available but those are most relevant for now.\n",
    "\n",
    "_If you want to check out more, place your cursor in the `boxplot` argument bracket and press `shift+tab` (Press four times repeatedly to bring up detailed documentation)._\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate of crime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent owner occupied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# business zone percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black population statistic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot_all'></a>\n",
    "\n",
    "### 8. Plot all the variables on boxplots together.\n",
    "\n",
    "---\n",
    "\n",
    "Plot all the variables using using a horizontal boxplot with seaborn. What is wrong with this plot, if anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='standardization'></a>\n",
    "\n",
    "### 9. Standardizing variables\n",
    "\n",
    "---\n",
    "\n",
    "Rescaling variables is very common, and sometimes essential. For example, when we get to regularization of models the rescaling procedure becomes a requirement before fitting the model.\n",
    "\n",
    "Here we'll rescale the variables using a procedure called \"standardization\", which forces the distribution of each variable to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Standardization is not complicated:\n",
    "\n",
    "    standardized_variable = (variable - mean_of_variable) / std_dev_of_variable\n",
    "    \n",
    "Note: Nothing else is changed about the distribution of the variable. It doesn't become normally distributed.\n",
    "\n",
    "**9.A Pull out rate of crime and plot the distribution.**\n",
    "\n",
    "Also print out the mean and standard deviation of the original variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.B Standardize the rate_of_crime variable. Notice the new mean is centered at 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.C Plot the original and standardized rate of crime. Notice that nothing changes about the distribution except for the location and the scale.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='plot_all_rescaled'></a>\n",
    "\n",
    "### 10. Standardize all of the columns and re-create the boxplot\n",
    "\n",
    "---\n",
    "\n",
    "Pandas DataFrames make it extremely easy to standardize the columns all at once. You can standardize the data like so:\n",
    "\n",
    "```python\n",
    "boston_stand = (boston - boston.mean()) / boston.std()\n",
    "```\n",
    "\n",
    "Create a standardized version of the data and recreate the boxplot. Now you can better examine the differences in the shape of distributions across our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='cov_cor'></a>\n",
    "\n",
    "### 11. Covariance and correlation matrices\n",
    "\n",
    "---\n",
    "\n",
    "A great way to easily get a feel for linear relationships between your variables is with a correlation matrix.\n",
    "\n",
    "Below is the formula for the covariance between two variables $X$ and $Y$:\n",
    "\n",
    "#### 11.A Covariance\n",
    "\n",
    "Given sample size $N$ variables $X$ and $Y$, with means $\\bar{X}$ and $\\bar{Y}$:\n",
    "\n",
    "### $$ \\text{covariance}(X, Y) = \\sum_{i=1}^N \\frac{(X - \\bar{X})(Y - \\bar{Y})}{N}$$\n",
    "\n",
    "The covariance is a measure of \"relatedness\" between variables. It is literally the sum of deviations from the mean of $X$ times deviations from the mean of $Y$ adjusted by the sample size $N$.\n",
    "\n",
    "Code the covariance between `pct_underclass` and `home_median_value` below by hand. Verify that you got the correct result using `np.cov()`. Set the keyword argument `bias=True` in `np.cov()` to have it use the same covariance calculation.\n",
    "\n",
    "Note: `np.cov` returns a covariance _matrix_, which will be each values covariance with itself and the other variable in matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.B Correlation\n",
    "\n",
    "Covariance is not very interpretable. The values are difficult to read because they are relative to the variance of the variables.\n",
    "\n",
    "A much more common metric, and one directly calculable from the covariance, is the correlation.\n",
    "\n",
    "Again, let $X$ and $Y$ be our two variables, with covariance $cov(X, Y)$ that we calculated above:\n",
    "\n",
    "### $$ \\text{pearson correlation}\\;r = cor(X, Y) =\\frac{cov(X, Y)}{std(X)std(Y)}$$\n",
    "\n",
    "Calculate the correlation between `pct_under` and `med_value` by hand below. Check that it is the same as `np.corrcoef()` with `bias=True`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.C The correlation matrix\n",
    "\n",
    "We can see the correlation between all the numeric variables in our dataset by using pandas DataFrame's built in `.corr()` function. Use it below on the boston dataset.\n",
    "\n",
    "It is very useful to get a feel for what is related and what is not, which can help you decide what is worth investigating further (though with a lot of variables, the matrix can be a bit overwhelming...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seaborn also has a great way of showing this to us visually, if colors stick out to you more than decimol values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
