{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Coding AdaBoost from Scratch\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** This is intended to be a codealong with the instructor.\n",
    "\n",
    "Below we will be coding the AdaBoost algorithm from scratch, then visualizing the results as weak learners are chained onto the meta-estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost algorithm pt. 1\n",
    "\n",
    "---\n",
    "\n",
    "Recall,\n",
    "\n",
    "### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n",
    "\n",
    "$T$ is the set of \"weak learners\"\n",
    "\n",
    "$\\alpha_t$ is the contribution weight for weak learner $t$\n",
    "\n",
    "$h_t(X)$ is the prediction of weak learner $t$\n",
    "\n",
    "and $y$ is binary **with values -1 and 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the AdaBoost estimator function based on the algorithm described above.\n",
    "\n",
    "Your function will need arguments:\n",
    "- `X`, the design matrix\n",
    "- `estimators`, the weak learners\n",
    "- `alphas`, the contribution weight of each weak learner\n",
    "\n",
    "We will be using this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost algorithm pt. 2\n",
    "\n",
    "---\n",
    "\n",
    "The weak learner classifiers are trained sequentially.  After each fit, the importance weights on each observation need to be updated. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier itself represented by alpha $\\alpha$.\n",
    "\n",
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n",
    "\n",
    "Where $\\epsilon_t$ is the misclassification rate for the current classifier:\n",
    "\n",
    "### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a function to calculate the alpha weight.\n",
    "\n",
    "The function will take arguments:\n",
    "- `y`, the vector of true target values\n",
    "- `y_hat`, the vector of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost algorithm pt. 3\n",
    "\n",
    "---\n",
    "\n",
    "Adaboost sets up a weight vector on the observations, denoted $D_t$ where $t$ is the current model iteration. $D_t$ is a probability distribution that determines how likely it is a given observation will be selected as part of the training set for the current estimator.\n",
    "\n",
    "The $\\alpha$ weighting of the last fit estimator is used in the equation for the weighting distribution. The update equation is:\n",
    "\n",
    "### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n",
    "\n",
    "Where $i$ is the vector of observation indices and $x_i$ is the observation at the index. $y_i$ is the target.\n",
    "\n",
    "$h_t$ is the previous model fit in the boosting chain.\n",
    "\n",
    "And then divide the weights by the sum of weights to normalize them, ensuring that they sum to 1 and form a probability distribution:\n",
    "\n",
    "### $$ D_{t+1}(i) = \\frac{D_{t+1}(i)}{\\sum_{i=1}^N D_{t+1}(i)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a function to update the importance weights\n",
    "\n",
    "The function will take arguments:\n",
    "- `X`, the design matrix\n",
    "- `y`, the vector of target values\n",
    "- `current_D`, the current weights on observations\n",
    "- `alpha`, the weight on the current estimator\n",
    "- `estimator`, the current weak learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write a function that will sample observations according to weightings\n",
    "\n",
    "The function will take arguments:\n",
    "- `X`, the design matrix\n",
    "- `y`, the vector of target values\n",
    "- `D`, the probability weights on observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create some fake data (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# moons = make_moons(n_samples=30, noise=0.75)\n",
    "# X, y = moons\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n",
    "# y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "#                      np.arange(y_min, y_max, 0.02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write a function to plot the current AdaBoost iteration (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_adaboost(X, y, xx, xy, clf, D):\n",
    "#     from matplotlib.colors import ListedColormap\n",
    "\n",
    "#     Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "#     # Put the result into a color plot\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(9,7))\n",
    "    \n",
    "#     ss = [30 + 2500*w for w in D]\n",
    "\n",
    "#     cm_dark = ListedColormap(['#1F77B4', '#FF7F0E'])\n",
    "#     cm_light = ListedColormap(['#729ECE', '#FF9E4A'])\n",
    "#     # Plot the training points\n",
    "#     ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_dark, s=ss)\n",
    "\n",
    "#     ax.contourf(xx, yy, Z, cmap=cm_light, alpha=.25)\n",
    "\n",
    "#     ax.set_xlim(xx.min(), xx.max())\n",
    "#     ax.set_ylim(yy.min(), yy.max())\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Iterate the AdaBoost algorithm through 10 iterations, with each iteration adding a new weak learner\n",
    "\n",
    "This will need to follow the procedure outlined in the algorithm formulas above.\n",
    "\n",
    "You will need to keep track of:\n",
    "- `D`, the probability weights for sampling observations\n",
    "- a list of estimators (weak learners)\n",
    "- a list of alpha weightings on each weak learner\n",
    "\n",
    "At each iteration, plot AdaBoost with the function provided above and calculate the accuracy of the entire ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
