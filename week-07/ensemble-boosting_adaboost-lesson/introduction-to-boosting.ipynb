{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> **Jupyter slideshow:** This notebook can be displayed as slides. To view it as a slideshow in your browser type in the console:\n",
    "\n",
    "\n",
    "> `> jupyter nbconvert [this_notebook.ipynb] --to slides --post serve`\n",
    "\n",
    "\n",
    "> To toggle off the slideshow cell formatting, click the `CellToolbar` button, then `View --> Cell Toolbar --> None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Boosting\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Matt Brems (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the differences between bagging and boosting.\n",
    "- Understand how boosting is an ensemble method.\n",
    "- Learn the pros and cons to using boosting models.\n",
    "- Learn the effect of boosting on the bias-variance tradeoff.\n",
    "- Learn the math and procedure for AdaBoost, the \"classic\" boosting model.\n",
    "- Understand the differences between AdaBoost and Gradient Boosting Models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Boosting as an ensemble method](#intro)\n",
    "- [Pros and cons to boosting](#pros-cons)\n",
    "- [A visual description of bagging vs. boosting](#viz)\n",
    "- [Boosting and the bias-variance tradeoff](#bias-variance)\n",
    "- [AdaBoost](#adaboost)\n",
    "    - [Training example weights](#ex-weights)\n",
    "- [AdaBoost visualization](#adaboost-viz)    \n",
    "- [Gradient boosting models](#gradient)\n",
    "- [Gradient boosting visualization](#gboost-viz)\n",
    "- [The difference between the adaboost and gradient boosting](#the-difference-between-the-adaboost-and-gradient-boosting)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Boosting as an ensemble method\n",
    "\n",
    "---\n",
    "\n",
    "Boosting is another ensemble method with a different approach to bagging. Boosting takes a weak base learner and tries to make it a strong learner by re-training it on the misclassified samples.\n",
    "\n",
    "1. **Base model fitting is an iterative procedure**: it cannot be run in parallel.\n",
    "- **Weights assigned to observations indicating their \"importance\"**: samples with higher weights are given higher influence on the total error of the next model, prioritizing those observations.\n",
    "- **Weights change at each iteration with the goal of correcting the errors/misclassifications of the previous iteration**: the first base estimator is fit with uniform weights on the observations.\n",
    "- **Final prediction is typically constructed by a weighted vote**: weights for each base model depends on their training errors or misclassification rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='pros-cons'></a>\n",
    "## Pros and cons of boosting\n",
    "\n",
    "---\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Achieves higher performance than bagging when hyper-parameters tuned properly.\n",
    "- Can be used for classification and regression equally well.\n",
    "- Easily handles mixed data types.\n",
    "- Can use \"robust\" loss functions that make the model resistant to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Difficult and time consuming to properly tune hyper-parameters.\n",
    "- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\n",
    "- More risk of overfitting compared to bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='viz'></a>\n",
    "![boostvsbag](./images/BoostingVSBagging.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='bias-variance'></a>\n",
    "## Boosting and the bias-variance tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "Recall that **bagging aims to reduce variance**.\n",
    "\n",
    "**Boosting aims to reduce bias!** (and can reduce variance a bit as well).\n",
    "\n",
    "### Why?\n",
    "\n",
    "The rationale/theory behind Boosting is to combines **many weak learners into a single strong learner.**\n",
    "\n",
    "Instead of deep/full decision trees like in bagging, **boosting uses shallow/high bias base estimators.**\n",
    "\n",
    "Thus, each weak learner has:\n",
    "- Low variance\n",
    "- High bias\n",
    "\n",
    "And the iterative fitting to explain error/misclassification unexplained by the previous base models reduces bias without increasing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='adaboost'></a>\n",
    "## AdaBoost\n",
    "\n",
    "---\n",
    "\n",
    "AdaBoost is the original boosting algorithm. Predictions from AdaBoost follow the formula:\n",
    "\n",
    "\n",
    "### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n",
    "\n",
    "$T$ is the set of \"weak learners\"\n",
    "\n",
    "$\\alpha_t$ is the contribution weight for weak learner $t$\n",
    "\n",
    "$h_t(X)$ is the prediction of weak learner $t$\n",
    "\n",
    "and $y$ is binary **with values -1 and 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The core principle of AdaBoost is to **fit a sequence of weak learners**, i.e., models that are only slightly better than random guessing, such as decision stump (a single split tree) **on repeatedly modified versions of the data**. After each fit, the importance weights on each observation need to be updated. \n",
    "\n",
    "The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier itself represented by alpha $\\alpha$.\n",
    "\n",
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n",
    "\n",
    "Where $\\epsilon_t$ is the misclassification rate for the current classifier:\n",
    "\n",
    "### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n",
    "\n",
    "As iterations proceed, **examples that are difficult to predict receive ever-increasing influence**. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ex-weights'></a>\n",
    "### Training example weights\n",
    "\n",
    "Adaboost sets up a weight vector on the observations, denoted $D_t$ where $t$ is the current model iteration. $D_t$ is a probability distribution that determines how likely it is a given observation will be selected as part of the training set for the current estimator.\n",
    "\n",
    "The $\\alpha$ weighting of the last fit estimator is used in the equation for the weighting distribution. The update equation is:\n",
    "\n",
    "### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n",
    "\n",
    "Where $i$ is the vector of observation indices and $x_i$ is the observation at the index. $y_i$ is the target.\n",
    "\n",
    "$h_t$ is the previous model fit in the boosting chain.\n",
    "\n",
    "And then divide the weights by the sum of weights to normalize them, ensuring that they sum to 1 and form a probability distribution:\n",
    "\n",
    "### $$ D_{t+1}(i) = \\frac{D_{t+1}(i)}{\\sum_{i=1}^N D_{t+1}(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='adaboost-viz'></a>\n",
    "## AdaBoost visualization\n",
    "\n",
    "![boostvsbag](./images/adaboost-viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='gradient'></a>\n",
    "## Gradient boosting models\n",
    "\n",
    "---\n",
    "\n",
    "Gradient Boosting Classifier is a generalization of boosting to arbitrary differentiable loss functions. The intuition behind this mechanism is to:\n",
    "\n",
    "1. Fit a model $F$ to the data.\n",
    "2. Look at the difference between our observed $y$ and our model $F$. (The $y_i - F(x_i)$ can be thought of as residuals!)\n",
    "3. Fit a second model $F_2$ to (roughly) the residuals $y_i - F(x_i)$.\n",
    "4. Aggregate your model $F$ and $F_2$. While we won't get into the details here, we can interpret residuals as negative gradients. By doing this, we can apply our gradient descent algorithm to optimize our loss and generalize this to many loss functions.\n",
    "\n",
    "GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology.\n",
    "\n",
    "[This presentation](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) visually shows an example of AdaBoost at work and how AdaBoost laid the groundwork for Gradient Boosting Classifier.\n",
    "\n",
    "**The advantages of GBRT are:**\n",
    "\n",
    "- Natural handling of data of mixed type (= heterogeneous features).\n",
    "- Predictive power.\n",
    "- Robustness to outliers in output space (via robust loss functions).\n",
    "\n",
    "**The disadvantages of GBRT are:**\n",
    "- Scalability, due to the sequential nature of boosting it can hardly be parallelized.\n",
    "- Difficult hyperparameters to tune.\n",
    "\n",
    "\n",
    "> _For more detailed explanations see [here](https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting) and [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='gboost-viz'></a>\n",
    "## Gradient boosting visualization\n",
    "![boostvsbag](./images/gboost-viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='the-difference-between-the-adaboost-and-gradient-boosting'></a>\n",
    "## The difference between the AdaBoost and Gradient Boosting?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- AdaBoost is about re-weighting the preceding model's errors in subsequent iterations.\n",
    "- Gradient Boosting is about fitting subsequent models to the residuals of the last model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Random Forest on wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "- [Quora question on Random Forest](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n",
    "- [Scikit Learn Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Scikit Learn Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [Academic intro to Adaptive Boosting](http://rob.schapire.net/papers/explaining-adaboost.pdf)\n",
    "- [Stack Exchange AdaBoost vs Gradient Boosting](http://stats.stackexchange.com/questions/164233/intuitive-explanations-of-differences-between-gradient-boosting-trees-gbm-ad)\n",
    "- [Gentle intro to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)\n",
    "- [Quora on intuitive explanations of Adaboost](https://www.quora.com/What-is-AdaBoost)\n",
    "- MIT on [adaptive boosting](http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf)\n",
    "- A lighter [math introduction](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) to AdaBoosting and Gradient Boosting h/t Charlie\n",
    "- A [walkthrough](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) on tuning GBM h/t Sheena\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
