{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to DBSCAN\n",
    "\n",
    "_Authors: Joseph Nelson (DC), Alexander Combs(NYC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Introduce the DBSCAN algorithm\n",
    "- Explain how DBSCAN works\n",
    "- Compare DBSCAN to K-Means and Hierarchical Clustering\n",
    "- See DBSCAN in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Review of clustering](#review-of-clustering)\n",
    "\t- [K-means](#k-means)\n",
    "\t- [Hierarchical Clustering](#hierarchical-clustering)\n",
    "- [What is DBSCAN?](#what-is-dbscan)\n",
    "- [How does DBSCAN work?](#how-does-dbscan-work)\n",
    "\t- [The Parameters](#the-parameters)\n",
    "\t- [The DBSCAN Algorithm:](#the-dbscan-algorithm)\n",
    "\t- [DBSCAN algorithm in words/review of concept:](#dbscan-algorithm-in-wordsreview-of-concept)\n",
    "\t- [Algorithm visualization](#algorithm-visualization)\n",
    "- [How does DBSCAN compare to K-means and Hierarchical Clustering?](#how-does-dbscan-compare-to-k-means-and-hierarchical-clustering)\n",
    "\t- [Pros and Cons](#pros-and-cons)\n",
    "- [How do we implement DBSCAN?](#how-do-we-implement-dbscan)\n",
    "\t- [Key outputs](#key-outputs)\n",
    "- [How do you know what good estimates of epsilon and min pts are?](#how-do-you-know-what-good-estimates-of-epsilon-and-min-pts-are)\n",
    "- [Additional resources](#additional-resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"review-of-clustering\"></a>\n",
    "## Review of clustering\n",
    "---\n",
    "\n",
    "- Clustering is an unsupervised learning technique we employ to group “similar” data points together\n",
    "- With unsupervised learning, remember: there is no clear objective, there is no “right answer” (hard to tell how we’re doing), there is no response variable, just observations with features, and labeled data is not required\n",
    "\n",
    "![](./assets/images/clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"k-means\"></a>\n",
    "### K-means\n",
    "![](./assets/images/Kmeans_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Pros:**\n",
    "- Easy to implement even on relatively large data sets ( ~$O(n)$ )\n",
    "- Usually \"good enough\" results\n",
    "\n",
    "**Cons:**\n",
    "- Requires an arbitrary k\n",
    "- Sensitive to outliers (k-medians is more robust)\n",
    "- With random initial centroids lacks repeatability (but can be seeded)\n",
    "- Works best if data conforms to circular -> spherical -> hyperspherical shape (n.b. using means)\n",
    "- Works best with similar density clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"hierarchical-clustering\"></a>\n",
    "### Hierarchical Clustering\n",
    "\n",
    "![](./assets/images/agglomerative-clustering.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Pros:**\n",
    "- No need to pick explicit k (...but kind of because we need to pick a split point)\n",
    "- Repeatability/Deterministic model (always get the same clusters) \n",
    "- Can dial cluster levels at will\n",
    "    \n",
    "**Cons:**\n",
    "- Runs in $~O(n^2)$ time - so must be relatively small dataset\n",
    "- Requires selecting a linkage method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"what-is-dbscan\"></a>\n",
    "## What is DBSCAN?\n",
    "---\n",
    "\n",
    "- DBSCAN: Density-based Spatial Clustering of Applications with Noise\n",
    "- For DBSCAN, clusters of high density are separated by clusters of low density\n",
    "- DBSCAN is the most widely used and applicable clustering algorithm \n",
    "    - given that it takes minimum predefined input and can discover clusters of any shape, not just the sphere-like clusters that k-means often computes. This way, we can discover less pre-defined patterns and glean some more useful insights.\n",
    "    \n",
    "**Why Density?**\n",
    "\n",
    "Because DBSCAN uses a neighbor-based polling approach. It is ideal for clusters of that have similar variance.\n",
    "\n",
    "**Why noise?**\n",
    "\n",
    "Because not every point will be associated with a cluster. Some are left as outlier points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"how-does-dbscan-work\"></a>\n",
    "## How does DBSCAN work?\n",
    "---\n",
    "\n",
    "- DBSCAN is a density based clustering algorithm, meaning that the algorithm\n",
    "finds clusters by seeking areas of the dataset that have a higher density of points\n",
    "than the rest of the dataset.\n",
    "- Given this, unlike in our previous examples, after you apply DBSCAN there may be data points that aren't assigned to any cluster at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"the-parameters\"></a>\n",
    "### The Parameters\n",
    "When we use DBSCAN, it requires two input parameters: \n",
    "\n",
    "**Min Points** - This is the minimum number of points required to form a cluster.\n",
    "\n",
    "**Epsilon** - This is the maximum spanning distance that a point can be from the polling point in order to be recruited to the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"the-dbscan-algorithm\"></a>\n",
    "<a id=\"the-dbscan-algorithm\"></a>\n",
    "### The DBSCAN Algorithm:\n",
    "1. Choose an “epsilon” and “min_samples”\n",
    "2. Pick an arbitrary point, and check if there are at least “min_samples” points\n",
    "within distance “epsilon”\n",
    "    - If yes, add those points to the cluster and check each of the new points\n",
    "    - If no, choose another arbitrary point to start a new cluster\n",
    "3. Stop once all points have been checked\n",
    "\n",
    "![](./assets/images/dbscan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"dbscan-algorithm-in-wordsreview-of-concept\"></a>\n",
    "<a id=\"dbscan-algorithm-in-wordsreview-of-concept\"></a>\n",
    "### DBSCAN algorithm in words/review of concept:\n",
    "DBSCAN will take the epsilon and minimum points we provided it and cluster all of\n",
    "the points in a neighborhood, first passing the minimum points requirement and\n",
    "then clustering each of the points within epsilon distance to form the clusters.\n",
    "\n",
    "Once one cluster is formed, the algorithm then moves to a new datapoint, and\n",
    "seeks to find related points to form yet another cluster; this will continue until\n",
    "DBSCAN simply runs out of points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"algorithm-visualization\"></a>\n",
    "<a id=\"algorithm-visualization\"></a>\n",
    "### Algorithm visualization\n",
    "- http://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\n",
    "- Let’s play with this a bit^^^\n",
    "- Independently, select the “Pimpled Smiley” distribution of points. \n",
    "    - What is an optimal epsilon? \n",
    "    - What about minimum number of points?\n",
    "    - What about minimum number of points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"how-does-dbscan-compare-to-k-means-and-hierarchical-clustering\"></a>\n",
    "## How does DBSCAN compare to K-means and Hierarchical Clustering?\n",
    "---\n",
    "\n",
    "- Whereas k-means can be thought of as a \"general\" clustering approach, DBSCAN performs especially well with unevenly distributed, non-linear clusters.\n",
    "- The fundamental difference with DBSCAN lies in the fact that it is density based, as opposed to k-means which calculates clusters based on distance from a central point, or hierarchical clustering.\n",
    "- By choosing too few points for DBSCAN, i.e. less than two, we'll effectively get a straight line if we connect the points, just like linkage clustering.\n",
    "\n",
    "> *Note:* if you set the criteria for minimum points too low with DBSCAN (< 2), this becomes essentially the same result as hierarchical clustering. To diversify the DBSCAN, we therefore must give it a significant amount of points to form a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"pros-and-cons\"></a>\n",
    "### Pros and Cons\n",
    "DBSCAN can be useful to us when we have a lot of dense data. If we used kmeans\n",
    "on this data, the algorithm would effectively give us just one large cluster!\n",
    "However with DBSCAN, we can actually break down this cluster into smaller\n",
    "groups to see their attributes.\n",
    "\n",
    "- **Advantages:**\n",
    "    - Clusters can be any size or shape\n",
    "    - No need to choose number of clusters\n",
    "    \n",
    "\n",
    "- **Disadvantages:**\n",
    "    - More parameters to tune\n",
    "    - Doesn’t work with clusters of varying density\n",
    "    - NOTE: Not every point is assigned to a cluster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"how-do-we-implement-dbscan\"></a>\n",
    "## How do we implement DBSCAN?\n",
    "---\n",
    "\n",
    "To implement DBSCAN in Python, we first import it from sklearn:\n",
    "\n",
    "```Python\n",
    "from sklearn.cluster import DBSCAN\n",
    "```\n",
    "Next, assuming that we are using the classic Iris dataset, we define X as the data\n",
    "and y are the class variables\n",
    "\n",
    "```Python \n",
    "X, y = iris.data, iris.target\n",
    "```\n",
    "Next, we call DBSCAN from sklearn:\n",
    "\n",
    "```Python\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "```\n",
    "\n",
    "**Given the above input, what have we said about our clusters?**\n",
    "\n",
    "- Here, we've set epsilon to a standard value of .3 and set the minimum number of\n",
    "points at 10, and then fit the model to our data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"key-outputs\"></a>\n",
    "<a id=\"key-outputs\"></a>\n",
    "### Key outputs\n",
    "- The DBSCAN algorithm in Python returns two items - the core samples and the\n",
    "labels. The core samples are the points which the algorithm initially finds and\n",
    "searches around the neighborhood to form the cluster, and the labels are simply\n",
    "the cluster labels.\n",
    "\n",
    "**Check: how many labels should we expect to receive?**\n",
    "\n",
    "```Python\n",
    "core_samples = db.core_sample_indices_\n",
    "labels = db.labels_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"how-do-you-know-what-good-estimates-of-epsilon-and-min-pts-are\"></a>\n",
    "## How do you know what good estimates of epsilon and min pts are?\n",
    "---\n",
    "\n",
    "As a general rule when choosing the minimum points - you should always aim to have the **minimum number of points be greater or equal to the amount of dimensions in your data, plus one**. This typically will give the algorithm a good estimation of how to evaluate the clusters. \n",
    "\n",
    "Estimating epsilon is a bit trickier. One option is to use a method called the k-distance, which can help visualize the best epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"additional-resources\"></a>\n",
    "<a id=\"additional-resources\"></a>\n",
    "## Additional resources\n",
    "\n",
    "From the documentation http://scikitlearn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
