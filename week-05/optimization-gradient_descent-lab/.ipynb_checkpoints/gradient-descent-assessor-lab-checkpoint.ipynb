{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Gradient Descent in Sklearn\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "Until now we've been using specific sklearn model classes to perform regression and classification such as `LinearRegression` and `LogisticRegression`. Unfortunately, while these methods work well on smaller datasets with relatively small numbers of columns, once you start getting into \"Medium Data\" these slow down to a crawl, and take up so much memory that fitting them becomes mind-numbingly slow (especially on a laptop).\n",
    "\n",
    "Luckily, sklearn comes with  stochastic gradient descent solvers for regression and classification:\n",
    "- `SGDRegressor`\n",
    "- `SGDClassifier`\n",
    "\n",
    "Due to its ability to minimize the loss function iteratively on smaller portions of the data, it avoids the intense slowdown other models suffer on large datasets.\n",
    "\n",
    "> **Note:** The gradient descent solvers are very flexible and can fit a variety of different model types not covered here. I highly recommend reading their documentation in detail.\n",
    "\n",
    "---\n",
    "\n",
    "### SF assessor data\n",
    "\n",
    "This lab uses data from the SF assessor's office on housing prices in San Francisco - it's already cleaned up.\n",
    "\n",
    "You can see that the dataset has 250k rows. When expanding this with dummy-coded categorical columns it can become quite large. Be careful that you don't exceed the memory on your computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import patsy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data.\n",
    "\n",
    "Examine the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop = pd.read_csv('./datasets/assessor_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(754147, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sample down the data\n",
    "\n",
    "Despite this already being a sample of the full assessor dataset, you should sample the data down further the sake of speed and your computers memory.\n",
    "\n",
    "Use the `.sample()` function for pandas dataframes to subset this down to < 25000 rows. \n",
    "\n",
    "Sampling down large datasets is a common procedure. Finding the optimal parameters with larger subsets of the data may change the hyperparameters and the results, and will get you closer to the best coefficients, but the returns are marginal at a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prop_samp = prop.sample(n=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regression with stochastic gradient descent\n",
    "\n",
    "Below I set up X, y data predicting value (housing price) from the remaining variables. There are ~75,000 rows, with 170 columns.\n",
    "\n",
    "\n",
    "The `SGDRegressor` is very general and flexible, and can be customized with a variety of keyword arguments.\n",
    "\n",
    "**Arguments**\n",
    "- `loss`: `['squared_loss','huber', ...]`\n",
    "    - The `'squared_loss'` loss corresponds to solving a regression with the least squares loss. This is what I expect you'll use, but there are other options. Huber loss is a \"robust\" regression loss.\n",
    "- `penalty`: `['none','l1','l2','elasticnet']`\n",
    "    - This defines the penalty on the regression that you would like to solve. The l1 and l2 are the Lasso and Ridge, while the elasticnet is the combination of them both.\n",
    "- `alpha`\n",
    "    - The regularization strength to be used with a chosen penalty. Same as in Lasso and Ridge.\n",
    "- `l1_ratio`\n",
    "    - The mix of the Lasso and Ridge penalties when elasticnet is chosen as the penalty.\n",
    "- `n_iter`\n",
    "    - The number of training \"epochs\" over the data. This is the number of passes that the gradient descent algorithm will make over the data to iteratively fit the weights (defaults to 5).\n",
    "\n",
    "`SGDRegressor` is most often used in tandem with grid searching to find the optimal parameters for certain models. \n",
    "\n",
    "**It is up to you how you want to define the model. You should:**\n",
    "\n",
    "1. Choose a target to estimate (this should be continuous).\n",
    "- Select predictors to use.\n",
    "- Standardize your predictor matrix.\n",
    "- Build a stochastic gradient descent solver to fit your model. You will likely want to do some kind of gridsearch to find the optimal parameters for your model.\n",
    "- Describe the model selected through gridsearch and compare the performance to baseline.\n",
    "- Examine and interpret the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value ~ baths + beds + lot_depth + basement_area + front_ft + owner_pct + rooms + property_class + neighborhood + tax_rate + volume + sqft + stories + year_recorded + year_built + zone -1\n",
      "(25000,) (25000, 163)\n"
     ]
    }
   ],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification with stochastic gradient descent\n",
    "\n",
    "The `SGDClassifier` is very similar to the `SGDRegressor`. The main difference is that the loss functions are changed to regression loss functions.\n",
    "\n",
    "**Arguments**\n",
    "- `loss`: `['log', ...]`\n",
    "    - The `'log'` loss corresponds to solving a logistic regression classifier. This is what I expect you'll use, but there are many other options.\n",
    "- `penalty`: `['none','l1','l2','elasticnet']`\n",
    "    - This defines the penalty on the regression that you would like to solve. The l1 and l2 are the Lasso and Ridge, while the elasticnet is the combination of them both.\n",
    "- `alpha`\n",
    "    - The regularization strength to be used with a chosen penalty. Same as in Lasso and Ridge.\n",
    "- `l1_ratio`\n",
    "    - The mix of the Lasso and Ridge penalties when elasticnet is chosen as the penalty.\n",
    "- `n_iter`\n",
    "    - The number of training \"epochs\" over the data. This is the number of passes that the gradient descent algorithm will make over the data to iteratively fit the weights (defaults to 5).\n",
    "\n",
    "Like `SGDRegressor`, `SGDClassifier` is most often used in tandem with grid searching to find the optimal parameters for certain models. \n",
    "\n",
    "**It is up to you how you want to define the model. You should:**\n",
    "\n",
    "1. Choose a target to classify (you may need to engineer one from existing variables).\n",
    "- Calculate the baseline accuracy.\n",
    "- Select predictors to use.\n",
    "- Standardize your predictor matrix.\n",
    "- Build a stochastic gradient descent solver to fit your model. You will likely want to do some kind of gridsearch to find the optimal parameters for your model.\n",
    "- Describe the model selected through gridsearch and compare the performance to baseline.\n",
    "- Examine and interpret the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'baths', u'beds', u'lot_depth', u'basement_area', u'front_ft',\n",
       "       u'owner_pct', u'rooms', u'property_class', u'neighborhood', u'tax_rate',\n",
       "       u'volume', u'sqft', u'stories', u'year_recorded', u'year_built',\n",
       "       u'zone', u'value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
