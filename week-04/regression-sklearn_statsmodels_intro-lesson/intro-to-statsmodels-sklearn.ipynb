{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to `statsmodels` and `scikit-learn`\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://avatars2.githubusercontent.com/u/365630?v=3&s=400\" style=\"width: 300px; float: left; margin: 20px; margin-top: -20px; break: right;\"><img src=\"https://snag.gy/qfaubJ.jpg\" style=\"width: 300px; float: left; margin: 20px;\"> \n",
    "\n",
    "<br clear=\"all\">\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "- Overview what the statsmodels and scikit-learn modules are used for\n",
    "- Learn how to build a linear regression model with scikit-learn\n",
    "- Practice building models using scikit-learn\n",
    "- Learn how to build a linear regression model using statsmodels\n",
    "- Understand the practical differences between scikit-learn and statsmodels\n",
    "- Interpret the output of models from both packages\n",
    "- Learn how to create formulas using the patsy module to easily specify target and predictor matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [`sklearn` & `statsmodels`](#intro)\n",
    "- [Looking at the documentation](#documentation)\n",
    "- [History of scikit-learn](#sklearn-background)\n",
    "- [First steps with sklearn](#sklearn-first-steps)\n",
    "- [Fitting a model with sklearn](#first-model-sklearn)\n",
    "- [sklearn model class attributes](#model-attributes)\n",
    "- [Review metrics for evaluating regression models](#common-metrics)\n",
    "- [Fit a MLR using sklearn](#mlr-sklearn)\n",
    "- [A note on negative $R^2$ values](#negative-r2)\n",
    "- [Fitting a linear regression using statsmodels](#statsmodels-intro)\n",
    "- [Statsmodels `.summary()` function](#statsmodels-summary)\n",
    "- [Independent practice](#independent-practice)\n",
    "- [A brief introduction to `patsy` formulas](#patsy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "##  `sklearn` & `statsmodels`\n",
    "\n",
    "---\n",
    "\n",
    "This lesson intends to introduce the modeling packages `sklearn` and `statsmodels` in the context of regression modeling. These are both powerful python packages with different strengths. \n",
    "\n",
    "In general:\n",
    "- **`sklearn`** is the *machine learning* package\n",
    "- **`statsmodels`** is the *statistics* package\n",
    "\n",
    "Though the terms have immense overlap, machine learning tends to be more prediction focused while statistics is more inference focused. \n",
    "\n",
    "**Remember: even with all the power provided by these modeling tools, it's never a replacement for good EDA!**\n",
    "\n",
    "---\n",
    "\n",
    "### A preface on modeling\n",
    "\n",
    "As we venture down the path of modeling, it can be difficult to determine which choices are \"correct\" or \"incorrect\".  A primary challenge is to understand how different models will perform in different circumstances and different types of data. It's essential to practice modeling on a variety of data.\n",
    "\n",
    "As a beginner it is essential to learn which metrics are important for evaluating your models and what they mean. The metrics we evaluate our models with inform our actions.  \n",
    "\n",
    "*Exploring datasets on your own with the skills and tools you learn in class is highly recommended!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='documentation'></a>\n",
    "\n",
    "## `sklearn` and `statsmodels` documentation\n",
    "\n",
    "---\n",
    "\n",
    "Get familiar with looking up things in the `sklearn` and `statsmodels` documentation. You are going to be doing a lot of it over the course of DSI and beyond.\n",
    "\n",
    "[The statsmodels documentation can be found here.](http://statsmodels.sourceforge.net/devel/) Many recommend using the bleeding-edge version of statsmodels. [For that you can reference the code on github.](https://github.com/statsmodels/statsmodels/)\n",
    "\n",
    "[The sklearn documentation can be found here.](http://scikit-learn.org/stable/documentation.html)\n",
    "\n",
    "The packages have fairly different approaches and syntax for constructing models. Below are examples for linear regression in each package:\n",
    "* [Linear regression in statsmodels](http://statsmodels.sourceforge.net/devel/examples/#regression)\n",
    "* [Linear regression in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "If you haven't yet, familliarize yourself with the format of the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-background'></a>\n",
    "\n",
    "## Background: Scikit-learn / sklearn\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://avatars1.githubusercontent.com/u/25111?v=3&s=200\" style=\"float: left; margin: 0 25px;\"> Scikit-learn was founded in 2007 as a Google summer of code project by [David Cournapeau](https://github.com/cournape).  Later in 2007, Matthieu Brucher published his thesis on Scikit-learn.  Since then, the Scikit-learn project has taken on a worldwide team of owners. A great high level overview of the project can be found in a 2011 publication in [Journal of Machine Learning Research 12 (2011) 2825-2830](http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf).\n",
    "\n",
    "Skikit-learn is part of the Scipy family of \"kits\".  Explore some of the [other projects](https://scikits.appspot.com/scikits) in this family.\n",
    "<br clear=\"all\"><br>\n",
    "\n",
    "\n",
    "**Scikit-learn provides a wide variety of machine learning models, including:**\n",
    "\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Support Vector Machines\n",
    "- Classification And Regression Tree Models\n",
    "- Naive Bayes\n",
    "- Clustering Models (K-Means, Hierarchical, DBScan)\n",
    "\n",
    "**It also handles the construction of typical machine learning pipeline utilities for:**\n",
    "- Model evaluation\n",
    "- Model selection\n",
    "- Preprocessing\n",
    "- Natural Language Processing\n",
    "- Dimensionality Reduction\n",
    "\n",
    "**Scikit-learn comes with a ton of datasets that are cleaned and formatted to work with the models provided by their library:**\n",
    "- Boston Housing\n",
    "- Iris Flowers\n",
    "- Diabetes Diagnostics\n",
    "- Various sample images (for classification)\n",
    "  - Faces\n",
    "  - MINIST (handwriting examples)\n",
    "- Random data generators\n",
    "- Spam examples\n",
    "- Newsgroup classfication\n",
    "\n",
    "[Read more about Scikit-learn datasets](http://scikit-learn.org/stable/datasets/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sklearn` under the hood\n",
    "\n",
    "- **Numpy**:  The base for data structures and transformations. Input data is represented as numpy arrays, integrating seamlessly with other scientific Python libraries. Numpy’s viewbased memory model limits copies, even when binding with compiled code.  It also provides basic arithmetic and linear algebra operations.<br><br>\n",
    "\n",
    "- **Scipy**:  Efficient algorithms for linear algebra, sparse matrix representation, special functions and basic statistical functions.<br><br>\n",
    "\n",
    "- **Cython**:  A language for combining C with Python. Cython makes it easy to reach the performance of compiled languages with Python-like syntax and high-level operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-first-steps'></a>\n",
    "\n",
    "## First steps with `sklearn`: loading the data\n",
    "\n",
    "---\n",
    "\n",
    "We will load the boston housing dataset using sklearn and then construct and fit a linear regression model on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Don't forget to turn on plotting display in the notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the boston housing data with the `datasets.load_boston()` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data object we've loaded has attributes with the features, target variable, and design matrix:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the data in pandas for convenience\n",
    "\n",
    "Our target is what we are predicting.  Sometimes this is called the **response variable**.\n",
    "\n",
    "The target and the data are what we use to train, or **fit** the model with.\n",
    "\n",
    "Scikit-learn has already split our data into the **predictors** and **response** for us. It has also stored the names of the features in a separate array. \n",
    "\n",
    "So we can print things like the header of the data it will be more convenient to have our data in a pandas dataframe.\n",
    "\n",
    "**Use the predictors and the feature names to create a pandas dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set is a matrix / dataframe with many variables (**CRI, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B,** and **LSTAT**). We have **13** predictors with **506** rows/observations.\n",
    "\n",
    "Our target is a vector that represents a single variable (**MEDV**), which has exactly the same number of observations as our training set: **506**.\n",
    "\n",
    "> _Training (fit) and target datasets must always match in length!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index correspondence between target and predictors\n",
    "\n",
    "Row 0 of **`df`**, our training data, is:\n",
    "\n",
    "```\n",
    "[0.00632\t18.0\t2.31\t0.0\t0.538\t6.575\t65.2\t4.0900\t1.0\t296.0\t15.3\t396.90\t4.98]\n",
    "```\n",
    "\n",
    "This corresponds to the 0 index obersvation in our target vector:\n",
    "```\n",
    "24.0\n",
    "```\n",
    "\n",
    "These two seperate datasets (a matrix/dataframe, and a vector), are what we will use in the `.fit(predictors, target)` function in sklearn's models.  \n",
    "\n",
    "- The training data is 2D with dimensions: `n_samples x n_features`\n",
    "- The response is 1D with dimensions: `n_samples`, matching the `n_samples` of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='first-model-sklearn'></a>\n",
    "\n",
    "## Fitting our 1st model with `sklearn`\n",
    "\n",
    "---\n",
    "\n",
    "Now let's fit a linear regression model with the housing data. \n",
    "\n",
    "First let's visually identify some predictors that seem to have a relationship with house value. \n",
    "\n",
    "**Plot RM and LSTAT against the target variable with seaborn.** \n",
    "\n",
    "> _Note: If for some reason scikit-learn crashes the jupyter notebook, have conda remove mkl (there's an issue with the newer build on some systems)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we fit a linear regression model predicting `MEDV` (the target vector) from `RM`.**\n",
    "\n",
    "> **Note:** sklearn models expect the predictor matrix to be 2D and the target to be 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lm = linear_model.LinearRegression()\n",
    "\n",
    "# X = df[[\"RM\"]]\n",
    "# y = target \n",
    "\n",
    "# model = lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions for the X matrix using `.predict(X)`, and score the model ($R^2$) using `model.score(X, y)`.**\n",
    "\n",
    "Plot the predicted values against the true values of the target, and print the model $R^2$.\n",
    "\n",
    "> **`.score(predictors, target)`**: a class method / function that returns the coefficient of determination R^2 of the prediction (for regression models).  Found in many models in scikit-learn (but not all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can this plot tell us about the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-attributes'></a>\n",
    "\n",
    "## sklearn model class attributes\n",
    "\n",
    "---\n",
    "\n",
    "After you run `.fit()`, a sklearn model object often contains a variety of calculated metrics, coefficients, and other information. Which metrics and attributes are present will depend on the model – consult the documentation for specifics. \n",
    "\n",
    "Attributes in the `LinearRegression` object include:\n",
    "- **`.coef_`**: property containing the coeffients for the predictor variables\n",
    "- **`.intercept_`**: value of the intercept\n",
    "\n",
    "**Print out the beta coefficient and intercept for the model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the coefficient mean in the context of your model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common-metrics'></a>\n",
    "\n",
    "## Review: common metrics for evaluating regression models\n",
    "\n",
    "---\n",
    "\n",
    "The [root mean squared error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is a standard measure of model performance. It is the square root of the mean of the sum of squared residuals:\n",
    "\n",
    "### $$ \\operatorname{RMSE}= \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i} - y_i)^2} $$\n",
    "\n",
    "The smaller the root mean squared error, the better your model fits the data. \n",
    "\n",
    "You are already familiar with the [coefficient of determination $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination):\n",
    "\n",
    "### $$ R^2 = 1 - \\frac{SS_{reg}}{SS_{tot}} $$\n",
    "\n",
    "Where the regression sum of squares is the sum of squared residuals for our model:\n",
    "\n",
    "$SS_{reg}=\\sum_i (\\hat{y} -\\bar{y})^2$\n",
    "\n",
    "And the total sum of squares is the sum of squared residuals for the *baseline* model. This is essentially the variance of our target.\n",
    "\n",
    "$SS_{tot} = \\sum_i (y_i-\\bar{y})^2$\n",
    "\n",
    "$R^2$ is the most common metric to evaluate a regression and is the default scoring measure in sklearn. When we cover classification models, the `.score` function instead defaults to accuracy.\n",
    "\n",
    "\n",
    "**Calculate the RMSE of your model by leveraging `sklearn.metrics.mean_squared_error`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mlr-sklearn'></a>\n",
    "\n",
    "## Fit a MLR using sklearn\n",
    "\n",
    "---\n",
    "\n",
    "We have fit a simple linear regression predicting `MEDV ~ RM + 1` (where the 1 represents the intercept). Use the same sklearn process and `LinearRegression` model to estimate the target with both `RM` and `LSTAT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the coefficients from this MLR model and interpret them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='negative-r2'></a>\n",
    "\n",
    "## A note on negative $R^2$ values\n",
    "\n",
    "---\n",
    "\n",
    "Over the course of this class you will encounter negative $R^2$ values. This may seem impossible, and it is in the standard scenario where we are calculating the $R^2$ score on the data we fit the model with.\n",
    "\n",
    "However, if you fit your model on one sample of data, *then score the model on new data not used to fit the model*, it is possible to end up with negative $R^2$.\n",
    "\n",
    "**What does it mean to have a negative $R^2$?**\n",
    "\n",
    "Remember that $R^2$ is 1 minus the error of your regression model divided by the error of the baseline model. A negative $R^2$ means that the regression model is performing *worse* than the baseline model. In the context of fitting our data on one sample of data and scoring on another sample, this means that we would have been better off making predictions on the test sample just using the mean of the target variable in our training set.\n",
    "\n",
    "We will return to the topic of negative $R^2$ when we talk about training and testing sets and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='statsmodels-intro'></a>\n",
    "\n",
    "## Fitting a linear regression using `statsmodels`\n",
    "\n",
    "---\n",
    "\n",
    "Now we will fit the linear regression model predicting the target from `RM` and `LSTAT`, but this time using `statsmodels`.\n",
    "\n",
    "The format looks like:\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = df[[\"RM\",\"LSTAT\"]].values\n",
    "# manually add the intercept column:\n",
    "X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "y = target\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "model = model.fit()\n",
    "predictions = model.predict()\n",
    "```\n",
    "\n",
    "First we load the statsmodels api module, which contains the ordinary least squares `OLS` model class. The statsmodels process is slightly different:\n",
    "- We manually make a new column for the intercept in our design matrix $X$.\n",
    "- The $y$ target variable comes before the $X$ predictor\n",
    "- The data is provided during the instantiation of the model object, then fit is called without the data.\n",
    "\n",
    "**Fit the model using statsmodels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='statsmodels-summary'></a>\n",
    "\n",
    "### Statsmodels `.summary()`  function\n",
    "\n",
    "Once a model is fit with statsmodels, you can print out a variety of summary statistics, metrics, and properties of the model using the `model.summary()` function.\n",
    "\n",
    "You are already familiar with some of the information available in the summary:\n",
    "- R-squared\n",
    "- Number of observations\n",
    "- Coefficients for the variables and the intercept (const)\n",
    "- Standard errors of the coefficients, t-statistics, p-values, and confidence intervals\n",
    "\n",
    "There is also a variety of different metrics that we have not yet talked about. Don't hesitate to look up any of the statistics online if you are curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='independent-practice'></a>\n",
    "\n",
    "## Independent practice\n",
    "\n",
    "---\n",
    "\n",
    "Using either scikit-learn or statsmodels (or both, if you prefer), build a model using any set of **continuous** variables of your choice. Evaluate your model using $R^2$. Describe what the $R^2$ means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='patsy'></a>\n",
    "\n",
    "## A brief introduction to `patsy` formulas\n",
    "\n",
    "---\n",
    "\n",
    "Why slice and dice the data yourself when you just write a formula that defines your model?\n",
    "\n",
    "The `patsy` package allows you to specify the construction of your model using a formula string, and then returns the matrices required to fit the model.\n",
    "\n",
    "Let's say we wanted to predict `CRIM` from `TAX`, `AGE` and `ZN`. We would write a string formula like so:\n",
    "\n",
    "```\n",
    "formula = 'CRIM ~ TAX + AGE + ZN'\n",
    "```\n",
    "\n",
    "Then, after importing patsy, we can generate our target and predictor matrix by supplying the formula and the dataframe that contains the corresponding columns.\n",
    "\n",
    "```python\n",
    "import patsy\n",
    "\n",
    "y, X = patsy.dmatrices(formula, data=df, return_type='dataframe')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that with `return_type='dataframe'` patsy's `.dmatrices()` function returns two pandas dataframes, one for the target and one for the design matrix. \n",
    "\n",
    "You'll also notice that it creates an intercept column by default. **If you do not want it to create an intercept column, add a -1 to the formula string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# formula = 'CRIM ~ TAX + AGE + ZN -1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then feed in these matrices into statsmodels or sklearn. It is generally a good practice to convert your target matrix into a 1D vector, especially when using sklearn.\n",
    "\n",
    "> **Tip:** The `.ravel()` function for numpy arrays will \"unravel\" a multidimensional matrix into a one dimensional vector of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
