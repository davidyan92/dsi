{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Classification with K-Nearest Neighbors\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Alexander Barriga (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the difference between classification and regression models\n",
    "- Understand the K-Nearest Neighbors algorithm visually and in pseudocode\n",
    "- Explain the differences between distance metrics and explore the two most common\n",
    "- Apply KNN classification to the Wisconsin breast cancer dataset\n",
    "- Practice manually performing stratified cross-validation\n",
    "- Visually examine the effect of K neighbors on the decision boundary\n",
    "- Explain the effect of choosing K on the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction: classification vs. regression](#intro)\n",
    "- [K-Nearest Neighbors visually](#knn-visual-intro)\n",
    "- [The K-Nearest Neighbors (KNN) algorithm](#knn)\n",
    "    - [Note on parametric vs. nonparametric methods](#nonparametric)\n",
    "- [The KNN distance metric](#distance)\n",
    "    - [Euclidean distance](#euclidean)\n",
    "    - [Manhattan distance](#manhattan)\n",
    "- [Load the Wisconsin breast cancer dataset](#wisconsin)\n",
    "    - [Rename columns and subset the data](#rename-subset)\n",
    "    - [Encode the target as a binary class](#target)\n",
    "- [Examine the correlation structure of the dataset](#correlations)\n",
    "    - [Use a heatmap](#heatmap)\n",
    "    - [Use a pairplot](#pairplot)\n",
    "- [Using sklearn's `KNeighborsClassifier` and `StratifiedKFold`](#kneighborsclassifier)\n",
    "    - [Create the target and predictors](#target-predictors)\n",
    "    - [Standardize the predictor matrix](#standardize)\n",
    "    - [Write a function to manually perform the cross-validation procedure](#manual-cv)\n",
    "    - [Calculate the \"baseline\" accuracy](#baseline)\n",
    "    - [Cross-validate the mean accuracy with 5 neighbors](#cv-knn5)\n",
    "    - [Cross-validate the mean accuracy with 1 neighbor](#cv-knn5)\n",
    "- [Visualize the knn decision boundary](#visualize-knn)\n",
    "- [How is bias-variance affected by number of neighbors?](#bias-variance)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction: regression vs. classification\n",
    "\n",
    "We've discussed the difference between continuous and discrete numbers. We've predicted continuous numbers using regression. What about discrete?\n",
    "\n",
    "Think back to the wine quality dataset we used in the past. We used Linear Regression to predict the quality ranging from 0-10. What if we just wanted to predict whether wine was good or bad? Red or white? \n",
    "\n",
    "Classification algorithms do just that; they predict categories, or classes. Split the data into groups and place new data into those groups. \n",
    "\n",
    "![](http://ipython-books.github.io/images/ml.png \"Best Split vs Best Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn-visual-intro'></a>\n",
    "\n",
    "---\n",
    "\n",
    "### K-Nearest Neighbors (KNN) visually\n",
    "\n",
    "**KNN works similarly to how we humans might choose to classify things. Below we have some red and blue dots:**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_reds_and_blues.png \"Some Dots\")\n",
    "\n",
    "**A new dot appears without a color and we need to decide which color it is most likely going to be.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point.png \"A New Dot Appears\")\n",
    "\n",
    "**We compare it to its three nearest neighbors â€“ its neighbors are more often red, so we label it red.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred.png \"3 Nearest Neighbors\")\n",
    "\n",
    "**What if we increase the number of neighbors to consider to 5?**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred_blue.png \"5 Nearest Neighbors\")\n",
    "\n",
    "**This is in essence the K-Nearest Neighbors (KNN) algorithm. The K represents the number of \"neighbors\" you use.**\n",
    "\n",
    "> ***Images above credited to the yhat blog.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>\n",
    "\n",
    "## The KNN algorithm\n",
    "\n",
    "---\n",
    "\n",
    "K-Nearest Neighbors takes a different approach to modeling than we have been practicing with linear models. In order to estimate a value (regression) or class membership (classification), the algorithm finds the observations in its training data that are \"nearest\" to the observation to predict. It then averages or takes a vote of those training observations' target values to estimate the value for the new data point.\n",
    "\n",
    "Distance is usually calculated using the euclidean distance. The \"K\" in KNN refers to the number of nearest neighbors that will be contributing to the prediction. \n",
    "\n",
    "Today we will be looking at KNN only in the context of classification.\n",
    "\n",
    "**The KNN can be concisely represented with pseudocode:**\n",
    "\n",
    "```\n",
    "for unclassified_point in sample:\n",
    "    for known_point in known_class_points:\n",
    "        calculate distances (euclidean or other) between known_point and unclassified_point\n",
    "    for k in range of specified_neighbors_number:\n",
    "        find k_nearest_points in known_class_points to unclassified_point\n",
    "    assign class to unclassified_point using \"votes\" from k_nearest_points\n",
    "```\n",
    "\n",
    "> **Note**: in the case of ties, sklearn's `KNeighborsClassifier()` will just choose the first class (when weights are uniform)! If this is unappealing to you you can change the weights keyword argument to 'distance'. More on this later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='nonparametric'></a>\n",
    "\n",
    "### Note on parametric vs. nonparametric methods\n",
    "\n",
    "Thus far, all of our tests and methods have been **parametric**. That is, we have assumed a certain distribution for our data. In linear regression our parameters are the coefficients in our model, and our estimate of the target is calculated from these parameters.\n",
    "\n",
    "There are alternatives in the case where we cannot assume a particular distribution for our data or choose not to. These methods are **nonparametric** When we make no assumptions about the distribution for our data, we call our data nonparametric. For nearly every parametric test, there is a nonparametric analog available. The KNN model is an example of a nonparametric model. You can see that there are no coefficients for the different predictors and our estimate is not represented by a formula of our predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='distance'></a>\n",
    "## The KNN distance metric\n",
    "\n",
    "---\n",
    "KNN typically uses one of two distance metrics: euclidean or manhattan. Other distance metrics are possible, but more rare (sometimes it makes sense to create your own distance function.\n",
    "\n",
    "<a id='euclidean'></a>\n",
    "### Euclidean distance\n",
    "\n",
    "Recal the famous Pythagorean Theorem\n",
    "![Alt text](http://ncalculators.com/images/pythagoras-theorem.gif)\n",
    "\n",
    "We can apply the theorem to calculate distance between points. This is called Euclidean distance. \n",
    "\n",
    "![Alt text](http://rosalind.info/media/Euclidean_distance.png)\n",
    "\n",
    "### $$\\text{Euclidean  distance}=\\sqrt{(x_1-x_2)^2+(y_1-y_1)^2}$$\n",
    "\n",
    "There are many different distance metrics, but Euclidean is the most common (and default in sklearn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='manhattan'></a>\n",
    "### Manhattan distance (A.K.A Taxicab Distance)\n",
    "\n",
    "Another way to measure distance between two points is to take the sum of the absolute value of their differences. \n",
    "\n",
    "### $$ D = \\sum_{i=1}^n | x_i - y_i | $$\n",
    "\n",
    "The name Manhattan distance comes from the fact that taxicabs in Manhattan must drive from point A to point B on streets that force traffic to flow forward or backwards and left or right -- but never diagonally. \n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Manhattan_distance_bgiu.png/261px-Manhattan_distance_bgiu.png)\n",
    "![](https://pbs.twimg.com/media/CgIlqLTWEAAedKB.jpg)\n",
    "\n",
    "**Note that the Manhattan distance is a less common choice.**\n",
    "- Manhattan distance is more restrictive than Euclidean distance in how distance is measured\n",
    "- [Manhattan distance comes from $L_{p = 1}$ space and Euclidean distance comes from $L_{p = 2}$ space.](https://en.wikipedia.org/wiki/Lp_space)\n",
    "- In practice, we can cross-validate KNN using both types of distances to see which performs best. \n",
    "\n",
    "![](http://www.improvedoutcomes.com/docs/WebSiteDocs/image/diagram_euclidean_manhattan_distance_metrics.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wisconsin'></a>\n",
    "\n",
    "## Load the Wisconsin breast cancer dataset\n",
    "\n",
    "---\n",
    "\n",
    "Below we will be testing out the KNN classification algorithm on the classic [Wisconsin breast cancer dataset](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)\n",
    "\n",
    "> **Note:** (The file as suffix '.data' but is actually formatted as a .csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bcw = pd.read_csv('./datasets/wdbc.data', header=None, index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='rename-subset'></a>\n",
    "### Rename columns and subset the data\n",
    "\n",
    "The attributes below are the columns of the dataset.\n",
    "\n",
    "The column names are taken from the dataset info file. For more information check out the information file:\n",
    "\n",
    "`../assets/datasets/wdbc.names`\n",
    "\n",
    "You can open it with a text editor of your choice.\n",
    "\n",
    "      Attribute                     \n",
    "    --------------------------------------------\n",
    "    1. Sample code number [subject ID]\n",
    "    2. Class\n",
    "    3. Cell nucleus mean radius\n",
    "    4. Cell nucleus SE radius\n",
    "    5. Cell nucleus worst radius\n",
    "    6. Texture mean\n",
    "    7. Texture SE\n",
    "    8. Texture worst\n",
    "    9. Perimeter mean\n",
    "    10. Perimeter SE\n",
    "    11. Perimeter worst\n",
    "    12. Area mean\n",
    "    13. Area SE\n",
    "    14. Area worst\n",
    "    15. Smoothness mean\n",
    "    16. Smoothness SE\n",
    "    17. Smoothness worst\n",
    "    18. Compactness mean\n",
    "    19. Compactness SE\n",
    "    20. Compactness worst\n",
    "    21. Concavity mean\n",
    "    22. Concavity SE\n",
    "    23. Concavity worst\n",
    "    24. Concave points mean\n",
    "    25. Concave points SE\n",
    "    26. Concave points worst\n",
    "    27. Symmetry mean\n",
    "    28. Symmetry SE\n",
    "    29. Symmetry worst\n",
    "    30. Fractal dimension mean\n",
    "    31. Fractal dimension SE\n",
    "    32. Fractal dimension worst\n",
    "   \n",
    "**Using the provided list, reassign the column names in the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = ['id','malignant',\n",
    "                'nucleus_mean','nucleus_se','nucleus_worst',\n",
    "                'texture_mean','texture_se','texture_worst',\n",
    "                'perimeter_mean','perimeter_se','perimeter_worst',\n",
    "                'area_mean','area_se','area_worst',\n",
    "                'smoothness_mean','smoothness_se','smoothness_worst',\n",
    "                'compactness_mean','compactness_se','compactness_worst',\n",
    "                'concavity_mean','concavity_se','concavity_worst',\n",
    "                'concave_pts_mean','concave_pts_se','concave_pts_worst',\n",
    "                'symmetry_mean','symmetry_se','symmetry_worst',\n",
    "                'fractal_dim_mean','fractal_dim_se','fractal_dim_worst']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the columns pertaining to the standard deviation and \"worst\" measurements, leaving only the mean measurement columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='target'></a>\n",
    "### Encode the target class variable `malignant` to be a binary 0 vs. 1\n",
    "\n",
    "The `malignant` class target variable is coded as \"B\" for benign and \"M\" as malignant. \n",
    "\n",
    "We need to recode this to a binary integer for classification:\n",
    " - Encode malign as 1\n",
    " - Encode benign as 0\n",
    " \n",
    "Malign is assigned to 1 because our goal is to predict malign tumors with the data. In binary classification problems the category \"of interest\" to predict is typically encoded as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='correlations'></a>\n",
    "## Examine the correlation structure of the dataset\n",
    "\n",
    "---\n",
    "\n",
    "You should exclude the `id` column as this is just an indicator variable for the subject.\n",
    "\n",
    "<a id='heatmap'></a>\n",
    "### Method 1: plot a heatmap of the correlation matrix\n",
    "\n",
    "Plot a seaborn heatmap of the correlation matrix to visually examine which variables are correlated and anti-correlated, and to what degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pairplot'></a>\n",
    "### Method 2: Use seaborn's pairplot to visualize relationships between variables\n",
    "\n",
    "When you have a small number of predictor variables, seaborn's `pairplot` function will give you a more detailed visual look at the relationships between variables. The pairplot is similar to a correlation matrix, but displays scatterplots of variable pairs. Along the diagonal line are histograms showing the distribution of each variable.\n",
    "\n",
    "One of the most appealing aspects of the pairplot function for classification tasks is that the scatterplots and histograms can be split along a hue variable. If we use the `malignant` target class as the hue we are able to see how the classes are distributed across these variables as well.\n",
    "\n",
    "Plot data using seaborn's `pairplot()` function. The hue will be the class variable \"malignant\". The variables will be the other columns excluding, of course, the subject ID column. This function can take some time to run.\n",
    "\n",
    "> **Note:** Most of these predictors are highly correlated with the \"class\" variable. This is already an indication that our classifier is very likely to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kneighborsclassifier'></a>\n",
    "\n",
    "## Using sklearn's `KNeighborsClassifier` and `StratifiedKFold`\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how the sklearn KNN classifier performs on our dataset predicting the malignant target class using cross-validation.\n",
    "\n",
    "Load the KNN classifier like so:\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "\n",
    "**We are going to set some arguments when instantiating the model:**\n",
    "1. **n_neighbors** specifies how many neighbors will vote on the class\n",
    "2. **weights** uniform weights indicate that all neighbors have the same weight\n",
    "3. **metric** and **p**: when distance is minkowski (the default) and p == 2 (the default), _this is equivalent to the euclidean distance metric_\n",
    "\n",
    "Also load sklearn's `StratifiedKFold` from the `model_selection` module:\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "```\n",
    "\n",
    "The `StratifiedKFold` object will return cross-validation _indices_ which you can use to subset your data (in a for loop, for example) that runs the model and tests it. \n",
    "\n",
    "This is the **stratified** version of the `KFold` class. Stratification ensures that there are equal proportions the predicted class in each train-test fold. This is a best practice in classification tasks.\n",
    "\n",
    "> **Note:** The `cross_val_score` can also stratify for you. However, you should get familiar with using indices for cross-validation on data. Being able do cross-validation at a more \"manual\" level allows for a lot more power and customization. It also reinforces what is happening in your head during cross-validation, since you have to divide up the data yourself with the indices!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='target-predictors'></a>\n",
    "### Create your target vector and predictor matrix\n",
    "\n",
    "The target should be the binary `malignant` column. The predictors are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardize'></a>\n",
    "\n",
    "### Standardize the predictor matrix\n",
    "\n",
    "Standardization should be done for the predictors when using a KNN model. Why? \n",
    "\n",
    "Remember that KNN finds the nearest neighbors according to a distance metric. If the predictors are left unstandardized, then it is possible that some predictors will have an unfair impact on the distance measure simply because they are on a larger scale than the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-inds'></a>\n",
    "### Create the cross-validation indices using `StratifiedKFold`\n",
    "\n",
    "`StratifiedKFold` takes is instantiated with `n_splits` number of train-test pairs desired.\n",
    "\n",
    "The built-in `.split()` function will take a predictor matrix and target array and return the training and testing indices.\n",
    "\n",
    "> **Note:** The `split` function will return a python *generator*. This can be iterated, but works differently from a list in that once iterated once it will be \"empty\". You can convert the output to a list using a list comprehension if you need to use the indices multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='manual-cv'></a>\n",
    "### Write a function to manually perform cross-validation using your stratified indices\n",
    "\n",
    "Now that we have the indices (row indexes for our train-test splits), write a function that will:\n",
    "- Split the X and y into training and testing subsets\n",
    "- Fit a KNN classifier on the training set\n",
    "- Calculate the accuracy of the classifier on the test set\n",
    "- Store the accuracies for each fold and return them as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "### Calculate the \"baseline\" accuracy\n",
    "\n",
    "Before we can evaluate whether our classifier's accuracy is good or bad, we need to know the baseline accuracy.\n",
    "\n",
    "**The baseline accuracy is the proportion of the majority class.**\n",
    "\n",
    "For a binary classification, this means that the baseline accuracy is the percent of the dataset that is labeled malignant or benign, depending on whichever of malignant or benign is greater. This can be calculated:\n",
    "\n",
    "```python\n",
    "baseline = np.mean(y)  # if np.mean(y) is >= 0.5\n",
    "baseline = 1. - np.mean(y) # if np.mean(y) is < 0.5\n",
    "```\n",
    "\n",
    "**It is critical that you know your baseline accuracy!**\n",
    "\n",
    "If your dataset for example had 95 1's and 5 0's, and you got a 95% accuracy using KNN, if you had not looked at your baseline accuracy you may conclude that your classifier is doing great. In fact, it's doing no better than chance! The classifier could have guessed only 1's and gotten a 95% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn5'></a>\n",
    "### Cross-validate the mean accuracy for a KNN model with 5 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn1'></a>\n",
    "### Cross-validate the mean accuracy for a KNN model with 1 neighbor\n",
    "\n",
    "As you can see the mean cross-validated accuracy is very high with 5 neighbors. \n",
    "\n",
    "Let's see what it's like when we use only 1 neighbor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualize-knn'></a>\n",
    "\n",
    "## Visualize the KNN decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "Even with 1 neighbor we can do quite well predicting the malignant observations.\n",
    "\n",
    "Below you can load an interactive KNN visualization that shows how the decision boundary of KNN changes as the number of neighbors changes.\n",
    "\n",
    "The `KNNBoundaryPlotter` class has 4 required arguments:\n",
    "\n",
    "    KNNBoundaryPlotter(data, predictor1, predictor2, class_target)\n",
    "    \n",
    "It will by default fit a visualization of the decision boundary across 1 to 100 nearest neighbors.\n",
    "\n",
    "The boundary is where the classifier will vote for malignant vs. benign classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import imp\n",
    "# plotter = imp.load_source('plotter', './knn_plotter.py')\n",
    "# from plotter import KNNBoundaryPlotter\n",
    "\n",
    "# kbp = KNNBoundaryPlotter(bcw, 'area_mean', 'symmetry_mean', 'malignant', nn_range=range(1,101))\n",
    "\n",
    "# kbp.knn_mesh_runner()\n",
    "# kbp.knn_interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bias-variance'></a>\n",
    "### How does increasing the number of neighbors impact the bias and variance of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Scott Foreman-Roe's [breakdown](http://scott.fortmann-roe.com/docs/BiasVariance.html) (required) of the bias-variance tradeoff featuring a discussion of KNN is an excellent read\n",
    "- A [detailed discussion](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/) of KNN\n",
    "- A long, applied example of KNN applied to [image classification](http://cs231n.github.io/classification/ )\n",
    "- If academic breakdowns are your thing, be sure to visit [this](http://me.seekingqed.com/files/intro_KNN.pdf) resource\n",
    "- Read the SKLearn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on implementing KNN\n",
    "- Choosing the right [algorithm from SKLearn](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "- A deeper dive into [Euclidian distance](http://www.econ.upf.edu/~michael/stanford/maeb4.pdf)\n",
    "- Classifier comparsion from [SKLearn](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) (this is also in our [repository](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-04/4.01%20Intro%20to%20Classification/classification-methods.py))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
