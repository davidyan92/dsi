{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Explain the benefits of scaling data.\n",
    "- Identify situations where scaling data is beneficial. \n",
    "- Scale data using Python and SKLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to feature scaling](#intro)\n",
    "- [Why scale data?](#why-scale)\n",
    "- [Centering](#centering)\n",
    "- [Standardization](#standardization)\n",
    "    - [Standardizing with sklearn's `StandardScaler`](#standard-scaler)\n",
    "- [Normalization](#normalization)\n",
    "    - [Normalizing with sklearn's `MinMaxScaler`](#minmax)\n",
    "- [Independent practice scaling the wine dataset](#independent-practice)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to feature scaling\n",
    "\n",
    "---\n",
    "\n",
    "Scaling data is the process of increasing or decreasing the magnitude according to a fixed ratio. In other words, you change the size but not the shape of the data (the shape of the distribution is unchanged).\n",
    "\n",
    "Some data scaling methods often change the *location* of the data as well. For example, when \"centering\" we take a distribution and change it's mean to be zero by subtracting the mean of the distribution from each data point in the distribution. While this is not technically \"scaling\", changing the location is often a component of the process and preserves the shape of the data (it just shifts it around).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='why-scale'></a>\n",
    "\n",
    "## Why should we scale data?\n",
    "\n",
    "---\n",
    "\n",
    "**There are a number of good reasons why we scale our data:**\n",
    "- To handle disparities in units.\n",
    "- Cut computational expense.\n",
    "- Improve model performance (Especially Machine Learning).\n",
    "- We scale for models to prevent the steps on different axes from varying widely.\n",
    "\n",
    "**Itâ€™s rarely a bad idea to scale your data!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='centering'></a>\n",
    "\n",
    "## Centering: changing the location of data\n",
    "\n",
    "---\n",
    "\n",
    "We can start with the simplest transformation example: centering. If we have a distribution of values $X$, then to center our data to a new distribution $X_c$:\n",
    "\n",
    "### $$ X_c = X - \\bar{X} $$\n",
    "\n",
    "### Benefits of centering data\n",
    "\n",
    "The primary benefit of centering your predictor data in linear modeling is so **the intercept represents the estimate of the target when all predictors are at their mean value.**\n",
    "\n",
    "If we don't center, the intercept is the estimate of our model when all predictors are at value 0. It often makes the intercept much more interpretable when you center your predictors.\n",
    "\n",
    "### Centering example: baseball player height and weight\n",
    "\n",
    "Load in the dataset on the heights, weights, and ages of baseball players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseball = pd.read_csv('./datasets/baseball_height_weight.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the distribution of the heights and weights below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct a linear regression predicting weight from height. Interpret the value of the intercept and the coefficient from this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Center the height variable and re-run the regression with the centered height. Interpret the new intercept and coefficient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardization'></a>\n",
    "\n",
    "## Standardization\n",
    "\n",
    "---\n",
    "\n",
    "The most common method of scaling is standardization. In standardization we first center the data, then we divide by the standard devation to enforce that the standard deviation of the variable is one:\n",
    "\n",
    "### $$ X_{std} = \\frac{X - \\bar{X}}{s_{X}} $$\n",
    "\n",
    "### Benefits of standardizing data\n",
    "\n",
    "There are many benefits to standardizing our data, especially when we have more than one predictor:\n",
    "- Intercepts are interpreted as the estimate when all predictors are at their mean value.\n",
    "- Coefficients are in units of standard deviations of the original predictors. This allows for direct comparison of the magnitude of impact between different predictors.\n",
    "- Optimization methods (minimizing loss functions) are faster and more stable.\n",
    "- It is required for regularization penalties where the magnitude of coefficients for different predictors must have the same meaning.\n",
    "- In K-Nearest Neighbors methods it is necessary if you want features to contribute equally since these models use the distance between observations calculated from the features.\n",
    "- K-means clustering is affected by the scale of the data and standardizing the features will prevent variables from dominating simply based on their scale.\n",
    "- In logistic regression, neural networks, and support vector machines unscaled data can result in a disproportionate effect of some data points over others.\n",
    "\n",
    "> **Note:** In ordinary linear regression centering and scaling your variables does *not* impact the amount of variance you can account for. This is because we are only moving and and adjusting the magnitude of the distribution: the shape of the distribution does not change.\n",
    "\n",
    "### Standardization example\n",
    "\n",
    "First, plot the original height variable against the weight variable. Use seaborn's `sns.jointplot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create standardized versions of the height and weight variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the standardized weight against the height. Notice the distribution shapes and relationship between the variables is unchanged.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standard-scaler'></a>\n",
    "### Using sklearn's `StandardScaler`\n",
    "\n",
    "Sklearn comes packaged with a class `StandardScaler` that will preform the standardization on a matrix for you. \n",
    "\n",
    "Load in the package like so:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "Once instantiated, the standard scaler object has three primary methods built in:\n",
    "- `.fit(X)` will calculate the mean and standard deviations for each column of X\n",
    "- `.transform(X)` will take X and return a transformed version of X where each column is standardized according to their means and standard deviations (must have run `.fit()` first).\n",
    "- `.fit_transform(X)` combines the `.fit()` method and the `.transform()` method.\n",
    "\n",
    "**Use `StandardScaler` to standardize a predictor matrix containing height and weight from the baseball data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a linear regression predicting age from the standardized height and weight data. Interpret the coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='normalization'></a>\n",
    "\n",
    "## Normalization\n",
    "\n",
    "---\n",
    "\n",
    "Normalization most often refers to the process of \"normalizing\" a variable to be between 0 and 1. Think of this as squishing the variable to be constrained to a specific range.\n",
    "\n",
    "### $$ X_{norm} = \\frac{X - min(X)}{max(X) - min(X)} $$\n",
    "\n",
    "This type of normalization is typically referred to as \"min-max scaling\". \n",
    "\n",
    "### Benefits of normalization\n",
    "\n",
    "Typically standardization is preferred to min-max normalization. However, there are some applications where min-max scaling would be preferable:\n",
    "- Neural networks often require their inputs to be bounded between 0 and 1. \n",
    "- In images, for example, where pixels can only take on a specific range of RGB values, data may have to be normalized.\n",
    "\n",
    "<a id='minmax'></a>\n",
    "### Normalization with `MinMaxScaler`\n",
    "\n",
    "Sklearn also has a class for normalization called `MinMaxScaler`:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "```\n",
    "\n",
    "The `MinMaxScaler` has the same `fit()`, `transform()`, and `fit_transform()` methods.\n",
    "\n",
    "**Normalize the age, height, and weight variables using `MinMaxScaler`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the min and max ranges for the normalized matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the normalize height against the normalized weight.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='independent-practice'></a>\n",
    "\n",
    "## Independent practice: scaling the wine dataset\n",
    "\n",
    "---\n",
    "\n",
    "Below you'll load in the wine quality dataset. This dataset contains a variety of features for different types/brands of wine. \n",
    "\n",
    "**You should:**\n",
    "1. Load and examine the data.\n",
    "2. Create a target variable for wine quality.\n",
    "3. Create a predictor matrix with variables of your choice.\n",
    "4. Create a standardized and normalized version of your predictor matrix.\n",
    "5. Using cross-validation, calculate the average $R^2$ score for wine quality using the original predictors, the standardized predictors, and the normalized predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load and examine the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create a target variable for wine quality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Create a predictor matrix with variables of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Create a standardized and normalized version of your predictor matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Using cross-validation, calculate the average $R^2$ score for wine quality using the original predictors, the standardized predictors, and the normalized predictors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "[About feature scaling and normalization.](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
