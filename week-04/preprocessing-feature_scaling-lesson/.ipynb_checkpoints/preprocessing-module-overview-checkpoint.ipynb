{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Preprocessing Libraries in sklearn\n",
    "\n",
    "_Authors: Richard Harris (CHI)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Get an overview of pre-processing modules in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Intro to Preprocessing Modules in Sklearn](#intro-to-preprocessing-modules-in-sklearn)\n",
    "\t- [What's happening with fit, transform and fit_transform?](#whats-happening-with-fit-transform-and-fittransform)\n",
    "- [Different Modules in sklearn](#different-modules-in-sklearn)\n",
    "\t- [Binarizer](#binarizer)\n",
    "\t- [FunctionTransformer](#functiontransformer)\n",
    "\t- [Imputer](#imputer)\n",
    "\t- [LabelBinarizer](#labelbinarizer)\n",
    "\t- [PolynomialFeatures](#polynomialfeatures)\n",
    "\t- [Scalers](#scalers)\n",
    "- [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro-to-preprocessing-modules-in-sklearn\"></a>\n",
    "## Intro to Preprocessing Modules in Sklearn\n",
    "\n",
    "As we've seen, creating custom transformers for every one of our columns can be a little time consuming. Sklearn provides a number of libraries that preprocess your data. We've used a few of them as well.\n",
    "\n",
    "For modeling techniques in sklearn, all libraries come standard with the following methods:\n",
    "\n",
    "- `fit()`\n",
    "- `predict()`\n",
    "- `score()`\n",
    "\n",
    "Within the preprocessing libraries, we see another set of standard behavior:\n",
    "\n",
    "- `fit()`\n",
    "- `transform()`\n",
    "- `fit_transform()`\n",
    "\n",
    "This standard interface makes it easy to combine transformations within pipelines and let sklearn do much of our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"whats-happening-with-fit-transform-and-fittransform\"></a>\n",
    "### What's happening with fit, transform and fit_transform?\n",
    "\n",
    "**fit()** will plan out the steps necessary for transformation but not apply it. For example, if we use `StandardScaler()` on a column, _inside_ of our instantiated object, the following instructions are written:\n",
    "\n",
    "1. Here is the mean I will apply: *x*\n",
    "2. Here is the standard deviation I will apply: *y*\n",
    "3. When transform is called, take each value, subtract the mean I have stored, divide it by the standard deviation I have stored, return those transformed values\n",
    "\n",
    "In other words, it lets us apply the same standards across multiple sets of data!\n",
    "\n",
    "**transform()** just carries out the steps that have been stored during fitting. \n",
    "\n",
    "**fit_transform()** does both of these steps in one go -- it's what we do when we just want to transform a column right out of the gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"different-modules-in-sklearn\"></a>\n",
    "## Different Modules in sklearn\n",
    "\n",
    "We'll talk through each of the different libraries below with some sample code. The documentation and other details can be found in the sklearn API reference [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing). Libraries are presented in alpabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"binarizer\"></a>\n",
    "### Binarizer\n",
    "\n",
    "Binarizer takes a threshold and will code your data '1' if the threshold is above that threshold and '0' if the data is below that threshold.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "bin = Binarizer(20)\n",
    "print bin.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"functiontransformer\"></a>\n",
    "### FunctionTransformer\n",
    "\n",
    "This lets us take a function and apply it to the input as if it were an sklearn object. Probably best to look at it in terms of an example.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def get_first_column(numpy_array):\n",
    "    return numpy_array[:,0]\n",
    "\n",
    "ft = FunctionTransformer(get_first_column)\n",
    "ft.fit_transform(data)\n",
    "```\n",
    "\n",
    "This will let you reproducibly extract the first column out of any numpy array that you fit the transformer to. Good for custom scalers, data extraction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"imputer\"></a>\n",
    "### Imputer\n",
    "\n",
    "This processor gives you an sklearn implementation of `.fillna()` in Pandas, but within sklearn. Your choices for imputation are the average value, median value, or most frequent value. If you have a more specific imputation that you want to add, you may want to write a function and use `FunctionTransformer` instead.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "impute = Imputer(strategy='median')\n",
    "impute.fit_transform(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"labelbinarizer\"></a>\n",
    "### LabelBinarizer\n",
    "\n",
    "LabelBinarizer transforms a set of classes into binary features (yes or no). You could imagine this working similar to `pd.get_dummies()`:\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "data = np.array([1, 3, 7, 2, 4])\n",
    "\n",
    "preprocessing.LabelBinarizer().fit_transform(data)\n",
    "\n",
    "array([[1, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 1],\n",
    "       [0, 1, 0, 0, 0],\n",
    "       [0, 0, 0, 1, 0]])\n",
    "\n",
    "```\n",
    "\n",
    "This creates a column for each potential value (in numeric order) and then encodes it using 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"polynomialfeatures\"></a>\n",
    "### PolynomialFeatures\n",
    "\n",
    "This preprocessor creates polynomial terms and, optionally, interaction terms from your input. By default it will also put a _bias_ term as well. This example shows a case without the bias term.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "pf = PolynomialFeatures(3, include_bias=False)\n",
    "pf.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "array([[   1.,    1.,    1.],\n",
    "       [   2.,    4.,    8.],\n",
    "       [   3.,    9.,   27.],\n",
    "       [   4.,   16.,   64.],\n",
    "       [   5.,   25.,  125.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scalers\"></a>\n",
    "### Scalers\n",
    "\n",
    "The following sklearn scalers all rescale data according to certain methods, but are used the same way in each case:\n",
    "\n",
    "- **MinMaxScaler** - scales the data using the max and min values so that it fits between 0 and 1\n",
    "- **StandardScaler** - scales the data so that it has mean 0 and variance of 1 \n",
    "- **RobustScaler** - scales the data similarly to Standard Scaler, but makes use of the median and scales using the interquartile range so as to avoid issues with large outliers.\n",
    "\n",
    "There are other scalers but these commonly-used and easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "\n",
    "These tools can make it very easy to do common tasks as part of a reproducible data pipeline. Every transformation here can be reapplied to new data as it comes in. It's not necessarily the most user-friendly system for iterating during model generation, but can come in very handy when creating a data _pipeline_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
