{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Simple and Multiple Linear Regression from Scratch\n",
    "\n",
    "_Authors: Kiefer Katovich (SF) and Matt Brems (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Code simple linear regression from scratch using a simple housing price dataset\n",
    "- Understand and code the loss function MSE in regression\n",
    "- Write functions to calculate the R^2 metric\n",
    "- Understand what R^2 represents\n",
    "- Plot the regression line and predictions against the true values\n",
    "- Understand the difference between multiple linear regression and simple linear regression\n",
    "- Derive the beta coefficients in MLR using linear algebra\n",
    "- Construct an MLR, calculate the coefficients manually, and evaluate the R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Load the real estate data](#load-data)\n",
    "- [Build a simple linear regression](#build-slr)\n",
    "    - [Define the target and predictor variables](#target-predictor)\n",
    "    - [Code prediction function](#pred-func)\n",
    "    - [Code regression plotting function](#plot-regline)\n",
    "    - [Code function to calculate residuals](#calc-resids)\n",
    "    - [Code function to calculate SSE](#calc-sse)\n",
    "    - [Minimize the SSE](#minimize-sse)\n",
    "- [R2: the coefficient of determination](#r2)\n",
    "- [From SLR to MLR](#slr-to-mlr)\n",
    "- [Assumptions of MLR](#assumptions)\n",
    "- [Fitting a MLR](#fit-mlr)\n",
    "    - [Deriving the MLR coefficients with linear algebra](#mlr-beta-derivation)\n",
    "    - [Code the MLR fit](#code-mlr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-data'></a>\n",
    "\n",
    "## Load the real estate data\n",
    "\n",
    "---\n",
    "\n",
    "Over the course of this lesson we will be constructing a simple linear regression and then extend this to multiple linear regression. Included in the datasets folder is a very simple dataset on real estate prices.\n",
    "\n",
    "**Load the data using pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_csv = './datasets/housing-data.csv'\n",
    "\n",
    "# load data with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "\n",
    "    sqft: the size of the house in sq. ft\n",
    "    bdrms: number of bedrooms\n",
    "    age: age in years of house\n",
    "    price: the price of the house\n",
    "    \n",
    "**Convert `price` to be in units of 1000 (thousands of dollars).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform price to new units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='build-slr'></a>\n",
    "\n",
    "## Build a SLR: estimating `price` with `sqft`\n",
    "\n",
    "---\n",
    "\n",
    "We will start by constructing the simple linear regression. Below is the formulation for the SLR and our specific model of interest:\n",
    "\n",
    "### $$ y = \\beta_0 + \\beta_1 x + \\epsilon \\\\\n",
    "\\text{price} = \\beta_0 + \\beta_1 \\text{sqft} + \\epsilon$$\n",
    "\n",
    "> $\\beta_0$: the intercept\n",
    "\n",
    "Without the intercept term the regression line would always have to pass through the origin, which is almost never an optimal way to represent the relationship between our target and predictor variable.\n",
    "\n",
    "> $\\beta_1$: the coefficient on $x$ \n",
    "\n",
    "We intend to estimate the values of $y$ from $x$. Each value of $x$ is multiplied by the same coefficient, which is why linear regression models model a _linear_ relationship between our predictor and target variable.\n",
    "\n",
    "Recall that a 1 unit increase in $x$ will correspond to a $\\beta_1$ unit increase in $y$ according to our model.\n",
    "\n",
    "> $\\epsilon_1$: the error (residuals)\n",
    "\n",
    "This is the difference between the predicted and true values that are unexplained by $x$ in the regression.\n",
    "\n",
    "---\n",
    "\n",
    "<a id='target-predictor'></a>\n",
    "\n",
    "### Define the target and predictor variables\n",
    "\n",
    "Extract the target variable and predictor variable from our pandas dataframe. Classically, target and predictor are referred to dependent and independent variables. There are many different terms for what $x$ and $y$ represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define predictor and target as variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred-func'></a>\n",
    "\n",
    "### Build a function to predict $\\hat{y}$ given $x$\n",
    "\n",
    "Build a function to represent the formula below:\n",
    "\n",
    "### $$\\hat{y} = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "**Note:** I have removed the error term from the equation. Obviously we do not know the error or we would be able to model $y$ perfectly. We can assume that our prediction $\\hat{y}$ is an imperfect estimation of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot-regline'></a>\n",
    "\n",
    "### Write a function to plot a regression line\n",
    "\n",
    "Your function should:\n",
    "- Accept $\\beta_0$, $\\beta_1$, $x$, and $y$ as arguments\n",
    "- Calculate the predicted values $\\hat{y}$ given $x$ (using the function you wrote above)\n",
    "- Plot the original points\n",
    "- Plot the predicted points (in a different color)\n",
    "- Plot the regression line defined by the slope and intercept\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to plot regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use your function with $\\beta_0 = 0$ and $\\beta_1 = 1$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='calc-resids'></a>\n",
    "\n",
    "### Write a function to calculate residuals\n",
    "\n",
    "Recall that the residuals are simply the error of the model:\n",
    "\n",
    "### $$ \\text{residual}_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "Where $y_i$ is the true value of our target at this observation $i$, and $\\hat{y}_i$ is the predicted value of our target. Simple enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='calc-sse'></a>\n",
    "\n",
    "### Write a function to calculate the sum of squared errors (SSE)\n",
    "\n",
    "Simple linear regression can use \"ordinary least squares\" method for identifying linear relations between variables. Here the [\"least squares\"](https://www.mathworks.com/help/optim/ug/least-squares-model-fitting-algorithms.html) means that it _minimizes the sum of the squared residuals._\n",
    "\n",
    "\n",
    "> **Aside:** Why the squared residuals instead of just the absolute value of the residuals? Well, both can be used â€“ absolute value of residuals is often used when there are large outliers or other abnormalities in variables. [Solving for the least absolute deviations (LAD)](https://en.wikipedia.org/wiki/Least_absolute_deviations) is a type of \"robust\" regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the sum of squared errors from your initial regression with $\\beta_0 = 0$ and $\\beta_1 = 1$ using the functions we defined earlier.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose a new $\\beta_0$ and $\\beta_1$ you think might be better, and calculate the SSE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot new regression, calculate new SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='minimize-sse'></a>\n",
    "\n",
    "### Minimizing the sum of squared errors\n",
    "\n",
    "Deriving the equation that minimizes the sum of squared errors in simple linear regression can be done using calculus. [See here](http://web.cocc.edu/srule/MTH244/other/LRJ.PDF) or [here](https://en.wikipedia.org/wiki/Simple_linear_regression) for descriptions of the derivation.\n",
    "\n",
    "For those familiar with calculus: **set the derivative of the loss function to 0 and solve for $\\beta_0$ and $\\beta_1$.** The loss function is \"convex\" and therefore it is at its minimum where the derivative is 0. Solving involves taking the partial derivatives for $\\beta_0$ and $\\beta_1$. \n",
    "\n",
    "The equations for the $\\beta_0$ and $\\beta_1$ that minimize the sum of squares are:\n",
    "\n",
    "### $$ \\beta_1 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y} ) (x_i - \\bar{x} )}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "### $$ \\beta_0 = \\bar{y} - \\beta_1\\bar{x} $$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of $x$ and $y$, respectively.\n",
    "\n",
    "#### Write functions below to calculate $\\beta_0$ and $\\beta_1$ based on these equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to calculate betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the optimal $\\beta_1$ and $\\beta_0$ using your functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the regression with the optimal betas and calculate the SSE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot best fit regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='r2'></a>\n",
    "\n",
    "## $R^2$: the \"coefficient of determination\"\n",
    "\n",
    "---\n",
    "\n",
    "> **$R^2$ is the amount of variance explained above baseline in your target $y$ by predictor $x$**.\n",
    "\n",
    "It is comprised of two parts: the **total sum of squares** and the **residual sum of squares**.\n",
    "\n",
    "The total sum of squares is defined:\n",
    "\n",
    "### $$ SS_{tot} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2 $$\n",
    "\n",
    "The residual sum of squares you are already familiar with. It is defined:\n",
    "\n",
    "### $$ SS_{res} = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 $$\n",
    "\n",
    "$R^2$ is then calculated:\n",
    "\n",
    "### $$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "\n",
    "The total sum of squares is the **baseline model**: the amount of variance in $y$ we would explain if we were to predict each point of $y$ with just the mean of $y$, $\\bar{y}$.\n",
    "\n",
    "This is equivalent to estimating $y$ by fitting a regression with nothing but the intercept term $\\beta_0$, which becomes the mean of $y$ (the best possible estimator of $y$ using a single value):\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 = \\bar{y} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the quotient of the the $SS_{res}$ and $SS_{tot}$ decreases, the $R^2$ value gets closer to 1. While the maximum $R^2$ is 1, an $R^2$ can be infinitely negative as well.  Having a negative $R^2$ indicates that your predictive equation has greater error than the baseline mode.  \n",
    "\n",
    "_In other words, your equation is worse at representing the relationship than a horozontal line through the Y intercept._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot your regression again, with a new regression line representing the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot regression with baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the SSE for the baseline model and for the model with predictor `sqft`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the SSE for model and baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to calculate $R^2$. Print out the $R^2$ of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate R2 for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='slr-to-mlr'></a>\n",
    "\n",
    "## From simple linear regression (SLR) to multiple linear regression (MLR)\n",
    "\n",
    "---\n",
    "\n",
    "The TL;DF of multiple linear regression:\n",
    "\n",
    "> Instead of using just one predictor to estimate a continuous target, we build a model with multiple predictor variables. You will be using MLR way more than SLR going forward.\n",
    "\n",
    "These variables will be represented as columns in a matrix (often a pandas dataframe).\n",
    "\n",
    "**Brainstorm some examples of real-world scenarios where multiple predictors would be beneficial. Can you think of cases where it might be detrimental?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assumptions'></a>\n",
    "\n",
    "## Assumptions of MLR\n",
    "\n",
    "---\n",
    "\n",
    "Like SLR, there are assumptions associated with MLR. Luckily, they're quite similar to the SLR assumptions.\n",
    "\n",
    "1. **Linearity:** $Y$ must have an approximately linear relationship with each independent $X_i$.\n",
    "2. **Independence:** Errors (residuals) $\\epsilon_i$ and $\\epsilon_j$ must be independent of one another for any $i \\ne j$.\n",
    "3. **Normality:** The errors (residuals) follow a Normal distribution.\n",
    "4. **Equality of Variances**: The errors (residuals) should have a roughly consistent pattern, regardless of the value of the $X_i$ predictors. (There should be no discernable relationship between the $X$ predictors and the residuals.)\n",
    "5. **Independence of Predictors**: The independent variables $X_i$ and $X_j$ must be independent of one another for any $i \\ne j$.\n",
    "\n",
    "The mnemonic LINEI is a useful way to remember these five assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-mlr'></a>\n",
    "\n",
    "## Fitting a multiple linear regression\n",
    "\n",
    "---\n",
    "\n",
    "The formula for computing the $\\beta$ values in multiple regression is best done using linear algebra. We will cover the derivation, but for more depth  [these slides are a great resource](http://statweb.stanford.edu/~nzhang/191_web/lecture4_handout.pdf).\n",
    "\n",
    "$X$ is now a _matrix_ of predictors $x_1$ through $x_i$ (with each column a predictor), and $y$ is the target vector we are seeking to estimate. There is still only 1 *estimated* variable!\n",
    "\n",
    "### $$ \\hat{y} = \\beta X$$\n",
    "\n",
    "**Note:** $\\beta$ in the formula above is a *vector* of coefficients now, rather than a single value.\n",
    "\n",
    "In different notation we could write $\\hat{y}$ calculated with:\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n",
    "\n",
    "---\n",
    "\n",
    "<a id='mlr-beta-derivation'></a>\n",
    "\n",
    "### Deriving the $\\beta$ coefficients\n",
    "\n",
    "$\\beta$ is solved with the linear algebra formula:\n",
    "\n",
    "### $$ \\beta = (X'X)^{-1}X'y $$\n",
    "\n",
    "Where $X'$ is the transposed matrix of original matrix $X$ and $(X'X)^-1$ is the inverted matrix of $X'X$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation using true $y$ is:\n",
    "\n",
    "### $$ y = \\beta X + \\epsilon $$\n",
    "\n",
    "Again, $\\epsilon$ is our vector of errors, or residuals.\n",
    "\n",
    "We can equivalently formulate this as in terms of the residuals:\n",
    "\n",
    "### $$ \\epsilon = \\beta X - y $$\n",
    "\n",
    "Our goal is to minimize the sum of squared residuals. The sum of squared residuals is equivalent to the dot product of the vector of residuals:\n",
    "\n",
    "### $$ \\sum_{i=1}^n \\epsilon_i^2 = \n",
    "\\left[\\begin{array}{cc}\n",
    "\\epsilon_1 \\cdots \\epsilon_n\n",
    "\\end{array}\\right] \n",
    "\\left[\\begin{array}{cc}\n",
    "\\epsilon_1 \\\\ \\cdots \\\\ \\epsilon_n\n",
    "\\end{array}\\right] = \\epsilon' \\epsilon\n",
    "$$\n",
    "\n",
    "Therefore we can write the sum of squared residuals as:\n",
    "\n",
    "### $$ \\epsilon' \\epsilon = (\\beta X - y)' (\\beta X - y) $$\n",
    "\n",
    "Which becomes:\n",
    "\n",
    "### $$ \\epsilon' \\epsilon = y'y - y'X\\beta - \\beta' X' y + \\beta' X' X \\beta $$\n",
    "\n",
    "Now take the derivative with respect to $\\beta$:\n",
    "\n",
    "### $$ \\frac{\\partial \\epsilon' \\epsilon}{\\partial \\beta} = \n",
    "-2X'y + 2X'X\\beta$$\n",
    "\n",
    "We want to minimize the sum of squared errors, and so we set the derivative to zero and solve for the beta coefficient vector:\n",
    "\n",
    "### $$ 0 = -2X'y + 2X'X\\beta \\\\\n",
    "X'X\\beta = X'y \\\\\n",
    "\\beta = (X'X)^{-1}X'y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='code-mlr'></a>\n",
    "\n",
    "### Code a MLR\n",
    "\n",
    "**First, we need to create the \"design matrix\" of our predictors.**\n",
    "\n",
    "The first column will be a column of all 1s (the intercept) and the other columns will be `sqft`, `bdrms`, and `age`.\n",
    "\n",
    "This is easiest to do with pandas: add a column for the intercept first, then extract the matrix using with `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up X matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for the beta coefficients\n",
    "\n",
    "We are still predicting `price`. Implement the linear algebra equation to solve for the beta coefficients. \n",
    "\n",
    "### $$ \\beta = (X'X)^{-1}X'y $$\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "The transpose of a matrix is calculated by appending `.T` to the matrix:\n",
    "\n",
    "    X.T\n",
    "\n",
    "Matrices multipled in the formula should be done with the \"dot product\":\n",
    "\n",
    "    np.dot(mat1, mat2)\n",
    "\n",
    "Inverting a matrix is done using:\n",
    "\n",
    "    np.linalg.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate beta vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm that these betas are the same as the ones using `sklearn.linear_model.LinearRegression`**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression(fit_intercept=False)\n",
    "linreg.fit(X, price)\n",
    "\n",
    "print linreg.coef_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate beta vector is same as sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate predicted $\\hat{y}$ with your $X$ predictor matrix and $\\beta$ coefficients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the $R^2$ of the multiple regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MLR R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "[Maximum likelihood estimation](https://onlinecourses.science.psu.edu/stat504/node/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
