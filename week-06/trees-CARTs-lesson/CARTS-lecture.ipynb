{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) <br>Intro to Classification and Regression Trees (CARTs)\n",
    "Week 6 | Day 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Describe what a decision tree is\n",
    "- Explain how a classification tree works\n",
    "- Explain how a regression tree works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision tree example\n",
    "\n",
    "<img src=\"http://i.imgur.com/IGebRXH.png\" width=900 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "So far in this course, we've talked about regression and classification models.\n",
    "\n",
    "In both cases, we used a **linear**, **parametric** model (with one exception).\n",
    "\n",
    "What does that mean? And what was the exception?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear\n",
    "\n",
    "$ y = \\beta_0 + \\beta_1x_1...\\beta_nx_n$\n",
    "\n",
    "\n",
    "As the value of x increases, y increases in proportion to our $\\beta$ parameter - The model is **linear in its parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parametric vs. Non-parametric Models\n",
    "\n",
    "\n",
    "**Parametric models** have some fixed number of parameters that are solved for. There is an underlying assumption about the distribution from which the data is drawn.\n",
    "\n",
    "> Check: what is the assumption about the distribution for linear regression as it is implemented in sklearn?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Non-parametric models** make no assumptions about the distribution of the underlying data. Additionally, rather than being though of as having no parameters, they can be though of as infinite in the number of parameters. The number of parameters can also vary with the size of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro to Decision Trees\n",
    "\n",
    "\n",
    "### What a Decision Tree is\n",
    "_Decision trees_ are a _non-parametric, hierarchical_ technique for both regression and classification.\n",
    "\n",
    "**_Non-parametric_** means that the model is not described in terms of parameters like for example Logistic Regression. It also means that there is no underlying assumption about the distribution of data and of the error.\n",
    "\n",
    "**_Hierarchical_** means that the model consists of a sequence of questions which yield a class label or prediction when applied to any data observation. In other words, once trained, the model behaves like a recipe, which will tell us a result if followed exactly.\n",
    "\n",
    "---\n",
    "\n",
    "[Classification CART documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "[Regression CART documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)\n",
    "\n",
    "[Decision tree user guide](http://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How it works\n",
    "\n",
    "At each decision point, or node, in our tree, we need to choose one of the answers and move forward from there. This begins at our _root node_ and continues on through each _internal node_ until we reach a _leaf node_. At this leaf node, we have the answer to our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to our example\n",
    "\n",
    "<img src=\"http://i.imgur.com/IGebRXH.png\" width=900 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Definitions:**\n",
    "- Nodes\n",
    "- Leaf nodes\n",
    "- Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some questions to ponder:\n",
    "- Why is credit history the first question?\n",
    "- Why those specific income thresholds?\n",
    "- Does the order we ask these question in matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to build a decision tree\n",
    "\n",
    "In order to build a productive and efficient decision tree we need an algorithm that is capable of determining optimal choices for each node. One such algorithm is _Hunt's algorithm_: a _greedy recursive_ algorithm that leads to a _local optimum_.\n",
    "\n",
    "- _greedy_: makes the best local decision in the hope of ultimately making the best global decision\n",
    "- _recursive_: splits task into subtasks, solves each the same way\n",
    "- _local optimum_: solution for a given neighborhood of points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to build a decision tree cont...\n",
    "\n",
    "\n",
    "The algorithm works by recursively partitioning records into smaller & smaller subsets. The partitioning decision (which feature to use and how to split) is made at each node according to a metric called _purity_. A node is said to be 100% pure when all of its records belong to a single class (or have the same value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: binary classification with classes A, B\n",
    "\n",
    "Given a set of records `r` at node `n`:\n",
    "\n",
    "1. If all records in `r` belong exclusively to class A or exclusively to class B, then `n` is a _leaf node_ and no further iteration can take place.  This is a terminal node\n",
    "<br>\n",
    "2. If however `r` contains records from both A and B, do the following:\n",
    "    - create test condition to partition the records further\n",
    "    - partition records in `r` to children according to test\n",
    "    - `n` would then be internal node, with outgoing edges to child nodes\n",
    "\n",
    "These steps are then recursively applied to each child node until they end in a leaf node (purely A or B), or a stopping criterion is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of splits\n",
    "\n",
    "Splits can be 2 way or multi-way. \n",
    "\n",
    "Features can be categorical or continuous.\n",
    "\n",
    "<img src=\"http://i.imgur.com/JOR2H8J.png\" width=600>\n",
    "<img src=\"http://i.imgur.com/KI8aBJX.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization functions\n",
    "\n",
    "Recall from the algorithm above, that we iteratively create test conditions to split the data. How do we determine the best split amongst all possible splits? Recall that no split is necessary (at a given node) when all records belong to the same class. Therefore we want each step to create the partition with the **highest possible purity**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to do this, we will need an objective function to optimize. We want our objective function to measure the gain in purity from a particular split. Therefore we want it to depend on the class distribution over the nodes (before and after the split). \n",
    "\n",
    "For example, let $p(i|t)$ be the probability of class $i$ at node $t$ (eg, the fraction of records labeled $i$ at node $t$). For a binary (0/1) classification problem, what would the values be for maximum impurity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The maximum impurity partition is given by the distribution:\n",
    "$$\n",
    "p(0|t) = p(1|t) = 0.5\n",
    "$$\n",
    "\n",
    "where both classes are present in equal manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "On the other hand, the minimum impurity partition is obtained when only one class is present, i.e:\n",
    "$$\n",
    "p(0|t) = 1 $$\n",
    "$$p(1|t) = 0$$\n",
    "$$p(0|t) = 1 - p(1|t)\n",
    "$$\n",
    "\n",
    "Therefore in the case of a binary classification we need to define an _impurity_ function that will smoothly vary between the two extreme cases of minimum impurity (i.e. purity, where we have only one class or the other) and the maximum impurity case of equal mix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we measure \"purity\"\n",
    "\n",
    "We can define several functions that satisfy these properties. Here are three common ones:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(t) = - \\sum_{i=0}^{c-1} p(i|t)log_2 p(i|t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Gini}(t) = 1 - \\sum_{i=0}^{c-1} [p(i|t)]^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Classification error}(t) = 1 - \\max_i[p(i|t)]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://i.imgur.com/UGPGzRq.png\" width=600>\n",
    "\n",
    "<center>Note that each measure achieves its max at 0.5, min at 0 & 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One more step\n",
    "\n",
    "Impurity measures put us on the right track, but on their own they are not enough to tell us how our split will do. We still need to look at impurity before & after the split to understand how good the split is. We can make this comparison using the gain:\n",
    "$$\n",
    "\\Delta = I(\\text{parent}) - \\sum_{\\text{children}}\\frac{N_j}{N}I(\\text{child}_j)\n",
    "$$\n",
    "\n",
    "Where $I$ is the impurity measure, $N_j$ denotes the number of records at child node $j$, and $N$ denotes the number of records at the parent node. When $I$ is the entropy, this quantity is called the _information gain_.\n",
    "\n",
    "Generally speaking, a test condition with a high number of outcomes can lead to overfitting (ex: a split with one outcome per record). One way of dealing with this is to restrict the algorithm to binary splits only (CART). Another way is to use a splitting criterion which explicitly penalizes the number of outcomes (C4.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision tree regression\n",
    "\n",
    "In the case of regression, the outcome variable is not a category but a continuous value. We cannot therefore use the same measure of purity we used for classification. Instead we look to minimize the Sum of Square Errors at each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "Check: What is overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to preventing overfitting with decision trees\n",
    "\n",
    "In addition to determining splits, we also need a stopping criterion to tell us when weâ€™re done. For example, we can stop when all records in a given node belong to the same class, or when all records have the same attributes. This is correct in principle, but would likely lead to overfitting. \n",
    "\n",
    "**Pruning** - this builds the full tree and then prunes as a post-processing step. To prune a tree, we examine the nodes from the bottom-up and simplify pieces of the tree (according to some criteria). Complicated subtrees can be replaced either with a single node, or with a simpler (child) subtree.\n",
    "\n",
    "Sklearn doesn't do post-pruning. Instead you can set a maximum depth for a tree.\n",
    "\n",
    "**Minimum observations to make a split:**\n",
    "\n",
    "An alternative to maximum depth (and can be used at the same time), is to specify the minimum number of datapoints reqired to make a split at a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Decision Tree Example Example](Decision Tree Sklearn Example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='advantages'></a>\n",
    "## CART advantages\n",
    "---\n",
    "\n",
    "- Simple to understand and interpret. People are able to understand decision tree models after a brief explanation.\n",
    "    - Useful to work with non technical departments (marketing/sales).\n",
    "- Requires little data preparation. \n",
    "    - Other techniques often require data normalization, dummy variables need to be created and blank values to be removed.\n",
    "- Able to handle both numerical and categorical data. \n",
    "    - Other techniques are usually specialized in analyzing datasets that have only one type of variable.\n",
    "- Uses a **white box** model.\n",
    "    - If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic.\n",
    "    - By contrast, in a **black box** model, the explanation for the results is typically difficult to understand, for example in neural networks.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Robust. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "- Performs well with large datasets. Large amounts of data can be analyzed using standard computing resources in reasonable time.\n",
    "- Once trained can be implemented on hardware and has extremely fast execution.\n",
    "    - Real-time applications like trading, for example.\n",
    "- Performs feature selection for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='disadvantages'></a>\n",
    "## CART disadvantages\n",
    "---\n",
    "\n",
    "- Locally-optimal.\n",
    "    - Practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally-optimal decisions are made at each node. \n",
    "    - Such algorithms cannot guarantee to return the globally-optimal decision tree.\n",
    "- Overfitting.\n",
    "    - Decision-tree learners can create over-complex trees that do not generalize well from the training data.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. In such cases, the decision tree becomes prohibitively large.\n",
    "    - Neural networks, for example, are superior for these problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "- For continuous outcome variables, predictions may not be very specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice\n",
    "\n",
    "1. Walk through the visual tutorial on R2D3 [Decision Trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "2. Find a decision tree model at [BigML](https://bigml.com/gallery/models) that you find interesting. Be prepared to talk about what the model is trying to predict and how it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "In this class we learned about Classification and Regression trees. We learned how Hunt's algorithm helps us recursively partition our data and how impurity gain is useful for determining optimal splits.\n",
    "We've also reviewed pros/cons of decision trees and industry applications.\n",
    "In the lab we will learn to use them in `Scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "- [Scikit-Learn Documentation](http://scikit-learn.org/stable/modules/tree.html)\n",
    "- [Classification trees video](https://www.youtube.com/watch?v=p17C9q2M00Q)\n",
    "- [Regression trees video](https://www.youtube.com/watch?v=zvUOpbgtW3c)\n",
    "- [Decision trees on wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "- [Decision tree regression explained](http://www.saedsayad.com/decision_tree_reg.htm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
