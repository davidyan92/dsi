{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Timeseries Forecasting with LSTM Neural Networks\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Learn how to build a basic LSTM using Keras.\n",
    "- Train the LSTM model on unemployment data and forecast to future data.\n",
    "- Understand the different ways to fit the LSTM model for timeseries data.\n",
    "- Understand the \"stateful\" LSTM setup and how it compares to the standard fitting procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#introduction)\n",
    "\t- [A note on the lesson content](#a-note-on-the-lesson-content)\n",
    "- [Load the Keras modules](#load-the-keras-modules)\n",
    "- [Prepare the unemployment data for timeseries modeling](#prepare-the-unemployment-data-for-timeseries-modeling)\n",
    "\t- [Create the first-order differenced unemployment rate](#create-the-first-order-differenced-unemployment-rate)\n",
    "\t- [Normalize the differenced unemployement data with `MinMaxScaler`](#normalize-the-differenced-unemployement-data-with-minmaxscaler)\n",
    "\t- [Split the timeseries into 50% train/test splits](#split-the-timeseries-into--traintest-splits)\n",
    "- [Write a function to create the predictor and target data](#write-a-function-to-create-the-predictor-and-target-data)\n",
    "\t- [Create training and testing data for a lag of 1](#create-training-and-testing-data-for-a-lag-of-)\n",
    "- [Reshape the data to work with the LSTM](#reshape-the-data-to-work-with-the-lstm)\n",
    "- [Constructing the Keras model](#constructing-the-keras-model)\n",
    "\t- [Fit the LSTM model](#fit-the-lstm-model)\n",
    "\t- [Plot the original data, the training predictions and the testing predictions](#plot-the-original-data-the-training-predictions-and-the-testing-predictions)\n",
    "- [LSTM with multiple lags as predictors](#lstm-with-multiple-lags-as-predictors)\n",
    "- [Refraing the problem using the LSTM \"time steps\" dimension](#refraing-the-problem-using-the-lstm-time-steps-dimension)\n",
    "\t- [Rebuild and fit the LSTM model, and plot the predictions](#rebuild-and-fit-the-lstm-model-and-plot-the-predictions)\n",
    "- [\"Stateful\" LSTM models](#stateful-lstm-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "Modeling timeseries and forecasting with neural networks is a growing trend. The Long Short Term Memory (LSTM) recurrent neural network architecture is a popular choice when \"context\" or memory across time is a desired capability of the model.\n",
    "\n",
    "In this walkthrough/codealong lecture we will be building an LSTM using the Keras framework to forecast stock market timeseries data. \n",
    "\n",
    "<a id=\"a-note-on-the-lesson-content\"></a>\n",
    "### A note on the lesson content\n",
    "\n",
    "This codealong focuses primarily on the Keras implementation and application of LSTM neural networks. This lecture does not cover:\n",
    "- The theory behind recurrent neural networks.\n",
    "- The mathematics or theory behind the architecture of LSTM networks.\n",
    "\n",
    "There are a variety of great resources to dive deeper into LSTM networks:\n",
    "- [A beginners guide to recurrent networks and LSTMs](http://deeplearning4j.org/lstm.html#a-beginners-guide-to-recurrent-networks-and-lstms)\n",
    "- [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [This tutorial provides a great introduction to building a simple LSTM for timeseries forecasting.](http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) which \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-the-keras-modules\"></a>\n",
    "## Load the Keras modules\n",
    "---\n",
    "\n",
    "Keras is a popular, high-level framework for neural networks in python. It is a \"wrapper\" on top of lower level backend frameworks: particularly TensorFlow and Theano.\n",
    "\n",
    "> **Note:** this lesson is written with Keras running on the TensorFlow backend instead of the Theano backend. This does, unfortunately, have an impact on the way the code is written. Theano and TensorFlow have different indexing orders. [To change the backend that Keras runs on, please see this guide in the documentation.](https://keras.io/backend/)\n",
    "\n",
    "**We need to import four classes from Keras:**\n",
    "\n",
    "- **`Sequential`** is the model that will house our neural network layers. It stacks together the input, \"hidden\" and output layers of the network.\n",
    "- **`Dense`** is a standard fully-connected neural network layer where each node as inputs/output connections to every node in the layers it is connected to.\n",
    "- **`LSTM`** is the Long Short Term Memory recurrent neural network layer. This will house our LSTM \"neurons\" or \"cells\".\n",
    "\n",
    "We are also importing `MinMaxScaler` from sklearn. Inputs to the LSTM (and neural networks in general) perform better if they are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare-the-unemployment-data-for-timeseries-modeling\"></a>\n",
    "## Prepare the unemployment data for timeseries modeling\n",
    "---\n",
    "\n",
    "First we will load quarterly US unemployment data to do some basic forecasting using LSTMs. \n",
    "\n",
    "**Load the unemployment data from CSV file and perform any required cleaning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/seasonally-adjusted-quarterly-us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-the-first-order-differenced-unemployment-rate\"></a>\n",
    "### Create the first-order differenced unemployment rate\n",
    "\n",
    "In timeseries modeling the raw timeseries is rarely used. Typically we will use the first order differenced timeseries (or further order differences). Technically this differencing is done to ensure that the timeseries is stationary. \n",
    "\n",
    "However, there are more intuitive reasons why we would want to model the differences as opposed to the actual values. Take the AAPL stock price, for example, and imagine we are a day-trader. If we hold the AAPL stock, we are of course interested in having a model that can predict the price of the stock in the future. However, what we are *really* interested in is how the stock price will *change* in the future from the current point. It is more useful to say \"the stock will go up by one point\" than it is to say \"the stock price will be 51.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"normalize-the-differenced-unemployement-data-with-minmaxscaler\"></a>\n",
    "### Normalize the differenced unemployement data with `MinMaxScaler`\n",
    "\n",
    "We want the rate to be restricted to the range -1 to 1.\n",
    "\n",
    "> **Note:** the differencing will make the first value of the series a NaN value. Make sure to drop this from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split-the-timeseries-into--traintest-splits\"></a>\n",
    "### Split the timeseries into 50% train/test splits\n",
    "\n",
    "We don't want a random train/test split in this case. With timeseries data, we are interested in how are model generalizes to future data in particular. Make the test set the second half of the data through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"write-a-function-to-create-the-predictor-and-target-data\"></a>\n",
    "## Write a function to create the predictor and target data\n",
    "---\n",
    "\n",
    "The function will need to create a Y target and X predictor. The X predictor matrix will simply be the shifted versions of Y, our target unemployment timeseries. In other words, we want our features to be previous timesteps of the target data for given lags.\n",
    "\n",
    "**Make a function with two arguments:**\n",
    "1. The timeseries data.\n",
    "2. The number of lags of the timeseries to have as predictors.\n",
    "\n",
    "The default should create a dataset where X is the unemployment rate a given time (t) and Y is the unemployment rate at the next time (t + 1). At the default lag of 1 X will just be the unemployment timeseries of Y shifted back by 1.\n",
    "\n",
    "> **Note:** make sure that the output X and Y are of the same length! You will need to slice off a row (at least - depends on the lag order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-training-and-testing-data-for-a-lag-of-\"></a>\n",
    "### Create training and testing data for a lag of 1\n",
    "\n",
    "Again, this means our X will just have 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reshape-the-data-to-work-with-the-lstm\"></a>\n",
    "## Reshape the data to work with the LSTM\n",
    "---\n",
    "\n",
    "The format of data the LSTM expects is:\n",
    "\n",
    "    [samples, time_steps, features]\n",
    "    \n",
    "Which is a 3D matrix.\n",
    "\n",
    "We have been using 2D predictor matrices for our machine learning algorithms, where our X predictor matrix has been in the form:\n",
    "\n",
    "    [samples, features]\n",
    "\n",
    "Since we are working with timeseries (which is the data an LSTM expects), we are now required to provide information about the time.\n",
    "\n",
    "You can use the `np.reshape()` command to turn your 2D X matrix into a 3D matrix that works for the LSTM matrix. We will talk about the \"time step\" dimension more down the line. \n",
    "\n",
    "> **Note:** In the case of a single lag this time step dimension is redundant. Later on, when we redesign the X matrix so that our individual features have multiple timesteps, this 3D format requirement will be clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"constructing-the-keras-model\"></a>\n",
    "## Constructing the Keras model\n",
    "\n",
    "Our LSTM model will be constructed in three parts.\n",
    "\n",
    "First initialize the sequential layer-to-layer neural network model:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "    \n",
    "Add an LSTM layer with 4 blocks/cells/neurons. We specify the `input_shape` to be the same dimensions as our features. You will notice that the `input_shape` below takes a tuple `(None, lag)`. The `None` is a placeholder for the timesteps of our features. By putting `None` we are simply avoiding specifying the timesteps.\n",
    "\n",
    "```python\n",
    "model.add(LSTM(4, input_shape=(None, lag)))\n",
    "```\n",
    "\n",
    "Add the output layer as a layer of one neuron that is fully connected to all of the previous LSTM cells:\n",
    "\n",
    "```python\n",
    "model.add(Dense(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fit-the-lstm-model\"></a>\n",
    "### Fit the LSTM model\n",
    "\n",
    "We can fit the model with these commands:\n",
    "\n",
    "```python\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=100, batch_size=1, verbose=2)\n",
    "```\n",
    "\n",
    "Which will use the squared error loss (regression) and fit the data over 100 \"epochs\", or passes over the training data. It makes multiple passes because these LSTM neural networks learn according to a learning rate (which we have not specified).\n",
    "\n",
    "The `optimizer='adam'` selects the type of algorithm for gradient descent. The Adam optimizer performs well and is often recommended for many types of neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot-the-original-data-the-training-predictions-and-the-testing-predictions\"></a>\n",
    "### Plot the original data, the training predictions and the testing predictions\n",
    "\n",
    "You can predict from a Keras model much like with a sklearn model:\n",
    "\n",
    "```python\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "```\n",
    "\n",
    "Make sure you lag the data forward for training and testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lstm-with-multiple-lags-as-predictors\"></a>\n",
    "## LSTM with multiple lags as predictors\n",
    "---\n",
    "\n",
    "We can instead predict the unemployment rate from not just the rate prior, but the `t-1`, `t-2`, and `t-3` rates.\n",
    "\n",
    "You can use the function you wrote above to construct a new X and y where X now has 3 predictors according to the different lags.\n",
    "\n",
    "Create the new Y and X variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refraing-the-problem-using-the-lstm-time-steps-dimension\"></a>\n",
    "## Refraing the problem using the LSTM \"time steps\" dimension\n",
    "---\n",
    "\n",
    "Recall that our X matrix is converted to the form:\n",
    "\n",
    "    [samples, time steps, features]\n",
    "    \n",
    "In the model we just made, we were saying that we had 3 different features, each of 1 time step long. This works fine, but it would be more appropriate to say that we had 1 feature with three different time steps, since that's what the data actually is (the unemplyment rate lagged to different degrees).\n",
    "\n",
    "Instead of reshaping our data where the time step dimension is 1, we can instead reshape it so that the feature dimension is 1 and the time step dimension is 3, like so:\n",
    "\n",
    "```python\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "```\n",
    "\n",
    "In this toy example, this will for all intents and purposes be the same, but it is more appropriate to specify it this way since the variable is the same.\n",
    "\n",
    "Recreate your training and testing data but reshaping your lags on the time dimension rather than the feature dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rebuild-and-fit-the-lstm-model-and-plot-the-predictions\"></a>\n",
    "### Rebuild and fit the LSTM model, and plot the predictions\n",
    "\n",
    "You will need to change the `input_shape` now to be `input_shape=(lag, 1)`, which indicates we have \"lag\" number of timesteps and 1 predictor/feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"stateful-lstm-models\"></a>\n",
    "## \"Stateful\" LSTM models\n",
    "---\n",
    "\n",
    "There is another even more appropriate way we can construct this LSTM model. Right now we are restricting the model to use the previous 3 timesteps only, but wouldn't it be better to allow the model to \"remember\" *all* of the previous timesteps? How would we do this?\n",
    "\n",
    "LSTM models in Keras can be set as \"stateful\". This means that instead of resetting their internal state after each training batch, the internal state of the neurons is maintained. We can use this to our advantage to have the LSTM maintain a memory of all the previous timesteps as it fits:\n",
    "1. Build the X and y data with lag 1.\n",
    "2. Reshape the data to be 3D.\n",
    "3. Set the batch size to 1 (this means that only 1 observation will be fed into the model at a time.)\n",
    "4. Construct the LSTM model with a stateful LSTM layer.\n",
    "5. Fit the model multiple times setting `nb_ephoch=1`, `shuffle=False`, and `batch_size=1`. This will go through the observations sequentially till the end, feeding them into the model one by one.\n",
    "6. At the end of each iteration through all the data, call `model.reset_states()` to reset the internal state manually for the next iteration through.\n",
    "\n",
    "The code to do this is outlined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
