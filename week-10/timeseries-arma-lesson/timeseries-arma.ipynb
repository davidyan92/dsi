{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Timeseries Properties, Autoregressive and Moving Average Models\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- \"Rolling\" statistics through time, such as the rolling mean.\n",
    "- Exponentially weighted statistics.\n",
    "- Differences and stationarity.\n",
    "- Autocorrelation and partial autocorrelation.\n",
    "- Autoregressive models.\n",
    "- Moving average models.\n",
    "- ARMA and ARIMA models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Load the unemployment data](#load-the-unemployment-data)\n",
    "- [Create a datetime index](#create-a-datetime-index)\n",
    "- [Visually examine the unemployment rate](#visually-examine-the-unemployment-rate)\n",
    "- [\"Rolling\" functions](#rolling-functions)\n",
    "\t- [Parameters for `rolling` functions](#parameters-for-rolling-functions)\n",
    "- [The expanding mean](#the-expanding-mean)\n",
    "- [Exponentially weighted windows](#exponentially-weighted-windows)\n",
    "- [\"Differencing\" a timeseries and stationarity](#differencing-a-timeseries-and-stationarity)\n",
    "- [Autocorrelation and the autocorrelation function (ACF)](#autocorrelation-and-the-autocorrelation-function-acf)\n",
    "\t- [Computing autocorrelation](#computing-autocorrelation)\n",
    "\t- [Autocorrelation using statsmodels](#autocorrelation-using-statsmodels)\n",
    "- [Partial autocorrelation and the partial autocorrelation function (PACF)](#partial-autocorrelation-and-the-partial-autocorrelation-function-pacf)\n",
    "- [Autoregressive (AR) models](#autoregressive-ar-models)\n",
    "- [Moving Average (MA) models](#moving-average-ma-models)\n",
    "- [ARMA and ARIMA models](#arma-and-arima-models)\n",
    "- [How to choose the right `p` and `q` parameters.](#how-to-choose-the-right-p-and-q-parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-the-unemployment-data\"></a>\n",
    "<a id=\"load-the-unemployment-data\"></a>\n",
    "## Load the unemployment data\n",
    "---\n",
    "\n",
    "This is historical quarterly unemployment data in the US. Do any required cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datasets/seasonally-adjusted-quarterly-us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-a-datetime-index\"></a>\n",
    "<a id=\"create-a-datetime-index\"></a>\n",
    "## Create a datetime index\n",
    "---\n",
    "\n",
    "This is quarterly data and so converting to datetime is a little bit tricky. The `.dt.to_period('Q')` will help us represent the string as a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visually-examine-the-unemployment-rate\"></a>\n",
    "<a id=\"visually-examine-the-unemployment-rate\"></a>\n",
    "## Visually examine the unemployment rate\n",
    "---\n",
    "\n",
    "Make a plot of the unemployment rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rolling-functions\"></a>\n",
    "<a id=\"rolling-functions\"></a>\n",
    "## \"Rolling\" functions\n",
    "---\n",
    "\n",
    "With timeseries we can \"roll\" statistics across time. For example, the rolling mean is the mean of a moving window across the time periods. Pandas has a lot of functionality to create rolling statistics which we will only scratch the surface of. \n",
    "\n",
    "The syntax can be a little bit tricky at first. There is a `rolling()` function that has the statistical function chained to it. Details below:\n",
    "\n",
    "<a id=\"parameters-for-rolling-functions\"></a>\n",
    "<a id=\"parameters-for-rolling-functions\"></a>\n",
    "### Parameters for `rolling` functions\n",
    "\n",
    "**`rolling().mean()`** (as well as **`rolling().median()`**) can take these parameters:\n",
    "\n",
    "- the first is the series to aggregate\n",
    "- **`window`** is the number of days to include in the average\n",
    "- **`center`** is whether the window should be centered on the date or use data prior to that date\n",
    "- **`freq`** is on what level to roll-up the averages to (as used in **`resample`**). Either **`D`** for day, **`M`** for month or **`A`** for year, etc.\n",
    "\n",
    "\n",
    "\n",
    "> **Note:** For more information see: http://stackoverflow.com/questions/17001389/pandas-resample-documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the rolling mean of years with a `window=3` and without centering.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the dates from the index as timestamps.**\n",
    "\n",
    "> *Hint: the `.to_timestamp()` function lets you extract the timestamps.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the timestamps you extracted to label the index of your plot.**\n",
    "\n",
    "Plot both the original unemployment rate values as well as the rolling mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the rolling median with `window=5` and `window=15`. Plot both together.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-expanding-mean\"></a>\n",
    "<a id=\"the-expanding-mean\"></a>\n",
    "## The expanding mean\n",
    "---\n",
    "\n",
    "The \"expanding mean\" simply uses all datapoints up to the current time to calculate the mean, as opposed to a moving window.\n",
    "\n",
    "**Calculate and plot the expanding mean below. Resample by quarter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exponentially-weighted-windows\"></a>\n",
    "<a id=\"exponentially-weighted-windows\"></a>\n",
    "## Exponentially weighted windows\n",
    "---\n",
    "\n",
    "Exponentially weighted windows are one of the most common and effective ways of averaging out noise in timeseries data. The averaging is done with an \"exponential decay\" on the contribution of prior means, decreasing the contribution of timepoints further in the past.\n",
    "\n",
    "The (adjusted) exponentially weighted mean for time $t$ is defined as:\n",
    "\n",
    "<a id=\"-xt--fracxt-----alphaxt------alphaxt--------alphatx------alpha-----alpha-------alphat-\"></a>\n",
    "### $$ x_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2x_{t-1} + ... + (1 - \\alpha)^{t}x_0} {1 + (1 - \\alpha) + (1 - \\alpha)^2 + ... + (1 - \\alpha)^{t}} $$\n",
    "\n",
    "> **Note:** See for more information: http://pandas.pydata.org/pandas-docs/stable/computation.html#exponentially-weighted-windows\n",
    "\n",
    "**Calculate and plot the exponentially weighted sum along with the rolling sum. What is the difference?**\n",
    "\n",
    "For example: `.resample('Q').sum().ewm(span=10).mean()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"differencing-a-timeseries-and-stationarity\"></a>\n",
    "<a id=\"differencing-a-timeseries-and-stationarity\"></a>\n",
    "## \"Differencing\" a timeseries and stationarity\n",
    "---\n",
    "\n",
    "If a time series is stationary, the mean, variance, and autocorrelation (covered in the next section) are constant over time. Forcasting methods typically assume that the timeseries you are forcasting on are stationary, or at least approximately stationary.\n",
    "\n",
    "The most common way to make a timeseries stationary is to perform \"differencing\". This procedure converts a timeseries into the difference between values:\n",
    "\n",
    "<a id=\"-delta-yt--yt---yt--\"></a>\n",
    "### $$ \\Delta y_t = y_t - y_{t-1} $$\n",
    "\n",
    "This removes trends in the timeseries and ensures that the mean across time is zero. In most cases there will only be a need for a single differencing, although sometimes a second difference (or even more) will be taken to remove trends.\n",
    "\n",
    "**Difference the unemployment rate and plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"autocorrelation-and-the-autocorrelation-function-acf\"></a>\n",
    "<a id=\"autocorrelation-and-the-autocorrelation-function-acf\"></a>\n",
    "##  Autocorrelation and the autocorrelation function (ACF)\n",
    "---\n",
    "\n",
    "In previous weeks, our analyses has been concerned with the correlation between two or more variables (height and weight, education and salary, etc.). In time series data, autocorrelation is a measure of _how correlated a variable is with itself_.\n",
    "\n",
    "Specifically, autocorrelation measures how closely related earlier values are with values occurring later in time.\n",
    "\n",
    "Examples of autocorrelation:\n",
    "\n",
    "    In stock market data the stock price at one point is correlated with the stock \n",
    "    price of the point directly prior in time. \n",
    "    \n",
    "    In sales data, sales on a Saturday are likely correlated with \n",
    "    sales on the next Saturday and the previous Saturday, as well as other days to more\n",
    "    or less extent.\n",
    "\n",
    "Below is the formula for the autocorrelation funtion (acf):\n",
    "\n",
    "$\\text{Given measurements } x_1, x_2, x_3 ... x_n \\text{ at time points } t_1, t_2, t_3 ... t_n:$\n",
    "\n",
    "### $$lag_k\\;acf() = \\frac{\\sum_{t=k+1}^{n}\\left(\\;x_t - \\bar{x}\\;\\right)\\left(\\;x_{t-k} - \\bar{x}\\;\\right)}{\\sum_{t=1}^n\\left(\\;x_t - \\bar{x}\\;\\right)^2}$$\n",
    "\n",
    "Compare this to the formula for correlation:\n",
    "\n",
    "$\\text{Given measurements } x_1, x_2, x_3 ... x_n \\text{ and measurements } y_1, y_2, y_3 ... y_n:$\n",
    "\n",
    "### $$r_{xy} = \\frac{\\sum_{i=1}^{n}\\left(\\;x_i - \\bar{x}\\;\\right)\\left(\\;y_{i} - \\bar{y}\\;\\right)}{\\sqrt{\\left(\\sum_{i=1}^{n}\\left(\\;x_i - \\bar{x}\\;\\right)^2\\sum_{i=1}^n\\left(\\;y_i - \\bar{y}\\;\\right)^2\\right)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"computing-autocorrelation\"></a>\n",
    "<a id=\"computing-autocorrelation\"></a>\n",
    "### Computing autocorrelation\n",
    "\n",
    "To compute autocorrelation, we fix a lag _k_ which is the delta between the given point and the prior point used to compute the correlation.\n",
    "\n",
    "With a _k_ value of 1, we'd compute how correlated a value is with the prior one. With a _k_ value of 10, we'd compute how correlated a variable is with one 10 time points earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"autocorrelation-using-statsmodels\"></a>\n",
    "<a id=\"autocorrelation-using-statsmodels\"></a>\n",
    "### Autocorrelation using statsmodels\n",
    "\n",
    "Statsmodels comes with some very convenient packages for calculating and plotting the autocorrelation. Load up these two functions and try them out:\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"partial-autocorrelation-and-the-partial-autocorrelation-function-pacf\"></a>\n",
    "<a id=\"partial-autocorrelation-and-the-partial-autocorrelation-function-pacf\"></a>\n",
    "## Partial autocorrelation and the partial autocorrelation function (PACF)\n",
    "---\n",
    "\n",
    "Another important chart to diagnose your timeseries is the partial autocorrelation chart (PACF). This is similar to the autocorrelation, but instead of just the correlation at increasing lags, it is the correlation at a given lag _controlling for the effect of previous lags._\n",
    "\n",
    "Load up the sister functions for partial autocorrelation from statsmodels and test them out on the differenced timeseries:\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"autoregressive-ar-models\"></a>\n",
    "<a id=\"autoregressive-ar-models\"></a>\n",
    "## Autoregressive (AR) models\n",
    "\n",
    "---\n",
    "\n",
    "Autoregressive (AR) models use data from previous time-points to predict the next time-point. These are essentially regression models where the predictors are previous timepoints of the outcome.\n",
    "\n",
    "Typically, AR models are denoted `AR(p)`, where _p_ indicates the number of previous time points to incorporate. `AR(1)` is the most common.\n",
    "\n",
    "In an autoregressive model we learn regression coefficients on the features that are the previous _p_ values.\n",
    "\n",
    "### $$y_i = c + \\beta_1  y_{i-1} + \\beta_2  y_{i-2}\\ +\\ ...\\ +\\ \\beta_p  y_{i-p}\\ +\\ \\epsilon \\\\\n",
    "y_i =\\sum_{j=1}^p \\beta_j y_{i-j} + \\epsilon$$\n",
    "\n",
    "We can build autoregressive models using the `ARMA` class from statsmodels. (Alternatively, there is a newer python package called pyflux that also looks promising for time series modeling.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "import pyflux as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"moving-average-ma-models\"></a>\n",
    "<a id=\"moving-average-ma-models\"></a>\n",
    "## Moving Average (MA) models\n",
    "---\n",
    "\n",
    "**Moving average models** take previous _error terms_ as inputs. They predict the next value based on deviations from previous predictions. This can be useful for modeling a sudden occurrence - like something going out of stock affecting sales or a sudden rise in popularity.\n",
    "\n",
    "As in autoregressive models, we have an order term, _q_, and we refer to our model as `MA(q)`.  This moving average model is dependent on the last _q_ errors. If we have a time series of sales per week, $y_i$, we can regress each $y_i$ on the last _q_ error terms.\n",
    "\n",
    "### $$y_t = \\epsilon_t + \\beta_{1} \\epsilon_{t-1} + ... \\beta_{n} \\epsilon_{t-n} \\\\\n",
    "y_t = \\sum_{i=1}^n \\beta_i \\epsilon_{t-i} + \\epsilon_t$$\n",
    "\n",
    "Sometimes the mean of the timeseries is included in the equation:\n",
    "\n",
    "### $$ y_t = \\mu + \\sum_{i=1}^n \\beta_i \\epsilon_{t-i} + \\epsilon_t $$\n",
    "\n",
    "Moving average models are not as trivial to fit as autoregressive models because the error terms are unobserved. [There are a variety of different ways you can estimate the parameters, some of which are covered in this paper.](https://www.it.uu.se/research/publications/reports/2006-022/2006-022-nc.pdf)\n",
    "\n",
    "In the simpler fitting procedures, a model is iteratively fit, errors are computed, then refit, over and over again until the parameters on the errors converge.\n",
    "\n",
    "MA includes the mean of the time series. The behavior of the model is therefore characterized by random jumps around the mean value.\n",
    "\n",
    "In an `MA(1)` model, there is one coefficient on the error of our previous prediction impacting our estimate for the next value in the timeseries.\n",
    "\n",
    "**We can also fit moving average models using statsmodels or pyflux.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"arma-and-arima-models\"></a>\n",
    "<a id=\"arma-and-arima-models\"></a>\n",
    "## ARMA and ARIMA models\n",
    "---\n",
    "\n",
    "**ARMA** models combine the autoregressive models and moving average models. We combine both, parameterizing the behavior of the model with `p` and `q` terms corresponding to the `AR(p)` model and `MA(q)` model.\n",
    "\n",
    "Autoregressive models slowly incorporate changes in preferences, tastes, and patterns. Moving average models base their prediction not on the prior value but the prior error, allowing us to correct sudden changes based on random events - supply, popularity spikes, etc.\n",
    "\n",
    "**ARIMA** is just like the `ARMA(p, q)` model, but instead of predicting the value of the series it predicts the _differenced_ series or changes in the series. The order of differencing is set by an _d_ term as in `ARIMA(p, d, q)`, or alternatively you can just fit an `ARMA(p, q)` model on a differenced timeseries.\n",
    "\n",
    "Recall the pandas `diff` function. This computes the difference between two consecutive values. In an ARIMA model, we attempt to predict this difference instead of the actual values.\n",
    "\n",
    "### $$y_t - y_{(t-1)} = ARMA(p, q)$$\n",
    "\n",
    "Timeseries are assumed to be \"stationary\" when modeling. This handles the stationarity assumption: instead of detrending or differencing manually, the model does this via the differencing term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"how-to-choose-the-right-p-and-q-parameters\"></a>\n",
    "<a id=\"how-to-choose-the-right-p-and-q-parameters\"></a>\n",
    "## How to choose the right `p` and `q` parameters.\n",
    "---\n",
    "\n",
    "In general it is never a bad idea to choose your parameters based on hold-out testing. That is to say, checking the performance of your model on future timepoints based on different choices of `p` and `q` for an ARIMA model.\n",
    "\n",
    "However, you can get a sense for what parameters will work best based on the autocorrelation and partial autocorrelation charts.\n",
    "\n",
    "[This site has a very detailed overview of how to use the acf and pacf to determine your parameters.](https://people.duke.edu/~rnau/411arim3.htm)\n",
    "\n",
    "In general though, below are some basic guidelines. Remember that these rules apply to the ACF and PACF plots of differenced timeseries rather than the original timeseries (the exception being if your timeseries is stationary and does not require differencing):\n",
    "\n",
    "1. If the PACF has a sharp cutoff and the lag-1 ACF value is positive then choose an AR(x) term where x is the lag in the PACF after the cutoff.\n",
    "2. If the ACF has a sharp cutoff and the lag-1 ACF value is negative, choose an MA(x) term where x is the lag in the ACF after the cutoff.\n",
    "3. If both the ACF and PACF show a gradual decay, and ARMA model is likely appropriate as opposed to the AR or MA alone.\n",
    "\n",
    "Context 1 above corresponds to timeseries that are \"underdifferenced\" as indicated by a positive autocorrelation at lat 1. Likewise, context 2 is \"overdifferenced\" as indicated by the negative autocorrelation.\n",
    "\n",
    "In general, you should try to choose an AR or MA model alone as opposed to an ARMA model. The AR and MA terms can work against each other in the model and create an overly-complex representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
