{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Neural Networks with TensorFlow\n",
    "\n",
    "_Authors: Richard Harris (CHI)_\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Get a quick overview of neural networks\n",
    "- Build a linear regression in Tensorflow\n",
    "- Tune our approach to linear regression to make it more efficient\n",
    "- Build a Multilayer Perceptron Feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [What are Neural Networks?](#what-are-neural-networks)\n",
    "- [Pros vs. Cons](#pros-vs-cons)\n",
    "- [Features](#features)\n",
    "- [Outputs](#outputs)\n",
    "- [Hidden Layers](#hidden-layers)\n",
    "- [Activation Function](#activation-function)\n",
    "\t- [ReLU](#relu)\n",
    "\t- [Softmax](#softmax)\n",
    "- [Backpropogation](#backpropogation)\n",
    "- [Epochs and Batch Sizes](#epochs-and-batch-sizes)\n",
    "- [Common Neural Net libraries in Python](#common-neural-net-libraries-in-python)\n",
    "- [Codealong: Linear Models in TensorFlow](#codealong-linear-models-in-tensorflow)\n",
    "\t- [Setup parameters and data that we will use later](#setup-parameters-and-data-that-we-will-use-later)\n",
    "\t- [Create tensors that correspond to our data](#create-tensors-that-correspond-to-our-data)\n",
    "\t- [Create weights to use in our model](#create-weights-to-use-in-our-model)\n",
    "\t- [Define a linear equation](#define-a-linear-equation)\n",
    "\t- [Define a loss function](#define-a-loss-function)\n",
    "\t- [Instantiate an optimizer](#instantiate-an-optimizer)\n",
    "\t- [Time to train our linear regression/neural network](#time-to-train-our-linear-regressionneural-network)\n",
    "\t- [Great, we've got a linear model, but there are still a few things we can improve](#great-weve-got-a-linear-model-but-there-are-still-a-few-things-we-can-improve)\n",
    "\t- [An example with real data](#an-example-with-real-data)\n",
    "\t- [Create tensors that correspond to our data](#create-tensors-that-correspond-to-our-data)\n",
    "\t- [Create weights to use in our model](#create-weights-to-use-in-our-model)\n",
    "\t- [Create model, loss function, and optimizaer](#create-model-loss-function-and-optimizaer)\n",
    "\t- [Scale our data](#scale-our-data)\n",
    "\t- [Train our network on the Boston data](#train-our-network-on-the-boston-data)\n",
    "\t- [Plot MSE over Epochs](#plot-mse-over-epochs)\n",
    "- [Train a Multilayer Perceptron on the Titanic data](#train-a-multilayer-perceptron-on-the-titanic-data)\n",
    "\t- [Load in the titanic data](#load-in-the-titanic-data)\n",
    "\t- [Do a bit of data cleaning](#do-a-bit-of-data-cleaning)\n",
    "\t- [Create tensors](#create-tensors)\n",
    "\t- [Create weights](#create-weights)\n",
    "\t- [Create the structure of the Multilayer Perceptron network](#create-the-structure-of-the-multilayer-perceptron-network)\n",
    "\t- [Define loss function](#define-loss-function)\n",
    "\t- [Define optimizer](#define-optimizer)\n",
    "\t- [Train the network](#train-the-network)\n",
    "\t- [A final example with more hidden layers](#a-final-example-with-more-hidden-layers)\n",
    "- [Additionl Resources](#additionl-resources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import Imputer, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "Neural networks are incredibly powerful and constantly talked about these days -- they've handled tasks such as image classification, [playing Go](http://www.nature.com/news/google-ai-algorithm-masters-ancient-game-of-go-1.19234), and [creating tweets in the style of President Trump](https://twitter.com/deepdrumpf?lang=en) with relatively little effort.\n",
    "\n",
    "This is a rapidly evolving field and represents some of the newest parts of Data Science, thanks to the increase in processing power and scale of data.\n",
    "\n",
    "In short, what you'll find with neural networks are some advantages and disadvantages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-are-neural-networks\"></a>\n",
    "## What are Neural Networks?\n",
    "---\n",
    "\n",
    "Neural networks, in a single line, attempt to iteratively train a set (or sets) of weights that, when used together, return the most accurate predictions for a set of inputs. Just like many of our past models, the model is trained using a loss function, which our model will attempt to minimize over iterations. Remember that a loss function is some function that takes in our predictions and the actual values and returns some sort of aggregate value that shows how accurate (or not) we were.\n",
    "\n",
    "Neural networks do this by establishing sets of neurons (known as hidden layers) that take in some sort of input(s), apply a weight, and pass that output onward. As we feed more data into the network, it adjusts those weights based on the output of the loss function, until we have highly trained and specific weights.\n",
    "\n",
    "Why does one neuron turn out one way and a second neuron another? That's not generally something we can understand (though attempts have been made, such as Google's Deep Dream). You can understand this as a kind of (very advanced) mathematic optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pros-vs-cons\"></a>\n",
    "## Pros vs. Cons\n",
    "---\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Exceptionally accurate because we can learn complicated decision boundaries\n",
    "- Appropriate for a vast range of techniques\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Long training time\n",
    "- Requires more data than most algorithms\n",
    "- Can become very complex and hard to interpret\n",
    "- Less user-friendly coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"features\"></a>\n",
    "## Features\n",
    "---\n",
    "\n",
    "Much like our other machine learning techniques, we do need to feed data into the network. While neural networks are pretty good at taking data in any form, it can help the network a lot to reduce the number of inputs when necessary -- particularly with image data. A smaller quantity inputs can usually give just as good results as a larger input without much change in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"outputs\"></a>\n",
    "## Outputs\n",
    "---\n",
    "\n",
    "Much like other techniques, we do want some sort of output at the end as well. In most cases:\n",
    "\n",
    "for a regression style technique, one output is usually fine\n",
    "for a classification technique, one output per class is a good idea (in other words, we model a one-against-all approach)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hidden-layers\"></a>\n",
    "## Hidden Layers\n",
    "---\n",
    "\n",
    "What makes neural networks tick is the idea of hidden layers. Hidden does not mean anything particularly devious here, just that it is not the input or the output layer.\n",
    "\n",
    "Hidden layers can have any number of neurons per layer and you can include any number of layers in a neural network. Inputs into a neuron have different weights that are modified across iterations of the model and have a bias term as well -- you can kind of almost imagine them as mini-linear models (though, that linearity does not need to hold at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"activation-function\"></a>\n",
    "## Activation Function\n",
    "---\n",
    "\n",
    "Neurons also may have an activation function that \"turns them on\" in certain cases. Some examples are:\n",
    "\n",
    "<a id=\"relu\"></a>\n",
    "### ReLU\n",
    "\n",
    "Also known as a [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), this turns the output to 0 if the output would be less than 0 (i.e., take the output and feed it through f(y) = max(0, y)). This means that the neuron is activated when its output is positive and not activated otherwise.\n",
    "\n",
    "<a id=\"softmax\"></a>\n",
    "### Softmax\n",
    "\n",
    "Used frequently at the output layer, this essentially \"squishes\" a bunch of inputs into a normalized scale of 0-1, which is great for creating something akin to a probability of falling into a given class. ([More information here](https://en.wikipedia.org/wiki/Softmax_function))\n",
    "\n",
    "There's a wealth of information on different types of activation functions within [this article](https://en.wikipedia.org/wiki/Activation_function) -- different activation functions, hidden layers, and neurons per layer can change how effective your neural network will be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"backpropogation\"></a>\n",
    "## Backpropogation\n",
    "---\n",
    "\n",
    "While there are many ways that a neural network learns, we'll focus in this seminar week on the easiest to understand method. Backpropogation is the process of using how well the network performed compared to the actual outputs and using those errors to propogate changes to the weights in each hidden layer.\n",
    "\n",
    "How do we make good or bad choices within the network? We compare the outputs of the predictions (using the loss function), and make tiny changes to compare the outputs. Most frequently we use a learning rate and a gradient descent method to estimate the changes that our successive models have used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"epochs-and-batch-sizes\"></a>\n",
    "## Epochs and Batch Sizes\n",
    "---\n",
    "\n",
    "Epochs: The number of iterations of full model fitting (i.e., how many times do you run through the fitting process?) There's no upper limit but generally there will be a point where additional epochs do not generate new insights\n",
    "Batch Size: Neural networks tend to work best when you feed portions of your data in at a time (versus the full set) and adjust weights in between. Smaller batches allow for more frequent updates but may be less consistent in what changes are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"common-neural-net-libraries-in-python\"></a>\n",
    "## Common Neural Net libraries in Python\n",
    "---\n",
    "\n",
    "There are a number of libraries available for neural networks in Python, including:\n",
    "\n",
    "- [Tensorflow](https://www.tensorflow.org/)\n",
    "- [Caffe](http://caffe.berkeleyvision.org/)\n",
    "- [Lasagne](https://lasagne.readthedocs.io/en/latest/)\n",
    "- [Theano](http://deeplearning.net/software/theano/)\n",
    "- [Keras](https://keras.io/)\n",
    "\n",
    "We'll be using the Tensorflow library (by Google) in these examples, but the development environment is very open right now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"codealong-linear-models-in-tensorflow\"></a>\n",
    "## Codealong: Linear Models in TensorFlow\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haven't you ever wanted to get rid of those pesky linear models in sklearn? Let's replicate that effort using Tensorflow.\n",
    "\n",
    "Tensorflow works by setting up `tensors`, which essentially hold numeric data of various shapes, and passes them through user-defined operations. The user-defined operations will be our Neural Network layers. When we initialize a session within Tensorflow, that starts the process of filling (or refilling) those tensors. You can imagine this system as actually setting up a set of calculations first and then filling them in with real data later.\n",
    "\n",
    "**Tensors are a lot like vectors and matricies:**\n",
    "- 0D Tensor = Scalar\n",
    "- 1D Tensor = Vector\n",
    "- 2D Tensor = Matrix\n",
    "- 4D Tensor = ??\n",
    "\n",
    "What we'll do here is start with some fake data to show off the basic process and then use some real data to reinforce those lessons.\n",
    "\n",
    "(This first section is taken from [this example](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup-parameters-and-data-that-we-will-use-later\"></a>\n",
    "### Setup parameters and data that we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up some parameters that we will use later\n",
    "\n",
    "learning_rate = 0.01 # Size of Gradient Descent step\n",
    "training_epochs = 2000 # How many times we'll iterate through the full set\n",
    "display_step = 50 # We want to see something every 50 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-tensors-that-correspond-to-our-data\"></a>\n",
    "### Create tensors that correspond to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-weights-to-use-in-our-model\"></a>\n",
    "### Create weights to use in our model\n",
    "- W will be the weight term, like the beta coefficient in a linear regression\n",
    "- b will be the bias term like the constant in a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define-a-linear-equation\"></a>\n",
    "### Define a linear equation\n",
    "- As we build more complex networks we can change this equation\n",
    "\n",
    "This sure starts to look like we're trying to identify the weights for an equation that looks like:\n",
    "\n",
    "$Y = b + W \\cdot X$\n",
    "\n",
    "is the same thing as\n",
    "\n",
    "$Y = \\alpha + \\beta \\cdot X$\n",
    "\n",
    "which should look a lot like a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = tf.add(tf.multiply(X, W), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define-a-loss-function\"></a>\n",
    "### Define a loss function\n",
    "- As most machine learning models, we need someway to measure how close our prediction are to our actual values.\n",
    "- In this case we use mean squared error, although the syntax may look a little foreign\n",
    "- reduce_sum is the same as a sum in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've written the optimization problem:\n",
    "\n",
    "$$\\min_{w, b} {\\|y - (Xw + b) \\|}$$\n",
    "\n",
    "Which says that we want to find a set of b and W that makes mean squared error as small as possible on our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instantiate-an-optimizer\"></a>\n",
    "### Instantiate an optimizer\n",
    "- We still need a way to solve for our weights W\n",
    "- Tensorflow comes with several solvers to do the work for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"time-to-train-our-linear-regressionneural-network\"></a>\n",
    "### Time to train our linear regression/neural network\n",
    "- We instantiate a tensorflow session using tf.Session()\n",
    "- Rather than training the network completely, we will loop every 50 epochs so we can see b and W change and watch our MSE drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost=  0.154262 W= 0.405064 b= -0.316979\n",
      "Epoch: 0100 cost=  0.145331 W= 0.395818 b= -0.25046\n",
      "Epoch: 0150 cost=  0.137431 W= 0.387121 b= -0.187897\n",
      "Epoch: 0200 cost=  0.130444 W= 0.378942 b= -0.129055\n",
      "Epoch: 0250 cost=  0.124264 W= 0.371249 b= -0.0737121\n",
      "Epoch: 0300 cost=  0.118798 W= 0.364013 b= -0.021661\n",
      "Epoch: 0350 cost=  0.113963 W= 0.357208 b= 0.0272943\n",
      "Epoch: 0400 cost=  0.109687 W= 0.350808 b= 0.0733379\n",
      "Epoch: 0450 cost=  0.105905 W= 0.344788 b= 0.116643\n",
      "Epoch: 0500 cost=  0.102560 W= 0.339127 b= 0.157373\n",
      "Epoch: 0550 cost=  0.099602 W= 0.333802 b= 0.19568\n",
      "Epoch: 0600 cost=  0.096985 W= 0.328793 b= 0.231709\n",
      "Epoch: 0650 cost=  0.094671 W= 0.324083 b= 0.265596\n",
      "Epoch: 0700 cost=  0.092624 W= 0.319653 b= 0.297467\n",
      "Epoch: 0750 cost=  0.090814 W= 0.315486 b= 0.327442\n",
      "Epoch: 0800 cost=  0.089213 W= 0.311567 b= 0.355634\n",
      "Epoch: 0850 cost=  0.087797 W= 0.307881 b= 0.38215\n",
      "Epoch: 0900 cost=  0.086545 W= 0.304414 b= 0.407089\n",
      "Epoch: 0950 cost=  0.085438 W= 0.301154 b= 0.430544\n",
      "Epoch: 1000 cost=  0.084459 W= 0.298088 b= 0.452605\n",
      "Epoch: 1050 cost=  0.083593 W= 0.295203 b= 0.473354\n",
      "Epoch: 1100 cost=  0.082827 W= 0.292491 b= 0.492868\n",
      "Epoch: 1150 cost=  0.082150 W= 0.289939 b= 0.511222\n",
      "Epoch: 1200 cost=  0.081551 W= 0.28754 b= 0.528484\n",
      "Epoch: 1250 cost=  0.081022 W= 0.285283 b= 0.54472\n",
      "Epoch: 1300 cost=  0.080553 W= 0.28316 b= 0.559991\n",
      "Epoch: 1350 cost=  0.080139 W= 0.281164 b= 0.574353\n",
      "Epoch: 1400 cost=  0.079773 W= 0.279286 b= 0.587862\n",
      "Epoch: 1450 cost=  0.079450 W= 0.27752 b= 0.600566\n",
      "Epoch: 1500 cost=  0.079164 W= 0.275859 b= 0.612516\n",
      "Epoch: 1550 cost=  0.078911 W= 0.274297 b= 0.623753\n",
      "Epoch: 1600 cost=  0.078687 W= 0.272828 b= 0.634322\n",
      "Epoch: 1650 cost=  0.078489 W= 0.271446 b= 0.644262\n",
      "Epoch: 1700 cost=  0.078315 W= 0.270146 b= 0.653611\n",
      "Epoch: 1750 cost=  0.078160 W= 0.268924 b= 0.662403\n",
      "Epoch: 1800 cost=  0.078024 W= 0.267775 b= 0.670673\n",
      "Epoch: 1850 cost=  0.077903 W= 0.266693 b= 0.678452\n",
      "Epoch: 1900 cost=  0.077796 W= 0.265676 b= 0.685768\n",
      "Epoch: 1950 cost=  0.077702 W= 0.26472 b= 0.69265\n",
      "Epoch: 2000 cost=  0.077619 W= 0.26382 b= 0.699121\n",
      "Optimization finished\n",
      "Training cost= 0.0776185 W= 0.26382 b= 0.699121 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103ac13d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Give initial values to b and w\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Print results from every few epochs\n",
    "    for epoch in range(training_epochs): \n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y:y})\n",
    "        \n",
    "        \n",
    "        if (epoch+1) % display_step == 0: \n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print 'Epoch:', '%04d' % (epoch+1), 'cost=', '{:9f}'.format(c), \\\n",
    "            \"W=\", sess.run(W), \"b=\", sess.run(b)\n",
    "    \n",
    "    print 'Optimization finished'\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print \"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n'\n",
    "    \n",
    "    \n",
    "    # Plot final regression\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original Data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted Data')\n",
    "    plt.title('Tensorflow-fit Linear Model')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"great-weve-got-a-linear-model-but-there-are-still-a-few-things-we-can-improve\"></a>\n",
    "### Great, we've got a linear model, but there are still a few things we can improve\n",
    "- We need to extend this concept to multiple regression with several Ws\n",
    "- We can standardize the inputs to make it easier for the solver to find a solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"an-example-with-real-data\"></a>\n",
    "### An example with real data\n",
    "- We'll use the boston data set to implement multiple regression in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6,  34.7,  33.4,  36.2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.target[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-tensors-that-correspond-to-our-data\"></a>\n",
    "### Create tensors that correspond to our data\n",
    "- This we tell Tensorflow that our feature matrix has 13 features and our outcome has one vector\n",
    "- We also tell it how many observations we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 13], name='Xs')\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1], name='Y')\n",
    "n_samples = df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"create-weights-to-use-in-our-model\"></a>\n",
    "### Create weights to use in our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([13, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-model-loss-function-and-optimizaer\"></a>\n",
    "### Create model, loss function, and optimizaer\n",
    "- We will modify the model a little with matrix multiplication so we can use mutiple Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = tf.add(tf.matmul(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scale-our-data\"></a>\n",
    "### Scale our data\n",
    "- Optimization problems tend to work best when all the numbers and derivatives are around the same magnitude\n",
    "    - Neural Nets are especially sensitive to this\n",
    "- We will use sklearn to standardize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train_X = StandardScaler().fit_transform(df.values)\n",
    "train_Y = boston.target\n",
    "cost_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train-our-network-on-the-boston-data\"></a>\n",
    "### Train our network on the Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up some parameters that we will use later\n",
    "\n",
    "learning_rate = 0.01 # Size of Gradient Descent step\n",
    "training_epochs = 1000 # How many times we'll iterate through the full set\n",
    "display_step = 50 # We want to see something every 50 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 109.496483 W= [-0.56201518  0.46835619 -0.61104381  0.55889142 -0.47626033  1.81559396\n",
      " -0.36564958 -0.1600478  -0.29272541 -0.57083827 -1.09925532  0.55320776\n",
      " -1.64753366] b= [ 8.90032101] \n",
      "\n",
      "Epoch: 0100 cost= 47.493828 W= [-0.58871984  0.45371988 -0.5614242   0.78619611 -0.43711033  2.575845\n",
      " -0.25068393 -0.6731317  -0.02960126 -0.4996174  -1.44008243  0.66400641\n",
      " -2.27466226] b= [ 14.28506851] \n",
      "\n",
      "Epoch: 0150 cost= 24.869040 W= [-0.59252065  0.44397196 -0.51269847  0.8609367  -0.46293616  2.95881987\n",
      " -0.18989979 -1.04056668  0.18664187 -0.44456077 -1.60275173  0.73729998\n",
      " -2.6562469 ] b= [ 17.54287148] \n",
      "\n",
      "Epoch: 0200 cost= 16.540710 W= [-0.59833997  0.44930342 -0.47929701  0.8714394  -0.53116816  3.14418721\n",
      " -0.16572933 -1.30951416  0.3538354  -0.42011723 -1.68824065  0.79211408\n",
      " -2.90699053] b= [ 19.51386833] \n",
      "\n",
      "Epoch: 0250 cost= 13.430635 W= [-0.60717493  0.46441856 -0.45492196  0.85811615 -0.61741561  3.2223146\n",
      " -0.1580416  -1.51833618  0.4876076  -0.41701069 -1.73758638  0.83189297\n",
      " -3.07863212] b= [ 20.70633125] \n",
      "\n",
      "Epoch: 0300 cost= 12.235949 W= [-0.61820573  0.48562366 -0.4357166   0.83850724 -0.70911598  3.2426517\n",
      " -0.15678839 -1.68852329  0.59888172 -0.42801651 -1.76993215  0.85962808\n",
      " -3.20069814] b= [ 21.42777634] \n",
      "\n",
      "Epoch: 0350 cost= 11.750593 W= [-0.63054341  0.51048243 -0.41958642  0.81957108 -0.80016696  3.2325449\n",
      " -0.15731177 -1.8322103   0.69472635 -0.44824922 -1.79415429  0.8780964\n",
      " -3.29069996] b= [ 21.86425781] \n",
      "\n",
      "Epoch: 0400 cost= 11.532336 W= [-0.64350456  0.53732431 -0.4053587   0.80355954 -0.8877185   3.20715737\n",
      " -0.157585   -1.95652592  0.77967757 -0.47447312 -1.81426358  0.88967198\n",
      " -3.359272  ] b= [ 22.12833214] \n",
      "\n",
      "Epoch: 0450 cost= 11.418046 W= [-0.65662366  0.56498498 -0.3923465   0.79076886 -0.97054261  3.17488003\n",
      " -0.15684137 -2.06586695  0.85665613 -0.50455534 -1.83201385  0.89626992\n",
      " -3.41303134] b= [ 22.28809929] \n",
      "\n",
      "Epoch: 0500 cost= 11.346886 W= [-0.66960186  0.59265786 -0.38012925  0.78078675 -1.04821467  3.14031482\n",
      " -0.15490142 -2.16310096  0.92756146 -0.5370844  -1.84814644  0.89937061\n",
      " -3.45620751] b= [ 22.38475609] \n",
      "\n",
      "Epoch: 0550 cost= 11.295694 W= [-0.6822558   0.61979467 -0.36843985  0.77302837 -1.12070453  3.10593891\n",
      " -0.15184405 -2.25020695  0.99364996 -0.57111776 -1.86297321  0.90008271\n",
      " -3.4915812 ] b= [ 22.4432373] \n",
      "\n",
      "Epoch: 0600 cost= 11.255282 W= [-0.69447571  0.64603603 -0.35710436  0.76694435 -1.18817043  3.07303095\n",
      " -0.14784828 -2.32862902  1.05577171 -0.60601813 -1.87663746  0.89921665\n",
      " -3.52103543] b= [ 22.47861862] \n",
      "\n",
      "Epoch: 0650 cost= 11.221727 W= [-0.70620024  0.6711579  -0.34600896  0.7620886  -1.25085545  3.0421946\n",
      " -0.14311743 -2.39947557  1.11451805 -0.64134759 -1.88922393  0.89735299\n",
      " -3.54588318] b= [ 22.50001907] \n",
      "\n",
      "Epoch: 0700 cost= 11.193125 W= [-0.71739721  0.69503415 -0.33507881  0.75811952 -1.30903685  3.01365733\n",
      " -0.13784531 -2.46363068  1.17031324 -0.67680019 -1.90080452  0.89489931\n",
      " -3.56706738] b= [ 22.51296425] \n",
      "\n",
      "Epoch: 0750 cost= 11.168408 W= [-0.72805369  0.71760541 -0.32426664  0.75478786 -1.36299992  2.98744154\n",
      " -0.13220263 -2.52182531  1.22347283 -0.71215832 -1.91145027  0.89213747\n",
      " -3.58528447] b= [ 22.52080345] \n",
      "\n",
      "Epoch: 0800 cost= 11.146878 W= [-0.73816925  0.73885953 -0.31354448  0.75191468 -1.41302574  2.96345949\n",
      " -0.1263333  -2.57467818  1.2742393  -0.74726546 -1.92123497  0.88925809\n",
      " -3.60106063] b= [ 22.52554321] \n",
      "\n",
      "Epoch: 0850 cost= 11.128015 W= [-0.74775159  0.75881588 -0.30289754  0.74937367 -1.45938408  2.941571\n",
      " -0.12035435 -2.62272239  1.32280588 -0.78200644 -1.93023193  0.88638663\n",
      " -3.61480546] b= [ 22.52840996] \n",
      "\n",
      "Epoch: 0900 cost= 11.111414 W= [-0.7568143   0.7775141  -0.29232073  0.74707639 -1.50233245  2.92161345\n",
      " -0.11435935 -2.6664238   1.36933136 -0.81629634 -1.93851185  0.88360339\n",
      " -3.62683988] b= [ 22.53014565] \n",
      "\n",
      "Epoch: 0950 cost= 11.096749 W= [-0.76537436  0.79500669 -0.28181511  0.74496138 -1.54211354  2.90341806\n",
      " -0.10842109 -2.70619512  1.41394949 -0.85007161 -1.94614196  0.88095683\n",
      " -3.63742399] b= [ 22.53120041] \n",
      "\n",
      "Epoch: 1000 cost= 11.083744 W= [-0.77345103  0.81135422 -0.27138665  0.74298579 -1.5789547   2.88682485\n",
      " -0.10259481 -2.74240303  1.45677555 -0.88328475 -1.95318472  0.87847435\n",
      " -3.64676929] b= [ 22.53183174] \n",
      "\n",
      "Optimization finished\n",
      "Training cost= 11.0837 \n",
      "intercept:  22.5318 \n",
      "\n",
      "CRIM : -0.773451\n",
      "ZN : 0.811354\n",
      "INDUS : -0.271387\n",
      "CHAS : 0.742986\n",
      "NOX : -1.57895\n",
      "RM : 2.88682\n",
      "AGE : -0.102595\n",
      "DIS : -2.7424\n",
      "RAD : 1.45678\n",
      "TAX : -0.883285\n",
      "PTRATIO : -1.95318\n",
      "B : 0.878474\n",
      "LSTAT : -3.64677\n",
      "MSE: 11.0837\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEPCAYAAACzwehFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1148e9410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs): \n",
    "        sess.run(optimizer, feed_dict={X: train_X, Y:train_Y.reshape(506, 1)})\n",
    "        cost_history.append(sess.run(cost, feed_dict={X: train_X, Y:train_Y.reshape(506, 1)}))\n",
    "        \n",
    "        if (epoch+1) % display_step == 0: \n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y: train_Y.reshape(506, 1)})\n",
    "            print 'Epoch:', '%04d' % (epoch+1), 'cost=', '{:9f}'.format(c), \\\n",
    "            \"W=\", sess.run(W).reshape(13,), \"b=\", sess.run(b), '\\n'\n",
    "            \n",
    "    \n",
    "    print 'Optimization finished'\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y.reshape(506,1)})\n",
    "    print \"Training cost=\", training_cost, '\\n', \"intercept: \", sess.run(b)[0],'\\n'\n",
    "    weights = sess.run(W).reshape(13,)\n",
    "    for x in zip(df.columns, weights):\n",
    "        print x[0], ':', x[1]\n",
    "        \n",
    "    pred_y = sess.run(pred, feed_dict={X: train_X})\n",
    "    mse = sess.run(cost, feed_dict={X:train_X, Y:train_Y.reshape(506,1)})\n",
    "    print(\"MSE: %.4f\" % mse) \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(train_Y, pred_y)\n",
    "    ax.plot([train_Y.min(), train_Y.max()], [train_Y.min(), train_Y.max()], 'k--', lw=3)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is hard to read because there is more going on, but it's just like Multiple Linear Regression. As we train our network further and further we get better estimates for our parameters and our MSE decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot-mse-over-epochs\"></a>\n",
    "### Plot MSE over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11808c210>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117ed8bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1000) , cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train-a-multilayer-perceptron-on-the-titanic-data\"></a>\n",
    "## Train a Multilayer Perceptron on the Titanic data\n",
    "---\n",
    "\n",
    "- A feedforward multilayer perceptron is one the most well known neural network archtectures\n",
    "- They a structured just like the picture in the intro\n",
    "    - We have an input layer of features\n",
    "    - These input features are passed into neurons in the hidden layers\n",
    "    - Each neuron is a perceptron, kind of like a bunch of small linear regressions\n",
    "    - We pass information from one layer of neurons to the next layer of neurons until we hit the output layer\n",
    "    - The output layer does one calculation to output a prediction of our outome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/images/neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-in-the-titanic-data\"></a>\n",
    "### Load in the titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('assets/datasets/titanic_train.csv')\n",
    "X = data.drop('Survived', axis=1)\n",
    "y = data[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"do-a-bit-of-data-cleaning\"></a>\n",
    "### Do a bit of data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 51.8625],\n",
       "       [ 15.5   ],\n",
       "       [ 41.5792],\n",
       "       [ 14.4542],\n",
       "       [ 10.5167]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a helper class to extract features one by one in a pipeline\n",
    "class FeatureExtractor(TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return x[self.column].values.reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "FeatureExtractor('Fare').fit_transform(X_train)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a pipeline to binarize labels and impute missing values with an appropriate method\n",
    "pclass_pipe = make_pipeline(\n",
    "    FeatureExtractor('Pclass'),\n",
    "    LabelBinarizer(),\n",
    "    Imputer(strategy='most_frequent')\n",
    ")\n",
    "sex_pipe = make_pipeline(\n",
    "    FeatureExtractor('Sex'),\n",
    "    LabelBinarizer(),\n",
    "    Imputer(strategy='most_frequent')\n",
    ")\n",
    "age_pipe = make_pipeline(\n",
    "    FeatureExtractor('Age'),\n",
    "    Imputer(strategy='mean')\n",
    ")\n",
    "sibsp_pipe = make_pipeline(\n",
    "    FeatureExtractor('SibSp'),\n",
    "    Imputer(strategy='most_frequent')\n",
    ")\n",
    "parch_pipe = make_pipeline(\n",
    "    FeatureExtractor('Parch'),\n",
    "    Imputer(strategy='most_frequent')\n",
    ")\n",
    "\n",
    "fu = make_union(pclass_pipe, sex_pipe, age_pipe, sibsp_pipe, parch_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.        ,   0.        ,   0.        ,   1.        ,\n",
       "         54.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   1.        ,   1.        ,\n",
       "         29.52598326,   0.        ,   0.        ],\n",
       "       [  0.        ,   1.        ,   0.        ,   1.        ,\n",
       "         25.        ,   1.        ,   2.        ],\n",
       "       [  0.        ,   0.        ,   1.        ,   1.        ,\n",
       "         26.        ,   1.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   1.        ,   0.        ,\n",
       "         22.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fu.fit(X_train)\n",
    "\n",
    "fu.transform(X_train)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create X, y, train, and test\n",
    "def multi_binarizer(data):\n",
    "    data = data.copy()\n",
    "    data['Survived Class 0'] = data['Survived'].apply(lambda x: 1 if x == 0 else 0)\n",
    "    return data[['Survived Class 0', 'Survived']].values\n",
    "\n",
    "train_X = fu.transform(X_train)\n",
    "train_Y = multi_binarizer(y_train)\n",
    "test_X = fu.transform(X_test)\n",
    "ttest_Y = multi_binarizer(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-tensors\"></a>\n",
    "### Create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = train_X.shape[1]\n",
    "n_train = train_X.shape[0]\n",
    "n_classes = 2\n",
    "\n",
    "x = tf.placeholder('float', [None, n_input])\n",
    "y = tf.placeholder('float', [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-weights\"></a>\n",
    "### Create weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the number of neurons in each hidden layer\n",
    "n_hidden_1 = 4\n",
    "n_hidden_2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stddev = 0.1\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], stddev=stddev))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-the-structure-of-the-multilayer-perceptron-network\"></a>\n",
    "### Create the structure of the Multilayer Perceptron network\n",
    "- This is very similar to what we did in linear regression\n",
    "    - `tf.add(tf.matmul(x, weights['h1']), biases['b1'])` is nearly identical\n",
    "- There are two major changes\n",
    "    - We now use a dictionary to make it easier to access our weight\n",
    "    - We wrap the output from each layer in a ReLu function, `tf.nn.relu()`\n",
    "- Each layer is fed directly into the next layer, which is why we call it a feed-forward network\n",
    "    \n",
    "    \n",
    "Note: ReLu functions have come into popularity lately because the flat part of the function allows us to train complex interactions, while the linear part of the function helps us avoid the [vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ReLu function**\n",
    "![](./assets/images/relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Layer 1, ReLU activation, 4 neurons\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    # Layer 2, ReLU activation, 4 Neurons\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    return tf.matmul(layer_2, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define-loss-function\"></a>\n",
    "### Define loss function\n",
    "- We no longer MSE because this is not a regression problem, but a classification\n",
    "- Cross-entropy is a common loss function to use with neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define-optimizer\"></a>\n",
    "### Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optm = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train-the-network\"></a>\n",
    "### Train the network\n",
    "- We've changed the training procedure a little\n",
    "- We've added a batch_size.\n",
    "    - This is a trick to make optimization faster\n",
    "    - Rather than using all of our samples on every epoch to train the network, we sample observations\n",
    "    - This reduces the computation time, without a heavy impact on the optimization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001 # A weight to determine how fast the network is trained\n",
    "training_epochs = 2500 # How many times we update the weights by the learning_rate\n",
    "batch_size = 50 # How many samples of data we use at a time to compute the update\n",
    "display_step = 100 # How often\n",
    "\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, 'float'))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/2500 cost: 0.801703930\n",
      " Training accuracy: 0.300\n",
      "Epoch: 100/2500 cost: 0.667392221\n",
      " Training accuracy: 0.600\n",
      "Epoch: 200/2500 cost: 0.653392163\n",
      " Training accuracy: 0.640\n",
      "Epoch: 300/2500 cost: 0.657487062\n",
      " Training accuracy: 0.700\n",
      "Epoch: 400/2500 cost: 0.664966264\n",
      " Training accuracy: 0.540\n",
      "Epoch: 500/2500 cost: 0.646538231\n",
      " Training accuracy: 0.620\n",
      "Epoch: 600/2500 cost: 0.652824120\n",
      " Training accuracy: 0.580\n",
      "Epoch: 700/2500 cost: 0.651596384\n",
      " Training accuracy: 0.620\n",
      "Epoch: 800/2500 cost: 0.651729746\n",
      " Training accuracy: 0.680\n",
      "Epoch: 900/2500 cost: 0.634381018\n",
      " Training accuracy: 0.560\n",
      "Epoch: 1000/2500 cost: 0.662174680\n",
      " Training accuracy: 0.640\n",
      "Epoch: 1100/2500 cost: 0.601430416\n",
      " Training accuracy: 0.620\n",
      "Epoch: 1200/2500 cost: 0.562362476\n",
      " Training accuracy: 0.760\n",
      "Epoch: 1300/2500 cost: 0.551218802\n",
      " Training accuracy: 0.620\n",
      "Epoch: 1400/2500 cost: 0.482181427\n",
      " Training accuracy: 0.860\n",
      "Epoch: 1500/2500 cost: 0.445274025\n",
      " Training accuracy: 0.880\n",
      "Epoch: 1600/2500 cost: 0.443089659\n",
      " Training accuracy: 0.820\n",
      "Epoch: 1700/2500 cost: 0.424853393\n",
      " Training accuracy: 0.820\n",
      "Epoch: 1800/2500 cost: 0.547499852\n",
      " Training accuracy: 0.720\n",
      "Epoch: 1900/2500 cost: 0.434633390\n",
      " Training accuracy: 0.720\n",
      "Epoch: 2000/2500 cost: 0.389567771\n",
      " Training accuracy: 0.840\n",
      "Epoch: 2100/2500 cost: 0.420492245\n",
      " Training accuracy: 0.840\n",
      "Epoch: 2200/2500 cost: 0.455976053\n",
      " Training accuracy: 0.880\n",
      "Epoch: 2300/2500 cost: 0.397744829\n",
      " Training accuracy: 0.880\n",
      "Epoch: 2400/2500 cost: 0.413989261\n",
      " Training accuracy: 0.760\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(n_train/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        random_index = np.random.randint(n_train, size=batch_size)\n",
    "        batch_xs = train_X[random_index, :]\n",
    "        batch_ys = train_Y[random_index, :]\n",
    "        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        avg_cost += sess.run(cost,\n",
    "            feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "        \n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % \n",
    "               (epoch, training_epochs, avg_cost))\n",
    "        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        print (\" Training accuracy: %.3f\" % (train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = sess.run(tf.argmax(pred, 1), feed_dict={x:test_X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81694915254237288"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test['Survived'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"a-final-example-with-more-hidden-layers\"></a>\n",
    "### A final example with more hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/7500 cost: 0.703459539\n",
      " Training accuracy: 0.540\n",
      "Epoch: 100/7500 cost: 0.648077277\n",
      " Training accuracy: 0.720\n",
      "Epoch: 200/7500 cost: 0.650019066\n",
      " Training accuracy: 0.720\n",
      "Epoch: 300/7500 cost: 0.633597667\n",
      " Training accuracy: 0.660\n",
      "Epoch: 400/7500 cost: 0.655787349\n",
      " Training accuracy: 0.640\n",
      "Epoch: 500/7500 cost: 0.655253416\n",
      " Training accuracy: 0.620\n",
      "Epoch: 600/7500 cost: 0.641669658\n",
      " Training accuracy: 0.760\n",
      "Epoch: 700/7500 cost: 0.606665308\n",
      " Training accuracy: 0.680\n",
      "Epoch: 800/7500 cost: 0.502929246\n",
      " Training accuracy: 0.840\n",
      "Epoch: 900/7500 cost: 0.458643954\n",
      " Training accuracy: 0.860\n",
      "Epoch: 1000/7500 cost: 0.474865602\n",
      " Training accuracy: 0.760\n",
      "Epoch: 1100/7500 cost: 0.460915506\n",
      " Training accuracy: 0.780\n",
      "Epoch: 1200/7500 cost: 0.426819742\n",
      " Training accuracy: 0.780\n",
      "Epoch: 1300/7500 cost: 0.450268859\n",
      " Training accuracy: 0.840\n",
      "Epoch: 1400/7500 cost: 0.432139212\n",
      " Training accuracy: 0.840\n",
      "Epoch: 1500/7500 cost: 0.445740963\n",
      " Training accuracy: 0.840\n",
      "Epoch: 1600/7500 cost: 0.413085572\n",
      " Training accuracy: 0.740\n",
      "Epoch: 1700/7500 cost: 0.417553113\n",
      " Training accuracy: 0.800\n",
      "Epoch: 1800/7500 cost: 0.388157303\n",
      " Training accuracy: 0.900\n",
      "Epoch: 1900/7500 cost: 0.384393613\n",
      " Training accuracy: 0.820\n",
      "Epoch: 2000/7500 cost: 0.374013085\n",
      " Training accuracy: 0.760\n",
      "Epoch: 2100/7500 cost: 0.412517865\n",
      " Training accuracy: 0.840\n",
      "Epoch: 2200/7500 cost: 0.397762632\n",
      " Training accuracy: 0.800\n",
      "Epoch: 2300/7500 cost: 0.412860112\n",
      " Training accuracy: 0.840\n",
      "Epoch: 2400/7500 cost: 0.405943497\n",
      " Training accuracy: 0.720\n",
      "Epoch: 2500/7500 cost: 0.393474281\n",
      " Training accuracy: 0.880\n",
      "Epoch: 2600/7500 cost: 0.418664634\n",
      " Training accuracy: 0.820\n",
      "Epoch: 2700/7500 cost: 0.405666899\n",
      " Training accuracy: 0.840\n",
      "Epoch: 2800/7500 cost: 0.436362256\n",
      " Training accuracy: 0.840\n",
      "Epoch: 2900/7500 cost: 0.420687096\n",
      " Training accuracy: 0.880\n",
      "Epoch: 3000/7500 cost: 0.372082748\n",
      " Training accuracy: 0.840\n",
      "Epoch: 3100/7500 cost: 0.414962400\n",
      " Training accuracy: 0.820\n",
      "Epoch: 3200/7500 cost: 0.385292609\n",
      " Training accuracy: 0.880\n",
      "Epoch: 3300/7500 cost: 0.404227227\n",
      " Training accuracy: 0.800\n",
      "Epoch: 3400/7500 cost: 0.366978583\n",
      " Training accuracy: 0.860\n",
      "Epoch: 3500/7500 cost: 0.371606951\n",
      " Training accuracy: 0.840\n",
      "Epoch: 3600/7500 cost: 0.372051624\n",
      " Training accuracy: 0.880\n",
      "Epoch: 3700/7500 cost: 0.390805822\n",
      " Training accuracy: 0.900\n",
      "Epoch: 3800/7500 cost: 0.420084165\n",
      " Training accuracy: 0.780\n",
      "Epoch: 3900/7500 cost: 0.374303254\n",
      " Training accuracy: 0.880\n",
      "Epoch: 4000/7500 cost: 0.382652444\n",
      " Training accuracy: 0.820\n",
      "Epoch: 4100/7500 cost: 0.332172021\n",
      " Training accuracy: 0.820\n",
      "Epoch: 4200/7500 cost: 0.405082830\n",
      " Training accuracy: 0.900\n",
      "Epoch: 4300/7500 cost: 0.387039594\n",
      " Training accuracy: 0.840\n",
      "Epoch: 4400/7500 cost: 0.366349242\n",
      " Training accuracy: 0.820\n",
      "Epoch: 4500/7500 cost: 0.402757276\n",
      " Training accuracy: 0.840\n",
      "Epoch: 4600/7500 cost: 0.370967104\n",
      " Training accuracy: 0.860\n",
      "Epoch: 4700/7500 cost: 0.379626049\n",
      " Training accuracy: 0.880\n",
      "Epoch: 4800/7500 cost: 0.355532950\n",
      " Training accuracy: 0.940\n",
      "Epoch: 4900/7500 cost: 0.356115932\n",
      " Training accuracy: 0.780\n",
      "Epoch: 5000/7500 cost: 0.379692821\n",
      " Training accuracy: 0.860\n",
      "Epoch: 5100/7500 cost: 0.382209090\n",
      " Training accuracy: 0.840\n",
      "Epoch: 5200/7500 cost: 0.402254636\n",
      " Training accuracy: 0.820\n",
      "Epoch: 5300/7500 cost: 0.410613279\n",
      " Training accuracy: 0.900\n",
      "Epoch: 5400/7500 cost: 0.449036021\n",
      " Training accuracy: 0.760\n",
      "Epoch: 5500/7500 cost: 0.358031568\n",
      " Training accuracy: 0.820\n",
      "Epoch: 5600/7500 cost: 0.397309913\n",
      " Training accuracy: 0.880\n",
      "Epoch: 5700/7500 cost: 0.394778964\n",
      " Training accuracy: 0.740\n",
      "Epoch: 5800/7500 cost: 0.398440169\n",
      " Training accuracy: 0.760\n",
      "Epoch: 5900/7500 cost: 0.416472053\n",
      " Training accuracy: 0.800\n",
      "Epoch: 6000/7500 cost: 0.366647035\n",
      " Training accuracy: 0.780\n",
      "Epoch: 6100/7500 cost: 0.367149765\n",
      " Training accuracy: 0.840\n",
      "Epoch: 6200/7500 cost: 0.370932509\n",
      " Training accuracy: 0.880\n",
      "Epoch: 6300/7500 cost: 0.385810597\n",
      " Training accuracy: 0.860\n",
      "Epoch: 6400/7500 cost: 0.418050408\n",
      " Training accuracy: 0.840\n",
      "Epoch: 6500/7500 cost: 0.396608231\n",
      " Training accuracy: 0.880\n",
      "Epoch: 6600/7500 cost: 0.414986166\n",
      " Training accuracy: 0.720\n",
      "Epoch: 6700/7500 cost: 0.346554837\n",
      " Training accuracy: 0.900\n",
      "Epoch: 6800/7500 cost: 0.397256567\n",
      " Training accuracy: 0.860\n",
      "Epoch: 6900/7500 cost: 0.371092165\n",
      " Training accuracy: 0.860\n",
      "Epoch: 7000/7500 cost: 0.363648602\n",
      " Training accuracy: 0.920\n",
      "Epoch: 7100/7500 cost: 0.342860265\n",
      " Training accuracy: 0.820\n",
      "Epoch: 7200/7500 cost: 0.381398732\n",
      " Training accuracy: 0.760\n",
      "Epoch: 7300/7500 cost: 0.382715613\n",
      " Training accuracy: 0.740\n",
      "Epoch: 7400/7500 cost: 0.375695513\n",
      " Training accuracy: 0.840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11892a290>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQDDGGANYQDDGGOOxgGCMMQaA/w+7nFQVw+CkigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1185cfd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.005\n",
    "training_epochs = 7500\n",
    "batch_size = 50\n",
    "display_step = 100\n",
    "\n",
    "n_hidden_1 = 8\n",
    "n_hidden_2 = 8\n",
    "n_hidden_3 = 8\n",
    "n_input = train_X.shape[1]\n",
    "n_train = train_X.shape[0]\n",
    "n_classes = 2\n",
    "\n",
    "x = tf.placeholder('float', [None, n_input])\n",
    "y = tf.placeholder('float', [None, n_classes])\n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Layer 1, ReLU activation, 4 neurons\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    # Layer 2, ReLU activation, 4 Neurons\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    # Layer 3, ReLu activation, 4 Neurons\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['h3']), biases['b3']))\n",
    "    return tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "\n",
    "stddev = 0.1\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=stddev)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=stddev)),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], stddev=stddev)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes], stddev=stddev))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optm = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# We also set up some helper functions\n",
    "\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, 'float'))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "cost_counter = []\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.0\n",
    "    total_batch = int(n_train/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        random_index = np.random.randint(n_train, size=batch_size)\n",
    "        batch_xs = train_X[random_index, :]\n",
    "        batch_ys = train_Y[random_index, :]\n",
    "        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        avg_cost += sess.run(cost,\n",
    "            feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "        \n",
    "    cost_counter.append(avg_cost)\n",
    "        \n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % \n",
    "               (epoch, training_epochs, avg_cost))\n",
    "        train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        print (\" Training accuracy: %.3f\" % (train_acc))\n",
    "        \n",
    "predictions = sess.run(tf.argmax(pred, 1), feed_dict={x:test_X})\n",
    "\n",
    "plt.plot(range(training_epochs), cost_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = sess.run(tf.argmax(pred, 1), feed_dict={x:test_X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82372881355932204"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test['Survived'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"additionl-resources\"></a>\n",
    "## Additional Resources\n",
    "---\n",
    "\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "- [Deep Learning](http://www.deeplearningbook.org/)\n",
    "- [Tensorflow Tutorials](https://github.com/pkmital/tensorflow_tutorials)\n",
    "- [Awesome Tensorflow](https://github.com/jtoy/awesome-tensorflow)\n",
    "- [Tensorflow Examples](https://github.com/aymericdamien/TensorFlow-Examples)\n",
    "- [Mind: How to Build a Neural Network](https://stevenmiller888.github.io/mind-how-to-build-a-neural-network/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
