{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "_Authors: Dan Wilhelm (LA) and Alex Combs (NYC) _\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Describe Naive Bayes\n",
    "- Choose a Naive Bayes implementation based on your use case\n",
    "- Implement a Naive Bayes model through scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Discriminative Models vs Generative Models](#discriminative-models-vs-generative-models)\n",
    "    - [Discriminative vs Generative Example](#discriminative-vs-generative-example)\n",
    "    - [Making a Generative Model (joint probability distribution)](#making-a-generative-model-joint-probability-distribution)\n",
    "    - [The Joint Probability Generative Model is not Practical](#the-joint-probability-generative-model-is-not-practical)\n",
    "    - [Making a Discriminative Model (logistic regression)](#making-a-discriminative-model-logistic-regression)\n",
    "\n",
    "\n",
    "- [A Better Generative Model?](#a-better-generative-model)\n",
    "    - [Bayes' Theorem](#bayes-theorem)\n",
    "    - [Conditional probability](#conditional-probability)\n",
    "\n",
    "\n",
    "- [A really simple spam example](#a-really-simple-spam-example)\n",
    "    - [Problem: Multiple features require joint probabilities](#problem-multiple-features-require-joint-probabilities)\n",
    "    - [Solution: The Naive Bayes Independence Assumption](#solution-the-naive-bayes-independence-assumption)\n",
    "    - [Spam application: Multiple Features](#spam-application-multiple-features)\n",
    "\n",
    "\n",
    "- [Production Issues](#production-issues)\n",
    "- [Summary](#summary)\n",
    "\n",
    "\n",
    "- [Implementation in Scikit-learn](#implementation-in-scikit-learn)\n",
    "- [Guided practice: Scikit-learn implementation](#guided-practice-scikit-learn-implementation)\n",
    "- [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"discriminative-models-vs-generative-models\"></a>\n",
    "## Discriminative Models vs Generative Models\n",
    "---\n",
    "\n",
    "Logistic Regression is a **discriminative model**. \n",
    "\n",
    "+ Its equation $P\\;(\\;class\\;|\\;features\\;)$ **discriminates** between two classes. In other words, it describes the boundary between the classes.\n",
    "+ From this equation, we cannot generate \"typical\" members of either class -- we only know their boundary.\n",
    "\n",
    "Naive Bayes is a **generative model**. Using it, we can **generate** typical members of each class, since we know what a \"typical\" member of each class looks like.\n",
    "\n",
    "+ Note that we are still estimating $P\\;(\\;class\\;|\\;features\\;)$ for each class!\n",
    "+ By computing how \"typical\" each example is to each class, we can choose the most likely class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"discriminative-vs-generative-example\"></a>\n",
    "### Discriminative vs Generative Example\n",
    "\n",
    "Let's make a simple generative model from scratch. Suppose we are attempting to infer whether someone is a GA data science student or not. To do this, we take members of the general population and evaluate three binary features:\n",
    "\n",
    "- **G**: At GA.\n",
    "- **C**: Has Computer.\n",
    "- **S**: Has Stats Book.\n",
    "\n",
    "Our model of a \"person\" is now the presence or absence of each of these three features.\n",
    "\n",
    "#### Data\n",
    "\n",
    "We sample ten students in each class. We'll wildly assume the GA/non-GA numbers are good, representative samples of the population for each combination of features. \n",
    "\n",
    "- **GA Student** - GCS, GC, GCS, GCS, GC, C, C, GC, GCS, GCS\n",
    "- **Not GA Student** - none, none, C, GC, C, none, C, C, CS, none\n",
    "\n",
    "We grabbed ten examples per class. Note this is typically the data we have access to in most data science problems -- examples of each class of interest. However, to be representative of the overall population, should we have gotten ten examples from each of the eight categories instead? Which technique would be more accurate? Think about that, while we take this toy data and tally it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"making-a-generative-model-joint-probability-distribution\"></a>\n",
    "### Making a Generative Model (joint probability distribution)\n",
    "\n",
    "Summarizing this data per class  (where $\\neg$ indicates negation):\n",
    "\n",
    "|                   | GCS | GC | CS | S | C | none\n",
    "| ---               | --- | -- | -- | - | - | -\n",
    "| **Student**       | 5   | 3  | 0  | 0 | 2 | 0 \n",
    "| **$\\neg$Student** | 0   | 1  | 1  | 0 | 4 | 4\n",
    "\n",
    "\n",
    "Directly from these, we'll make a generative model of a student:\n",
    "\n",
    "- $P(\\;Student\\;|\\;GCS\\;) = 1$.\n",
    "- $P(\\;Student\\;|\\;GC\\;) = 3/4$.\n",
    "- $P(\\;Student\\;|\\;C\\;) = 1/3$.\n",
    "- $P(\\;Student\\;|\\;none\\;) = 0$.\n",
    "\n",
    "Similarly for a non-student:\n",
    "\n",
    "- $P(\\;\\neg Student\\;|\\;GCS\\;) = 0$.\n",
    "- $P(\\;\\neg Student\\;|\\;GC\\;) = 1/4$.\n",
    "- $P(\\;\\neg Student\\;|\\;CS\\;) = 1$.\n",
    "- $P(\\;\\neg Student\\;|\\;C\\;) = 2/3$.\n",
    "- $P(\\;\\neg Student\\;|\\;none\\;) = 1$.\n",
    "\n",
    "\n",
    "You may recognize that we just computed the **joint probabilities**! \n",
    "\n",
    "Note we must store $2^3$ parameters in total -- the presence or absence of three features.\n",
    "\n",
    "#### Using the Generative Model\n",
    "\n",
    "+ Suppose we see someone with $GCS$. We would then guess with confidence 1 he or she is a GA Student. If someone is $GC$, we would guess GA Student with confidence $3/4$.\n",
    "\n",
    "+ Suppose we want to generate a sequence of \"typical\" GA Students. Easy -- with probability $\\frac{1}{1 + 3/4 + 1/3} = \\frac{12}{25}$ generate a $GCS$ person. With probability $\\frac{3/4}{1 + 3/4 + 1/3} = \\frac{9}{25}$ generate a $GC$ person!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"the-joint-probability-generative-model-is-not-practical\"></a>\n",
    "### The Joint Probability Generative Model is not Practical\n",
    "\n",
    "We saw earlier how to make a generative model strictly from joint probabilities. However, this method has major problems.\n",
    "\n",
    "+ The number of parameters stored in the model increases exponentially. For example, if each feature is binary and we have 100 features, we would need $2^{100} \\approx 10^{30}$ parameters for every joint probability!\n",
    "\n",
    "+ Hence, we would need _enormous_ amounts of data to ensure we have sufficient training examples to evaluate each joint probability robustly (again, just to emphasize -- $2^{100}$ joint probabilities for only $100$ binary features).\n",
    "\n",
    "Although each probability is easy to calculate, the joint probability model is simply not practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"making-a-discriminative-model-logistic-regression\"></a>\n",
    "### Making a Discriminative Model (logistic regression)\n",
    "\n",
    "For comparison, let's make a discriminative model using the same data. \n",
    "\n",
    "|                   | GCS | GC | CS | S | C | none\n",
    "| ---               | --- | -- | -- | - | - | -\n",
    "| **Student**       | 5   | 3  | 0  | 0 | 2 | 0 \n",
    "| **$\\neg$Student** | 0   | 1  | 1  | 0 | 4 | 4\n",
    "\n",
    "Let's just eyeball a hyperplane that separates the classes. For simplicity, suppose that our hyperplane that separates the classes (and corresponding link function) is the following formula, where $\\sigma$ is the logistic function and $X$ is our feature vector. $G = 1$ if true and $G = -1$ if false:\n",
    "\n",
    "$$P(\\;Student\\;|\\;X\\;) = \\sigma(5G - 2C + S)$$\n",
    "\n",
    "You can see this model is approximately correct. Recall that $5G - 2C + S > 0$ indicates a probability over 0.5. This model makes sense -- being seen at GA is a strong indicator of being a GA student! Having a computer is a bit of a less positive signal since so many non-students also have computers.\n",
    "\n",
    "#### Using the Discriminative Model\n",
    "\n",
    "+ We can predict the probability that $GC$ is a student by letting $G = 1, C = 1, S = -1$. So $5G - 2C + S = 2 > 0$, so likely a student.\n",
    "\n",
    "+ This model is more compact since we store fewer parameters -- $4$ (3 plus a bias) instead of $2^3$. (It even scales well -- linearly instead of exponentially!)\n",
    "\n",
    "+ However, we cannot generate \"typical\" students with any accuracy. (Actually, in this particular case we can attempt to; however, with any substantial number of features typical members would be far too ambiguous.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"a-better-generative-model\"></a>\n",
    "## A Better Generative Model?\n",
    "---\n",
    "\n",
    "There must be some reasonable simplifications we can make to build a practical generative model. There are -- and they're nearly as simple to calculate!\n",
    "\n",
    "Let's understand why we are doing this first. Recall that we are looking for this: $P\\;(\\;class\\;|\\;features\\;)$. Here, 'class' refers to a category such as 'vertosa', 'versicola', 'Student', etc.\n",
    "\n",
    "+ Computing this directly implies that we have lots of data for every feature combination. We typically don't have that! Typically, our training data only ensures we have sufficient examples for each **class**. For example, 10 examples of GA students and 10 examples of non-GA students.\n",
    "\n",
    "+ Because our training data has lots of examples of each class, it would be great if we can flip that conditional probability! Bayes' Theorem to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"bayes-theorem\"></a>\n",
    "### Bayes' Theorem\n",
    "\n",
    "### $$P\\left(\\;class\\;|\\;features\\;\\right) = \\frac{P\\left(\\;features\\;|\\;class\\;\\right)P\\left(\\;class\\;\\right)}{P(\\;features\\;)} $$\n",
    " \n",
    "Luckily for us, it's easy to compute these probabilities directly from a data table! \n",
    "\n",
    "+ **$P(\\;class\\;)$**. For example: $P(\\;student\\;) = P(\\;\\neg student\\;) = 1/2$.\n",
    "+ **$P(\\;features\\;)$**. For example, we had 5 $GCS$ combinations in 20 examples, so $P(\\;GCS\\;) = 5/20 = 1/4$.\n",
    "+ **$P(\\;features\\;|\\;class\\;)$**. For example, for $GCS$ we had 5 students of 10. $P(\\;GCS\\;|\\;Student\\;) = 1/2$.\n",
    "\n",
    "|                   | GCS | GC | CS | S | C | none\n",
    "| ---               | --- | -- | -- | - | - | -\n",
    "| **Student**       | 5   | 3  | 0  | 0 | 2 | 0 \n",
    "| **$\\neg$Student** | 0   | 1  | 1  | 0 | 4 | 4\n",
    "\n",
    "\n",
    "Then what is $P(\\;Student\\;|\\;GCS\\;)$? Easy, it's just $\\frac{1/2 * 1/2}{1/4} = 1$. That's exactly what we computed above as a joint probability!\n",
    "\n",
    "+ Does this work out for all of our earlier joint probabilities? See if you can understand why.\n",
    "+ Hint: Recall that Bayes' is just a single conditional probability. The numerator is just the chain rule: $P(\\;A \\cap B\\;) = P(\\;A\\;|\\;B\\;)\\;P(\\;B\\;)$.\n",
    "\n",
    "|                    | GCS                           | GC                 | ... | \n",
    "| ---                | ---------------------------   | ------------------ | --  | \n",
    "| **Student**        | Student $\\cap$ GCS            | Student $\\cap$ GC  | ... | P(Student) = 1/2\n",
    "| **$\\neg$ Student** | $\\neg$ Student $\\cap$ GCS     | $\\neg$ Student $\\cap$ GC | ... | P($\\neg$ Student) = 1/2\n",
    "|                    | P(GCS) = 5/20                 | P(GC) = 4/20       |     | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"conditional-probability\"></a>\n",
    "### Conditional probability\n",
    "\n",
    "Let's review and see how Bayes' Theorem is just a big conditional probability.\n",
    "\n",
    "In general, for events $A$ and $B$ the **conditional probability** is:\n",
    "\n",
    "### $$ P(\\;A\\;|\\;B\\;) = \\frac{P(\\;A \\cap B\\;)}{P(\\;B\\;)} $$\n",
    "\n",
    "Hence (since $\\cap$ is commutative, i.e. $P(A \\cap B) = P(B \\cap A)$):\n",
    "\n",
    "### $$ P(A \\cap B) = P(A\\;|\\;B) \\; P(B) = P(B\\;|\\;A) \\; P(A) $$\n",
    "\n",
    "This is often referred to as the \"chain rule\" of probability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Bayes' thereom\n",
    "\n",
    "From the above, just substitute the second equation into the first:\n",
    "\n",
    "### $$P\\left(\\;A\\;|\\;B\\;\\right) = \\frac{P(\\;A \\cap B\\;)}{P(\\;B\\;)} = \\frac{P\\left(\\;B\\;|\\;A\\;\\right)P\\left(\\;A\\;\\right)}{P(\\;B\\;)}$$\n",
    "\n",
    "\n",
    "### $$P\\left(\\;class\\;|\\;features\\;\\right) = \\frac{P\\left(\\;features\\;|\\;class\\;\\right)P\\left(\\;class\\;\\right)}{P(\\;features\\;)} $$\n",
    "\n",
    "As we saw earlier, it is very easy to compute $P(\\;features\\;|\\;class\\;)$ from the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"a-really-simple-spam-example\"></a>\n",
    "## A really simple spam example\n",
    "---\n",
    "\n",
    "Here is another example. We are trying to predict spam emails.  For now, we have one feature: whether the email mentions 'guarantee'.\n",
    "\n",
    "$G$ = Guarantee, $S$ = Is Spam.\n",
    "\n",
    " $$P\\left(\\;S\\;|\\;G\\;\\right) = \\frac{P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;G\\;)} = \\frac{P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;G\\;|\\;S)P(\\;S\\;) + P(\\;G\\;|\\;\\neg{S})P(\\;\\neg{S}\\;)}$$\n",
    "\n",
    "We saw earlier how it is possible to compute $P\\;(\\;G\\;)$ directly. In many books, you will see it computed in this alternative fashion which is again based on our classes rather than based on every combination of features. \n",
    "\n",
    "> The denominator looks complicated, but it actually isn't. Since $G$ is binary (either present or not), then:\n",
    "\n",
    "> $$P(G) = P(G \\cap S) + P(G \\cap \\neg{S})$$\n",
    "\n",
    "> Now, just expand each term using the \"chain rule\" and you get the denominator!\n",
    "\n",
    "Again, note we started with a term $P\\;(\\;features\\;)$ -- in general, using this would require the calculation of every combination of features (i.e. we would also need to compute $P\\;(\\;GS\\;)$, $P\\;(\\;GCS\\;)$, etc). However, by expanding $P\\;(\\;G\\;)$ via the chain rule, we got an expression that depends only on the individual classes that we have data about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"problem-multiple-features-require-joint-probabilities\"></a>\n",
    "### Problem: Multiple features require joint probabilities\n",
    "\n",
    "In this spam example, we only had one feature $G$. But in all likelihood, we'll use more than one feature. Really, we want to see some feature vector $X_1, X_2, ..., X_n$:\n",
    "\n",
    "$$P\\left(\\;S\\;|\\;X_1, ..., X_n\\;\\right) = \\frac{P\\left(\\;X_1,  ..., X_n\\;|\\;S\\;\\right)}{P(\\;X_1,  ..., X_n\\;|\\;S) + P(\\;X_1, ..., X_n\\;|\\;\\neg{S})}$$\n",
    "\n",
    "For example, what is the likelihood that something is spam given that the email mentions Guarantee, Oil, Prince, and Nigeria ... but not Meeting, Colleague, and Dad?\n",
    "\n",
    "With a lot of features, calculating the joint probabilities gets really complicated really quickly. We would definitely need lots of data to ensure we have enough feature combinations. If you reason this out, you quickly may realize we run into the same joint probability problem as before, requiring exponentially many joint probabilities!\n",
    "\n",
    "No matter how diligent we are, we may never collect a single training example that contains the precise combination of feature words we need. Hence, we would be unable to classify a new email containing a particular combination of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"solution-the-naive-bayes-independence-assumption\"></a>\n",
    "### Solution: The Naive Bayes Independence Assumption\n",
    "\n",
    "We are stuck again, since conditional joint probabilities are required for multiple features. This means exponentially many probabilities to compute, and exponentially more data collection.\n",
    "\n",
    "To get around this, let's make an assumption: **All $X_i$ are conditionally independent given $S$** (where $S$ indicates \"is spam\"). Despite the fancy words, this just means that given $S$, then no two $X_i$ depend on each other. For example, the words Nigeria and Prince each independently indicate that an email is spam. So, it is not the complex interaction of words that determines spam; each feature independently can indicate whether an email is spam.\n",
    "\n",
    "> Of course, this assumption is rarely (if ever) true! Often it requires precise reading to tell whether an email was written by a native speaker, for example. In this case, it is often not the particular words used but how they are used in context.\n",
    "\n",
    "Recall that if events $A$ and $B$ are independent, then the probability $P\\;(\\;AB\\;) = P\\;(\\;A\\;)\\;P\\;(\\;B\\;)$. Similarly, if A and B are conditionally independent on S, then $P\\;(\\;AB\\;|\\;S\\;) = P\\;(\\;A\\;|\\;S\\;)\\;P\\;(\\;B\\;|\\;S\\;)$.\n",
    "\n",
    "> This formula works out really well in general too:\n",
    "\n",
    "> $$P\\left(\\;X_{1}X_{2} \\dots X_{n}\\;|\\;S\\;\\right) = P\\left(\\;X_{1} |\\;S\\;\\right) * P\\left(\\;X_{2} |\\;S\\;\\right) ... P\\left(\\;X_{n} |\\;S\\;\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "To see if this **conditional independence assumption** might simplify the numerator $P(X_1, ..., X_n\\;|\\;S)$ to remove the joint probabilities, let's first apply the definition of conditional probability followed by applying the independence assumption:\n",
    "\n",
    "$$P\\;(\\;SGM\\;) = P\\;(\\;GM\\;|\\;S\\;)\\;P\\;(\\;S\\;) = P\\;(\\;G\\;|\\;S\\;)\\;P\\;(\\;M\\;|\\;S\\;)\\;P\\;(\\;S\\;)$$\n",
    "\n",
    "\n",
    "> None of these probabilities require us to examine multiple features at once in our dataset, making them drastically easier to compute. For example, $P(\\;A\\;|\\;S\\;)$ could indicate just the probability of the word A occuring in a spam email!\n",
    "\n",
    "In reality, model parameters / coefficients are unlikely to be independent.  But Naive Bayes makes exactly this assumption ... and it turns out to often work well despite this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a id=\"spam-application-multiple-features\"></a>\n",
    "### Spam application: Multiple Features\n",
    "\n",
    "How is this used in practice? Let's combine the naive bayes simplification above with our original formula (the denominator is computed the same as before combined with our naive assumption):\n",
    "\n",
    " $$P\\left(\\;S\\;|\\;GM\\;\\right) = \\frac{P(\\;SGM\\;)}{P(\\;GM\\;)} = \\frac{P\\left(\\;GM\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;GM\\;|\\;S)P(\\;S\\;) + P(\\;GM\\;|\\;\\neg{S})P(\\;\\neg{S}\\;)} = \\frac{P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;M\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;G\\;|\\;S)P(\\;M\\;|\\;S)P(\\;S\\;) + P(\\;G\\;|\\;\\neg{S})P(\\;M\\;|\\;\\neg{S})P(\\;\\neg{S}\\;)}$$\n",
    " \n",
    "Typically, we compute this probability for each class (in this case, just S or not S), then predict the class with highest probability. Note for all of these, the denominator $P(GM)$ is constant. Hence, this formula is often written as \"proportional\" ($\\propto$), considerably simplifying it. Instead of comparing the exact probabilities, we can just see how they score relative to each other. So:\n",
    "\n",
    " $$P\\left(\\;S\\;|\\;GM\\;\\right) \\propto P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;M\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)$$\n",
    "\n",
    "So again: don't be scared by the denominator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"production-issues\"></a>\n",
    "## Production Issues\n",
    "---\n",
    "\n",
    "Recall Naive Bayes is proportional to this:\n",
    "\n",
    " $$P\\left(\\;S\\;|\\;GM\\;\\right) \\propto P\\left(\\;G\\;|\\;S\\;\\right)P\\left(\\;M\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)$$\n",
    "\n",
    "Accidentally using a zero probability for any of these could present major problems -- the entire probabililty estimation would be zero!\n",
    "\n",
    "- **New features.** What if a particular feature was never seen in our training data? Instead of using a zero probability, we should use a technique such as [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to estimate a small non-zero probability for it.\n",
    "\n",
    "- **Underflow.** Probabilities could be very small if some features rarely occur in some classes. Recall that floating point often gives us trouble due to its limited precision -- small floats tend toward zero. We can approach this problem by storing the logarithm of each probability $P_i$ instead of $P_i$ itself:\n",
    "\n",
    "$$log(P_1P_2) = log\\ P_1 + log\\ P_2$$\n",
    "\n",
    "$$e^{log\\ P_1} = P_1$$\n",
    "\n",
    "So: $$P_1P_2 \\dots P_n = e^{log\\ P_1 + ... + log\\ P_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "---\n",
    "\n",
    "**Why is this Naive Bayes formula important?** With the independence assumption, we do not need to compute every joint probability distribution! Even if none of the training data contains 'guarantee' and 'millions', we only need to compute the probability of each word separately: $P(\\;G\\;|\\;S\\;)$ and $P(\\;M\\;|\\;S\\;)$. \n",
    "\n",
    "These calculations can be quickly performed using our training data. The downside is that if spam is actually determined by some complex interaction between 'guarantee' and 'millions' (e.g. only the presence of one but not the other), then the independence assumption does not hold and our model will not have the capacity to predict spam correctly.\n",
    "\n",
    "To make Naive Bayes a classifier, all we have to do is compute the probability of $P(y|X)$ for each class $y$. In math notation, this is:\n",
    "\n",
    "### $$ P(y \\;|\\; x_1, ..., x_n) \\propto P(y) \\prod_{i=1}^n P(x_i \\;|\\; y) \\\\\n",
    "\\downarrow \\\\\n",
    "\\hat{y} = arg \\; \\underset{y}{max} \\; P(y) \\prod_{i=1}^n P(x_i \\;|\\; y)$$\n",
    "\n",
    "> Recall that $arg\\underset{y}{max}$ means we find the class in vector of categories $y$ that gives us the maximum expression value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"implementation-in-scikit-learn\"></a>\n",
    "## Implementation in Scikit-learn\n",
    "---\n",
    "\n",
    "\n",
    "- [Docs 1](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "- [Docs 2](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "- [Docs 3](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "\n",
    "<img src=\"./images/naive-bayes.png\">\n",
    "\n",
    "The differences can be summarized as follows\n",
    "-    ***BernoulliNB*** is designed for binary/boolean features\n",
    "-    The ***multinomial Naive Bayes classifier*** is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as `tf-idf` may also work\n",
    "-    ***GaussianNB*** is designed for continuous features (that can be scaled between 0,1) and is assumed to be normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"guided-practice-scikit-learn-implementation\"></a>\n",
    "## Guided practice: Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./datasets/spam_base.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.40</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.44</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6 ...  0.40  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00 ...  0.00   \n",
       "\n",
       "    0.41  0.42  0.778   0.43   0.44  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Check: what do you think is going on with this dataset?\n",
    "\n",
    "> Which sklearn NB implementation should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "feature_set = data.iloc[:, :-1]\n",
    "target = data.iloc[:, -1]\n",
    "\n",
    "classifier1 = naive_bayes.MultinomialNB().fit(feature_set, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78718784  0.8165038   0.80978261  0.78346028  0.69314472]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print cross_val_score(classifier1, feature_set, target, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Check: is that good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.606086956521739"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1. - np.mean(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "---\n",
    "\n",
    "\n",
    "How does Naive Bayes fit into your toolkit? What are the pros and cons? How do you choose between variants?\n",
    "\n",
    "#### Additional Resources\n",
    "\n",
    "- [An interesting slide from a Stanford MOOC which had a section on Naive Bayes](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)\n",
    "- [A much more technical paper comparing Naive Bayes to Logistics Regressions](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf)\n",
    "- [More exposition on Naive Bayes](http://blog.yhat.com/posts/naive-bayes-in-python.html)\n",
    "- [Naive Bayes from scratch](http://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
