{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Sentiment Analysis of Movie Reviews with Spacy and VADER\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the goal of basic sentiment analysis.\n",
    "- Calculate sentiment scores manually using a reviews dataset and scores tagged by word.\n",
    "- Practice using the spacy parser to get out part of speech tags from text.\n",
    "- Fit a model using sentiment and grammar features.\n",
    "- Use the VADER sentiment analyzer to get out more accurate sentiment scores and compare the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to sentiment analysis](#intro)\n",
    "- [Load the word sentiment dataset](#load-sen)\n",
    "    - [Engineer objectivity and positive difference scores](#adj-scores)\n",
    "    - [Put scores in a part of speech dictionary](#pos-dict)\n",
    "- [Load the rotten tomatoes review dataset](#rt-reviews)\n",
    "    - [Restrict reviews to valid lengths and ratings](#subset)\n",
    "- [Import spacy](#spacy)\n",
    "    - [Parse all the quotes using spacy's multithreaded parser](#multi)\n",
    "- [Part of speech features](#pos-features)\n",
    "- [Assign sentiment scores](#assign)\n",
    "- [Print out the most positive and most negative reviews](#print-most)\n",
    "- [Print out the most objective and most subjective reviews](#print-most-obj)\n",
    "- [Build a model to classify fresh vs. rotten with the sentiment and grammar features](#model)\n",
    "- [User the VADER library to get better sentiment scores](#vader)\n",
    "    - [Build a model using the VADER sentiment features](#vader-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to sentiment analysis\n",
    "---\n",
    "\n",
    "Sentiment analysis is one of the most popular topics in NLP. Most commonly it is the quantification of text into valence and subjectivity scores.\n",
    "\n",
    "First we will load in a dataset of pre-coded sentiment scores for positivity and negativity on words. These words are also tagged with their part of speech in the sentence. We can use these valence scores to evaluate the sentiment of rottentomatoes movie reviews. Many packages such as TextBlob come pre-packaged with sentiment scores for words after parsing text, but doing the sentiment parsing manually will show you how it can be done without any \"magic\".\n",
    "\n",
    "We will also explore a more advanced sentiment analysis library in python: [VADER](https://github.com/cjhutto/vaderSentiment). We can parse the sentiment of the movie reviews using this package and compare it to our more basic method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load-sen'></a>\n",
    "\n",
    "## Load the word sentiment dataset\n",
    "---\n",
    "\n",
    "Below we will load in some pre-tagged positive and negative valence scores for a dictionary of words. Each row of the dataset contains the part of speech, the word, the positive score, and the negative score for the word. A word may appear more than once if it can appear with different part of speech tags. \n",
    "\n",
    "These scores are designed so that we can also derive the *objectivity score* of the word from the positive and negative scores.\n",
    "\n",
    "Objectivity is calculated: \n",
    "\n",
    "    1. - (positive_score + negative_score)\n",
    "\n",
    "Thus if a score has zero positive score and negative score it is completely objective. If a score has, for example, 0.5 positive and 0.5 negative, it may not be any more positive than negative but we can tell that it is subjective (objectivity = 0.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sen = pd.read_csv('../datasets/sentiment_words_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22_caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22_calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adj</td>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos         word  pos_score  neg_score\n",
       "0  adj  .22-caliber        0.0        0.0\n",
       "1  adj  .22-calibre        0.0        0.0\n",
       "2  adj  .22_caliber        0.0        0.0\n",
       "3  adj  .22_calibre        0.0        0.0\n",
       "4  adj  .38-caliber        0.0        0.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make the part of speech tags uppercase (this will come in handy later when we use Spacy).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen.pos = sen.pos.map(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adj-scores'></a>\n",
    "\n",
    "### Engineer objectivity and positive difference scores\n",
    "\n",
    "Since subjective vs. objective is embedded in the positive and negative scores, we should extract this and convert the positive and negative into a relative difference scores.\n",
    "\n",
    "**Calculate two new scores:**\n",
    "\n",
    "    objectivity = 1. - (pos_score + neg_score)\n",
    "    pos_vs_neg = pos_score - neg_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen['objectivity'] = 1. - (sen.pos_score + sen.neg_score)\n",
    "sen['pos_vs_neg'] = sen.pos_score - sen.neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>objectivity</th>\n",
       "      <th>pos_vs_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.22_caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.22_calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos         word  pos_score  neg_score  objectivity  pos_vs_neg\n",
       "0  ADJ  .22-caliber        0.0        0.0          1.0         0.0\n",
       "1  ADJ  .22-calibre        0.0        0.0          1.0         0.0\n",
       "2  ADJ  .22_caliber        0.0        0.0          1.0         0.0\n",
       "3  ADJ  .22_calibre        0.0        0.0          1.0         0.0\n",
       "4  ADJ  .38-caliber        0.0        0.0          1.0         0.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pos-dict'></a>\n",
    "\n",
    "### Put scores in a part of speech dictionary\n",
    "\n",
    "The dictionary format of the data will be much easier to index using our parsing functions later on. Create a dictionary where the keys are the four part of speech tags:\n",
    "\n",
    "    ADJ\n",
    "    NOUN\n",
    "    VERB\n",
    "    ADV\n",
    "\n",
    "For each key, store a dictionary that contains all of the words for that part of speech with their objectivity and positive vs. negative scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n"
     ]
    }
   ],
   "source": [
    "sen_dict = {'ADJ':{},'NOUN':{},'VERB':{},'ADV':{}}\n",
    "\n",
    "for i, row in enumerate(sen.itertuples()):\n",
    "    if (i % 10000) == 0:\n",
    "        print i\n",
    "    sen_dict[row[1]][row[2]] = {'objectivity':row[5], 'pos_vs_neg':row[6]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rt-reviews'></a>\n",
    "\n",
    "## Load the rotten tomatoes reviews dataset\n",
    "\n",
    "---\n",
    "\n",
    "This dataset has:\n",
    "    \n",
    "    critic: critic's name\n",
    "    fresh: fresh vs. rotten rating\n",
    "    imdb: code for imdb\n",
    "    publication: where the review was published\n",
    "    quote: the review snippet\n",
    "    review_date: date of review\n",
    "    rtid: rottentomatoes id\n",
    "    title: name of movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rt = pd.read_csv('../datasets/rt_critics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh      imdb     publication  \\\n",
       "0         Derek Adams  fresh  114709.0        Time Out   \n",
       "1     Richard Corliss  fresh  114709.0   TIME Magazine   \n",
       "2         David Ansen  fresh  114709.0        Newsweek   \n",
       "3       Leonard Klady  fresh  114709.0         Variety   \n",
       "4  Jonathan Rosenbaum  fresh  114709.0  Chicago Reader   \n",
       "\n",
       "                                               quote review_date    rtid  \\\n",
       "0  So ingenious in concept, design and execution ...  2009-10-04  9559.0   \n",
       "1                  The year's most inventive comedy.  2008-08-31  9559.0   \n",
       "2  A winning animated feature that has something ...  2008-08-18  9559.0   \n",
       "3  The film sports a provocative and appealing st...  2008-06-09  9559.0   \n",
       "4  An entertaining computer-generated, hyperreali...  2008-03-10  9559.0   \n",
       "\n",
       "       title  \n",
       "0  Toy story  \n",
       "1  Toy story  \n",
       "2  Toy story  \n",
       "3  Toy story  \n",
       "4  Toy story  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='subset'></a>\n",
    "\n",
    "### Restrict data to reviews with valid ratings and reviews over 10 words long\n",
    "\n",
    "Also clean up the reviews, making a column with the case and punctuation removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt = rt[rt.fresh.isin(['fresh','rotten'])]\n",
    "rt.fresh = rt.fresh.map(lambda x: 1 if x == 'fresh' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11215, 9)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt['quote_len'] = rt.quote.map(lambda x: len(x.split()))\n",
    "rt = rt[rt.quote_len > 10]\n",
    "rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "rt['qt'] = rt.quote.map(lambda x: unicode(''.join([y for y in list(x.lower()) if y in string.ascii_lowercase+\" -'\"])))\n",
    "rt.qt = rt.qt.map(lambda x: x.replace('-',' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='spacy'></a>\n",
    "\n",
    "## Import spacy\n",
    "\n",
    "---\n",
    "\n",
    "The spacy package is the current gold standard for parsing the grammatical structure of text (aside from neural network architectures). We are going to use it to find the part of speech tags for the review words. \n",
    "\n",
    "Once we have parsed the tags with spacy, we can assign objectivity and valence scores by finding the match in our sentiment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse a single quote:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = en_nlp(rt.qt.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "so ingenious in concept design and execution that you could watch it on a postage stamp sized screen and still be engulfed by its charm"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concept"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get out single words with indexing:\n",
    "tmp[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the part of speech tags for each word in the quote:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "ADJ\n",
      "ADP\n",
      "NOUN\n",
      "NOUN\n",
      "CCONJ\n",
      "NOUN\n",
      "ADJ\n",
      "PRON\n",
      "VERB\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "CCONJ\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADP\n",
      "ADJ\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in tmp:\n",
    "    print token.pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multi'></a>\n",
    "### Parse all the quotes using spacy's multithreaded parser\n",
    "\n",
    "Parsing a lot of text can take quite awhile. Luckily spacy comes with multithreading functionality to speed up the process considerably. Below is code that will parse the quotes across multiple threads and assign them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "parsed_quotes = []\n",
    "for i, parsed in enumerate(en_nlp.pipe(rt.qt.values, batch_size=50, n_threads=4)):\n",
    "    assert parsed.is_parsed\n",
    "    if (i % 1000) == 0:\n",
    "        print i\n",
    "    parsed_quotes.append(parsed)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pos-features'></a>\n",
    "\n",
    "## Create features with part of speech proportions\n",
    "\n",
    "---\n",
    "\n",
    "With our spacy parsed reviews, we have a lot of feature engineering potential even before we get to sentiment. Something simple we could do is calculate the proportion of words in the quote that have each part of speech tag. We can try using these as predictors in a model later.\n",
    "\n",
    "**Find all the unique part of speech categories in the reviews.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ADJ' u'ADP' u'ADV' u'CCONJ' u'DET' u'INTJ' u'NOUN' u'NUM' u'PART'\n",
      " u'PRON' u'PROPN' u'PUNCT' u'SPACE' u'SYM' u'VERB' u'X']\n"
     ]
    }
   ],
   "source": [
    "unique_pos = []\n",
    "for parsed in parsed_quotes:\n",
    "    unique_pos.extend([t.pos_ for t in parsed])\n",
    "unique_pos = np.unique(unique_pos)\n",
    "print unique_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the proportion columns for each part of speech.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pos in unique_pos:\n",
    "    rt[pos+'_prop'] = 0.\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterate through the reviews and calculate the proportions of each part of speech tag.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "rt = rt.reset_index(drop=True)\n",
    "for i, parsed in enumerate(parsed_quotes):\n",
    "    if (i % 1000) == 0:\n",
    "        print i\n",
    "    parsed_len = len(parsed)\n",
    "    for pos in unique_pos:\n",
    "        count = len([x for x in parsed if x.pos_ == pos])\n",
    "        rt.ix[i, pos+'_prop'] = float(count)/parsed_len\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assign'></a>\n",
    "\n",
    "## Assign sentiment scores\n",
    "---\n",
    "\n",
    "We will now use the parsed reviews and the sentiment dataset to assign the average objectivity and positive vs. negative scores.\n",
    "\n",
    "If a word cannot be found in the dataset we can ignore it. If a review has no words that match something in our dataset, will can assign overall neutral scores of `objectivity = 1` and `pos_vs_neg = 0`.\n",
    "\n",
    "There are definitely problems with this approach, but for now we can keep it \"dumb\" and see if things improve when we use the VADER analyzer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "def scorer(parsed):\n",
    "    obj_scores, pvn_scores = [], []\n",
    "    for token in [t for t in parsed if t.pos_ in ['NOUN','VERB','ADV','ADJ']]:\n",
    "        try:\n",
    "            obj_scores.append(sen_dict[token.pos_][str(token)]['objectivity'])\n",
    "            pvn_scores.append(sen_dict[token.pos_][str(token)]['pos_vs_neg'])\n",
    "        except:\n",
    "            pass\n",
    "    if len(obj_scores) == 0:\n",
    "        obj_scores = [1.]\n",
    "    if len(pvn_scores) == 0:\n",
    "        pvn_scores = [0.]\n",
    "    return [obj_scores, pvn_scores]\n",
    "\n",
    "\n",
    "scores = []\n",
    "for i, parsed in enumerate(parsed_quotes):\n",
    "    if (i % 1000) == 0:\n",
    "        print i\n",
    "    scores.append(scorer(parsed))\n",
    "    \n",
    "rt['objectivity_avg'] = [np.mean(x[0]) for x in scores]\n",
    "rt['pos_vs_neg_avg'] = [np.mean(x[1]) for x in scores]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='print-most'></a>\n",
    "## Print out the most positive and most negative reviews\n",
    "---\n",
    "\n",
    "Now that we have the average valence for reviews, try printing out the top 10 most positive and top 10 most negative reviews to visually verify that our approach makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streep (the best thing she has done in ages) carries it along.\n",
      "============================================================\n",
      "\n",
      "High Noon combines its points about good citizenship with some excellent picturemaking.\n",
      "============================================================\n",
      "\n",
      "Paths of Glory is all about that greatest of all movie subjects: power.\n",
      "============================================================\n",
      "\n",
      "Improbabilities and all, Simpatico still boasts wonderful scenes and a cast that is truly superb.\n",
      "============================================================\n",
      "\n",
      "As bustling and impassioned as the best Sturges and Capra movies.\n",
      "============================================================\n",
      "\n",
      "From Russia with Love is a preposterous, skillful slab of hardhitting, sexy hokum.\n",
      "============================================================\n",
      "\n",
      "Succeeds, in part, because the film is as non-judgmental as the famed sex researcher himself.\n",
      "============================================================\n",
      "\n",
      "It's an excellent movie for kids, because it is about how amazing children can be.\n",
      "============================================================\n",
      "\n",
      "Is Dr. Strangelove Kubrick's best movie? Along with Paths of Glory, absolutely.\n",
      "============================================================\n",
      "\n",
      "Hanks is superb, reemploying the childlike presence he brought to Big.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt.sort_values('pos_vs_neg_avg', ascending=False, inplace=True)\n",
    "for quote in rt.quote[0:10]:\n",
    "    print quote\n",
    "    print '============================================================\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What pulls you over the bum spots is the electrifying immediacy.\n",
      "============================================================\n",
      "\n",
      "Unoriginal and insulting, 3 Strikes goes down without scoring a single chuckle.\n",
      "============================================================\n",
      "\n",
      "Brooding, somber film is ragged around the edges and not without problematic aspects.\n",
      "============================================================\n",
      "\n",
      "Its tone is never exactly comedic and its horrific touches are more disgusting than scary.\n",
      "============================================================\n",
      "\n",
      "It's a disturbing, hopeless, irredeemable series of images that will scar you if you wander into it unprepared.\n",
      "============================================================\n",
      "\n",
      "...Liar Liar stands to make a liar out of those who predicted that Carrey's career was on the skids.\n",
      "============================================================\n",
      "\n",
      "A silly movie, with silly jokes and a silly story. But the talents at work in it are not silly.\n",
      "============================================================\n",
      "\n",
      "Likely to be disappointing to Almodovar's admirers, and inexplicable to anyone else.\n",
      "============================================================\n",
      "\n",
      "It retains the cheesy look of the 1979 original, pure schlock not gussied up to appear to be anything else.\n",
      "============================================================\n",
      "\n",
      "Both bleak and bleakly funny, appalling in its excesses and exhilarating in its execution.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt.sort_values('pos_vs_neg_avg', ascending=True, inplace=True)\n",
    "for quote in rt.quote[0:10]:\n",
    "    print quote\n",
    "    print '============================================================\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='print-most-obj'></a>\n",
    "\n",
    "## Print out the most objective and most subjective reviews\n",
    "---\n",
    "\n",
    "Do the same as above, but now sort by the objectivity. What kind of differences do you notice between these? Does our approach actually appear to capture meaningful subjectivity and objectivity in the reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Dolittle runs out of ideas long before the projector runs out of film.\n",
      "============================================================\n",
      "\n",
      "The funniest movie of 1975 and probably the silliest movie ever made.\n",
      "============================================================\n",
      "\n",
      "Either they had something on [Nick Gomez] or he needed the money.\n",
      "============================================================\n",
      "\n",
      "Some of the funniest scenes bounce off the nightmares of every bride and groom before the wedding.\n",
      "============================================================\n",
      "\n",
      "Everything about Something to Talk About feels off by a few beats.\n",
      "============================================================\n",
      "\n",
      "By the end of the film, Selena has been all but canonized.\n",
      "============================================================\n",
      "\n",
      "Magnolia makes it three-for-three for writer and director Paul Thomas Anderson\n",
      "============================================================\n",
      "\n",
      "It plays like a movie dreamed up by people in a studio marketing department who'd decided to bare their souls.\n",
      "============================================================\n",
      "\n",
      "Barbara Stanwyck is the sexiest con woman ever captured on film.\n",
      "============================================================\n",
      "\n",
      "Offers an abundance of pleasures, especially in the realm of characterization and atmosphere.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt.sort_values('objectivity_avg', ascending=False, inplace=True)\n",
    "for quote in rt.quote[0:10]:\n",
    "    print quote\n",
    "    print '============================================================\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They keep getting worse and worse and worse . . .\n",
      "============================================================\n",
      "\n",
      "At its best when it's being lighthearted and at its weakest when it takes a halfhearted stab at semi-seriousness.\n",
      "============================================================\n",
      "\n",
      "At its best, it's a valentine of venom, sent with mirth and malice aforethought.\n",
      "============================================================\n",
      "\n",
      "What pulls you over the bum spots is the electrifying immediacy.\n",
      "============================================================\n",
      "\n",
      "I am not sure why this isn't very funny, but it's not.\n",
      "============================================================\n",
      "\n",
      "Hawthorne is by turn outrageous and pathetic and imperious and poignant and very funny.\n",
      "============================================================\n",
      "\n",
      "At its best, this achieves the beauty and grandeur of a Kurosawa epic -- at its worst, however, it feels like a Python remake of The Vikings.\n",
      "============================================================\n",
      "\n",
      "In spite of its shortcomings, children love these characters and will enjoy Tigger.\n",
      "============================================================\n",
      "\n",
      "Hilarious, sexy, clever, playful and as initially teasing as it is ultimately satisfying.\n",
      "============================================================\n",
      "\n",
      "Jumps adroitly between the macho and anti-macho, the romantic and anti-romantic.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt.sort_values('objectivity_avg', ascending=True, inplace=True)\n",
    "for quote in rt.quote[0:10]:\n",
    "    print quote\n",
    "    print '============================================================\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There doesn't seem to be much signal in the objectivity score. They look pretty\n",
    "# similar to me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "\n",
    "## Build a model to classify fresh vs. rotten with the sentiment and grammar features\n",
    "\n",
    "---\n",
    "\n",
    "Let's use the features we've created to construct a Logistic Regression to predict whether a review is fresh vs. rotten. \n",
    "\n",
    "Don't forget to check the baseline score, and it's a good practice to standardize your predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.640740894059 0.615069103879\n"
     ]
    }
   ],
   "source": [
    "X = rt[['objectivity_avg','pos_vs_neg_avg','quote_len']+[x for x in rt.columns if x.endswith('_prop')]]\n",
    "y = rt.fresh.values\n",
    "\n",
    "ss = StandardScaler()\n",
    "Xs = ss.fit_transform(X)\n",
    "\n",
    "lr_scores = cross_val_score(LogisticRegression(), Xs, y, cv=10)\n",
    "print np.mean(lr_scores), rt.fresh.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We do a bit better than the baseline using the default logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'objectivity_avg',  u'pos_vs_neg_avg',       u'quote_len',\n",
       "              u'ADJ_prop',        u'ADP_prop',        u'ADV_prop',\n",
       "            u'CCONJ_prop',        u'DET_prop',       u'INTJ_prop',\n",
       "             u'NOUN_prop',        u'NUM_prop',       u'PART_prop',\n",
       "             u'PRON_prop',      u'PROPN_prop',      u'PUNCT_prop',\n",
       "            u'SPACE_prop',        u'SYM_prop',       u'VERB_prop',\n",
       "                u'X_prop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectivity_avg -0.126233159042\n",
      "pos_vs_neg_avg 0.456829490129\n",
      "quote_len 0.0972697524415\n",
      "ADJ_prop 0.0673598722125\n",
      "ADP_prop -0.00277071262457\n",
      "ADV_prop -0.116993974421\n",
      "CCONJ_prop 0.0659635231254\n",
      "DET_prop -0.0393094564402\n",
      "INTJ_prop -0.0148773028482\n",
      "NOUN_prop 0.115966462762\n",
      "NUM_prop 0.0297202568141\n",
      "PART_prop -0.0700633312946\n",
      "PRON_prop 0.0994019471933\n",
      "PROPN_prop 0.0642794454372\n",
      "PUNCT_prop -0.020160836971\n",
      "SPACE_prop -0.00129654936462\n",
      "SYM_prop 0.0122382782534\n",
      "VERB_prop -0.155693928364\n",
      "X_prop 0.0144606346576\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(Xs, y)\n",
    "for var, coef in zip(X.columns, lr.coef_[0]):\n",
    "    print var, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The coefficients make sense for the sentiment features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Try a lasso, find the best penalty.\n",
    "lrcv = LogisticRegressionCV(penalty='l1', solver='liblinear', Cs=25, cv=10)\n",
    "lrcv.fit(Xs, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# looks like it still keeps a fair amount of the features.\n",
    "lrcv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vader'></a>\n",
    "\n",
    "## Use the VADER library to get better sentiment scores\n",
    "---\n",
    "\n",
    "The [VADER](https://github.com/cjhutto/vaderSentiment) package for python is a more advanced way to calculate positivity, negativity, and objectivity in our reviews. The github page describes VADER as:\n",
    "\n",
    "> VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.\n",
    "\n",
    "You will likely need to install VADER with pip or conda. Instructions can be found on the github page. Once you have it installed you can load the `SentimentIntensityAnalyzer` and parse text.\n",
    "\n",
    "**Parse a couple of quotes with the `SentimentIntensityAnalyzer` and print out the dictionary of scores using `analyzer.polarity_scores`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in rt.quote.values[0:2]:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print sentence\n",
    "    print vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that these scores look more legitimate. VADER polarity score dictionaries have 4 elements: `neg`, `pos`, `neu` and `compound`. The compound score is a single metric that represents the \"overall\" valence.\n",
    "\n",
    "**Calculate the four scores for each review and save them as features in the dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt['vader_neg'] = 0\n",
    "rt['vader_pos'] = 0\n",
    "rt['vader_neu'] = 0\n",
    "rt['vader_compound'] = 0\n",
    "\n",
    "for i, q in enumerate(rt.quote.values):\n",
    "    vs = analyzer.polarity_scores(q)\n",
    "    rt.iloc[i, -4] = vs['neg']\n",
    "    rt.iloc[i, -3] = vs['pos']\n",
    "    rt.iloc[i, -2] = vs['neu']\n",
    "    rt.iloc[i, -1] = vs['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vader-model'></a>\n",
    "\n",
    "### Fit a model using the VADER sentiment features\n",
    "\n",
    "Does this model perform better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = rt[['vader_neg','vader_pos','vader_neu','vader_compound','quote_len']]\n",
    "y = rt.fresh.values\n",
    "\n",
    "Xs = StandardScaler().fit_transform(X)\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), Xs, y, cv=10)\n",
    "print scores\n",
    "print np.mean(scores)\n",
    "\n",
    "# We do slightly better. I've also left out the part of speech stuff so that has an impact too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression().fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c, v in zip(LogisticRegression().fit(Xs, y).coef_[0], ['neg','pos','neu','compound','len']):\n",
    "    print v, c\n",
    "    \n",
    "# All the coefficients make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vader-top'></a>\n",
    "\n",
    "### Print out the top most negative, positive, neutral, and subjective features by VADER score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rt.sort_values('vader_neg', ascending=False, inplace=True)\n",
    "for i in range(5):\n",
    "    print rt.quote.values[i]\n",
    "    print '---------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rt.sort_values('vader_pos', ascending=False, inplace=True)\n",
    "for i in range(5):\n",
    "    print rt.quote.values[i]\n",
    "    print '---------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rt.sort_values('vader_neu', ascending=False, inplace=True)\n",
    "for i in range(5):\n",
    "    print rt.quote.values[i]\n",
    "    print '---------------------------------------------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rt.sort_values('vader_neu', ascending=True, inplace=True)\n",
    "for i in range(5):\n",
    "    print rt.quote.values[i]\n",
    "    print '---------------------------------------------------\\n'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
