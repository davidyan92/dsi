{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intro to Spark\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/ieVW98.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Identify major data types used within Spark.\n",
    "- Identify basic operations within Spark for data munging.\n",
    "- Understand the Spark computation model.\n",
    "- Use SparkUI to view running \"jobs\".\n",
    "- Build simple queries and transformation for Spark via Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Pre-Work\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Install and configure the latest version of Apache Spark\n",
    "- Configure Jupyter notebook with spark context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [What is spark?](#intro)\n",
    "    - [Spark is a distributed framework for parallelized applications](#dist)\n",
    "    - [Spark is an API](#api)\n",
    "    - [Spark is machine learning](#ml)\n",
    "    - [Spark is a framework for building high volume stream processors](#stream)\n",
    "    - [Spark is a SQL interface](#sql)\n",
    "    - [Spark is parallel graph processing](#graph)\n",
    "- [Programming with Spark](#prog)\n",
    "- [Spark UI](#sparkui)\n",
    "- [Using Spark via Pyspark](#using)\n",
    "    - [Spark installation guide](#guide)\n",
    "    - [The spark context](#spark-context)\n",
    "- [RDDs vs DataFrames](#rdd)\n",
    "- [Transformations and Actions](#ta)\n",
    "    - [Common Spark transformations](#common-transformations)\n",
    "    - [Common Spark actions](#common-actions)\n",
    "- [Spark data types](#dtypes)\n",
    "    - [Resilient Distributed Dataset](#rdds)\n",
    "    - [DataFrames](#df)\n",
    "- [Common DataFrame operations and characteristics](#common-df)\n",
    "- [Some basic stats in Spark](#stats)\n",
    "- [Limiting results](#limiting)\n",
    "- [More DataFrame and Series operations](#more-ops)\n",
    "- [Activity: check out another dataset using Spark](#activity)\n",
    "- [Practical tips](#practical-tips)\n",
    "- [Additional Resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## What is Spark?\n",
    "---\n",
    "\n",
    "**How is it different and similar to:**\n",
    "\n",
    "- Hadoop\n",
    "- Map reduce\n",
    "- Random Forrests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this, \"Spark\" you speak of?\n",
    "\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "\n",
    "Spark provides an interface for programming entire computer clusters with implicit data parallelism and fault-tolerance.\n",
    "\n",
    "Spark has gained traction over the past few years because of its superior performace with respect to Hadoop-MapReduce.\n",
    "\n",
    "Spark relaxes the constraints of Map-Reduce by doing the following:\n",
    "\n",
    "- generalizes computation from Map/Reduce only graphs to arbitrary Directed Acyclic Graphs (DAGs)\n",
    "- removes a lot of boilerplate code present in Hadoop\n",
    "- allows to \"tweak\" parts that in Hadoop are not accessible, like for example the sort algorithm\n",
    "- allows to load data in a cluster memory, speeding up I/O enormously\n",
    "\n",
    "![](https://snag.gy/4oxeiA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Spark Core**: Contains the basic functionality of Spark; in particular the APIs that define RDDs and the operations and actions that can be undertaken upon them. The rest of Spark's libraries are built on top of the RDD and Spark Core.\n",
    "\n",
    "- **Spark SQL**: Provides APIs for interacting with Spark via the Apache Hive variant of SQL called Hive Query Language (HiveQL). Every database table is represented as an RDD and Spark SQL queries are transformed into Spark operations. For those that are familiar with Hive and HiveQL, Spark can act as a drop-in replacement.\n",
    "\n",
    "- **Spark Streaming**: Enables the processing and manipulation of live streams of data in real time. Many streaming data libraries (such as Apache Storm) exist for handling real-time data. Spark Streaming enables programs to leverage this data similar to how you would interact with a normal RDD as data is flowing in.\n",
    "\n",
    "- **MLlib**: A library of common machine learning algorithms implemented as Spark operations on RDDs. This library contains scalable learning algorithms like classifications, regressions, etc. that require iterative operations across large data sets. The Mahout library, formerly the Big Data machine learning library of choice, will move to Spark for its implementations in the future.\n",
    "\n",
    "- **GraphX**: A collection of algorithms and tools for manipulating graphs and performing parallel graph operations and computations. GraphX extends the RDD API to include operations for manipulating graphs, creating subgraphs, or accessing all vertices in a path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use Spark?\n",
    "\n",
    "Imagine you have to run a linear regression over several Terabytes of data. A few obvious problems arise:\n",
    "\n",
    "- It's hard to find a place to store all of that data.\n",
    "- It would take a painfully long time to compute the regression.\n",
    "\n",
    "Spark helps both of those problems by allowing us to distribute the data oer several computers and to use main memory to speed up the computation. Main memory runs about 1000 times faster than hard disk, but real performance is more complex than just those two components.\n",
    "\n",
    "Spark is also often used for streaming data. Perhaps a company needs instant information about a large number of incoming tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is many things.  \n",
    "\n",
    "- It's a set of tools for developing applications.  \n",
    "- It's parallel processing of applications, in a distributed environment.\n",
    "\n",
    "![](https://snag.gy/c9b1Kx.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dist'></a>\n",
    "### Spark is a distributed computer framework for parallelized applications like _Hadoop_.\n",
    "\n",
    "_Spark can interact with Hadoop's HDFS to access large amounts of data using high volume, distributed I/O._\n",
    "\n",
    "![](https://snag.gy/s8gSlG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='api'></a>\n",
    "### Spark is an API for handling large data tranformations like _Map Reduce_.\n",
    "\n",
    "_It is a data transformation and selection tool like **Pandas**.  You can develop transformations through an API that allows you to chain operations together in a modular fashion, similar to **Pandas**._\n",
    "\n",
    "![](https://snag.gy/9G4gJO.jpg)\n",
    "\n",
    "> However, Spark delivers the idea of data manipulation through the framework of **transformations** and **actions**.  These transformations and actions are performed in parallel, using many different worker nodes (which may be distributed on multiple machines).\n",
    "\n",
    "> Mapreduce is broken down into two main functions \"map\" and \"reduce\".  Spark on the other hand, operates through a set of operations determined through a **Directed Acyclic Graph** (or DAG for short)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>\n",
    "### Spark is machine learning like _Scikit Learn_.\n",
    "\n",
    "![](https://snag.gy/RnuX6h.jpg)\n",
    "\n",
    "_Spark provides an interface to MLib via Scala, Java, Python, and R.  The most common methods are provided such as regression, support vector machines, and random forrests, however, not all evaluation metrics are available in Python yet.  Spark is written in Scala, so features are prioritized to Scala first throughout the Spark ecosystem._\n",
    "\n",
    "> <i class=\"fa fa-question-circe\"></i> Anyone remember specific limitations? (You might not if you haven't done MLlib with Spark yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stream'></a>\n",
    "### Spark is a framework for building high volume stream processors.\n",
    "![](https://snag.gy/RCikuU.jpg)\n",
    "\n",
    "With Spark streaming, it's possible to build a proccess that can respond to data in real-time, using any of Spark's features including Mlib, GraphX, or any kinds of transformations you could do within the Spark context.  The streaming capabilities of Spark core make it possible to prodice real-time applications such as ETL, analytics dashboards, data mining, or large scale aggregations.\n",
    "\n",
    "In short, you can create a \"streaming context\" that listens on a specific port, and tie to to any number of operations that can be programed with spark.\n",
    "\n",
    ">```python\n",
    "># Save this as a file called \"network_streaming.py\"\n",
    ">from pyspark import SparkContext\n",
    ">from pyspark.streaming import StreamingContext\n",
    ">\n",
    "># Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    ">sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    ">ssc = StreamingContext(sc, 1)\n",
    ">\n",
    "># Create a DStream that will connect to hostname:port, like localhost:9999\n",
    ">lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    ">\n",
    "># Split each line into words\n",
    ">words = lines.flatMap(lambda line: line.split(\" \"))\n",
    ">\n",
    "># Count each word in each batch\n",
    ">pairs = words.map(lambda word: (word, 1))\n",
    ">wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    ">\n",
    "># Print the first ten elements of each RDD generated in this DStream to the console\n",
    ">wordCounts.pprint()\n",
    ">\n",
    ">ssc.start()             # Start the computation\n",
    ">ssc.awaitTermination()  # Wait for the computation to terminate\n",
    ">```\n",
    "\n",
    "Then you can have Spark run this stream using the unix utility called **netcat** to route your service once you run spark-submit.\n",
    "\n",
    ">```bash\n",
    ">$ nc -lk 9999\n",
    ">```\n",
    "\n",
    "Running spark-submit launches applications onto your Spark cluster.\n",
    "\n",
    ">```bash\n",
    ">./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sql'></a>\n",
    "### Spark is a SQL interface into dataframes like _Hive_.\n",
    "\n",
    "Spark isn't actually **Hive**, but it uses components from Hive.  However, you can use Spark dataframes easier to use with temporary SQL views.\n",
    "\n",
    ">```python\n",
    "># Load a dataset as a Spark DataFrame\n",
    ">df = spark.read.csv(\"datasets/somedataset/hamburgers_eaten_per_hour.csv\")\n",
    ">df.createOrReplaceTempView(\"hamburgers\")\n",
    ">```\n",
    "\n",
    "\n",
    "\n",
    "Then viola, you can slice and dice your dataframe as SQL:\n",
    "\n",
    ">```python\n",
    ">spark.sql(\"SELECT * FROM hamburgers\").show()\n",
    ">\n",
    "># +------+---------+\n",
    "># | eaten|     name|\n",
    "># +------+---------+\n",
    "># |null  |     Jeff|\n",
    "># |  30  |   Kiefer|\n",
    "># |  19  |     Hang|\n",
    "># +------+---------+\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='graph'></a>\n",
    "### Spark is also parallel graph processing.\n",
    "\n",
    "![](https://snag.gy/0KvrCw.jpg)\n",
    "\n",
    "Graph processing provides the ability to process data in terms of relationships.  The area of applications where traditional RDBMS systems fail to provide involve calculating relationships between entities that don't have a predetermined depth between sets.  \n",
    "\n",
    "Using graphs, you can represent irregular relationship shapes within data and apply functions recurvively and/or arbirarily on any depth of relationships.\n",
    "\n",
    "Both RDBMS and Graphs can be abused to equate features of the other but each have their strengths and tradeoffs depending on application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prog'></a>\n",
    "## Programming with Spark\n",
    "---\n",
    "\n",
    "Running applications build with Spark has a variety of options:\n",
    "\n",
    "- Pyspark\n",
    "- spark-submit\n",
    "\n",
    "### Pyspark\n",
    "Pyspark is a real-time interpreter that talks from Spark to Python.  You can integrate operations with the Spark Python libraries in real-time.  This is a great way to prototype applications much in the same way we do with Jupyter notebook.  It's also possible to connect Pyspark to Jupyter.\n",
    "\n",
    "### Spark-submit\n",
    "Spark-submit, on the other hand, is a way to run a set of application instructions bundled in a single file. It's possible to write these in Python, Scala, or Java.  Typically, it's convienient to prototype a set of operations that you want performed on a dataset with Pyspark, debug them to a high level of quality, then run them with Spark-submit.\n",
    "\n",
    "> With spark-submit, it's also possible to tune the paramters in which your application will run to a very granular degree including memory, number of total cores, and specific cluster modes to control test vs production deployments.\n",
    ">\n",
    "> [Read more about submitting applications](http://spark.apache.org/docs/latest/submitting-applications.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sparkui'></a>\n",
    "## Spark UI\n",
    "---\n",
    "\n",
    "Anytime a \"spark context\" is created, a cooresponding spark UI is launched.  Whenever you launch a Spark standalone instance, Pyspark, a Spark UI will be created.  Through the web UI, you can monitor how your applications run.  Anything that the Spark context handles, even the one line operations from PySpark can be observed as seperate jobs in Spark UI.\n",
    "\n",
    "### Spark Jobs\n",
    "![](https://snag.gy/JnuSKC.jpg)\n",
    "\n",
    "### Job Metrics\n",
    "![](https://snag.gy/JW2fOb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='using'></a>\n",
    "## Using Spark via Pyspark\n",
    "---\n",
    "\n",
    "If you haven't done so yet, it's nice to be able to install Spark manually so you can get familliar with how to use it locally.  The advantage of this, as apposed to using a virtual machine, is that you should be able to run Spark without the overhead of an additional virtual operating system running on top of your already \"actual\" operating system.\n",
    "\n",
    "<a id='guide'></a>\n",
    "### Spark installation guide\n",
    "> This is a good time to review or install Spark from our guide if you're installing Spark on your local machine:\n",
    ">\n",
    ">[Spark Installation Guide](./spark-installation-guide.md)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> <img src=\"https://snag.gy/OKyLhW.jpg\" style=\"width: 100px; float: left; margin-right: 25px;\">For those who are using Docker, this image / container combo is useful:<br>\n",
    "> ```bash \n",
    "docker run -p 8890:8888 -p 4040:4040 -p 4041:4041 -v `pwd`:/home/jovyan jupyter/pyspark-notebook start-notebook.sh \n",
    "```\n",
    "> _This will run a notebook instance available @ http://localhost:8890_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='spark-context'></a>\n",
    "### Pyspark is all about \"sc\"\n",
    "\n",
    "If you install Spark on your local machine using our guide, `sc` Spark Context is loaded in memory by default.  However, if you are using the recommended Docker container, you will need to manually create this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This will exist if you have used our installation guide for manual configuration\n",
    "# sc\n",
    "\n",
    "## Use this to set Spark context if running on Docker \n",
    "## Also, careful running this frame more than 1x!\n",
    "import pyspark, os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0 pyspark-shell'\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "## Additionally we have to create a spark session \n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Jupyter-Pyspark\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.0.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UI actually runs mapped @ http://localhost:4040, not http://172.17.0.2:4040\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"sc\"** represents your interface to a running spark cluster manager.  A Spark context is defined as a preconfigured cluster, an application name connected to it.  All **transformations** and **actions** performed by Spark, are handled through the Spark context (aka: **sc**).\n",
    "\n",
    ">**Context for standalone Spark apps**\n",
    ">\n",
    ">If we where to write a standalone app for spark-submit, we would need to create the context manually.\n",
    "\n",
    ">```python\n",
    ">from pyspark import SparkContext\n",
    ">sc = SparkContext(\"local\", \"Simple App\")\n",
    ">```\n",
    "\n",
    ">Otherwise, in Pyspark, and in the case of a connected Jupyter notebook, it's already created for us as a result of running **`pyspark`** from the shell.\n",
    "\n",
    "<img src=\"https://snag.gy/iCm4G1.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdd'></a>\n",
    "## RDDs vs DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "The two main types of data objects in Spark are the **Resilient Distributed Dataset** and the **DataFrame**.  Both types represent data in a distributed state.  RDDs store data in a more primitive state such as a list of pairs, integers, floats, or strings.  DataFrames have a rich structure defintion called a **schema** much like a Pandas dataframe.\n",
    "\n",
    "- You use **RDDs** to manage semi-structured data.\n",
    "- You use **DataFrames** to operate on typed series.\n",
    "\n",
    "Both RDDs and DataFrames can contain multiple types of objects.  **DataFrames** are much more constrained because data is represented by a 2 deminsional tabular structure where columns represent variables, and rows as observations.  **RDDs** are much more flexible if your data requires much less structure than a DataFrame while still being able to use _transformation_ methods such as **`map()`** and _action_ methods such as **`reduce()`**.\n",
    "\n",
    ">_Distributed Data in Spark_\n",
    ">![](https://snag.gy/vxVhri.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta'></a>\n",
    "\n",
    "## _Transformations_ and _Actions_\n",
    "\n",
    "---\n",
    "\n",
    "<a id='common-transformations'></a>\n",
    "### <i class=\"fa fa-cogs\" aria-hidden=\"true\"></i> Common Spark transformations\n",
    "\n",
    "\n",
    "Generally, **transformations** in Spark, modify data as new copies of whichever dataset that is used as input.  An analogy you will find the default docs talking about is _map_ as a **transformation**, and _reduce_ as an **action**.  **Transformations are lazy**, so no computations are performed until an **action** is requested.\n",
    "\n",
    "<style>\n",
    ".no-border:table, th, td {\n",
    "    border: none;\n",
    "    border-left: none;\n",
    "    margin: 10px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table class=\"no-border\" style=\"border: none;\">\n",
    "<tbody class=\"no-border\" style=\"border: none;\"><tr class=\"no-border\" style=\" background: #000000; color: #FFFFFF;\"><th>Transformation</th><th style=\"border: none; text-align: center;\">Meaning</th></tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border: none;\"> <b>map</b>(<i>func</i>) </td>\n",
    "  <td stye=\"border-right: none;\"> Return a new distributed dataset formed by passing each element of the source through a function <i>func</i>. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>filter</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Return a new dataset formed by selecting those elements of the source on which <i>func</i> returns true. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Sample a fraction <i>fraction</i> of the data, with or without replacement, using a given random number generator seed. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>distinct</b>([<i>numTasks</i>])) </td>\n",
    "  <td style=\"border-right: none;\"> Return a new dataset that contains the distinct elements of the source dataset.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>groupByKey</b>([<i>numTasks</i>]) <a name=\"GroupByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. <br>\n",
    "    <b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or\n",
    "      average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better\n",
    "      performance.\n",
    "    <br>\n",
    "    <b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.\n",
    "      You can pass an optional <code>numTasks</code> argument to set a different number of tasks.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>reduceByKey</b>(<i>func</i>, [<i>numTasks</i>]) <a name=\"ReduceByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numTasks</i>]) <a name=\"AggregateByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>sortByKey</b>([<i>ascending</i>], [<i>numTasks</i>]) <a name=\"SortByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>join</b>(<i>otherDataset</i>, [<i>numTasks</i>]) <a name=\"JoinLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "    Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the\n",
    "    process's stdin and lines output to its stdout are returned as an RDD of strings. </td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> For more info on **transformations**, check out the [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Spark programming guide on #trasformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations)\n",
    "\n",
    "> A great visual guide for common Spark transformations is available by [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Jeff Thompson @ Databricks](http://training.databricks.com/visualapi.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common-actions'></a>\n",
    "### <i class=\"fa fa-wrench\" aria-hidden=\"true\"></i> Common Spark Actions\n",
    "\n",
    "Spark **actions** are not performed until they are called (like a function) -- common sense right?  Careful, because in _Pandas_ we're accustomed to getting results as a result of most transformations performed.  So, if we have a few operations programmed out, nothing will actually be transformed until we call an **action**.\n",
    "\n",
    "```python\n",
    "# 1. Read text file\n",
    "#########\n",
    "# This line reads a text file from the filesystem.  Each line becomes an element in a list-like object (RDD).\n",
    "# No operations have been done as of yet.  We don't know if the text file actually exists or read any data.\n",
    "#############################################################################################################\n",
    "\n",
    "text_lines = sc.textFile(\"somefile.txt\")\n",
    "\n",
    "# 2. Map lengths of each line\n",
    "#########\n",
    "# Once data is available, we simply count the length of lines (length of string), and create a new RDD \n",
    "# which only has the length of each line as a new object.\n",
    "#############################################################################################################\n",
    "\n",
    "text_line_lengths = text_lines.map(lambda s: len(s))\n",
    "\n",
    "# 3. Reduce to find total sum of lengths\n",
    "#########\n",
    "# Once reduce is called, all prior operations are run.  Meaning, we actually sc.textFile(), \n",
    "# and text_lines.Map(), before finally text_line_lengths.reduce() is ran.\n",
    "#############################################################################################################\n",
    "\n",
    "total_length = text_line_lengths.reduce(lambda a, b: a + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table\" style=\"border-left: none; border-right: none;\">\n",
    "<tbody style=\"border-left: none;\"><tr style=\"background: black; color: white;\"><th>Action</th><th style=\"text-align: center;\">Meaning</th></tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>reduce</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>collect</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>count</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Return the number of elements in the dataset. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>first</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Return the first element of the dataset (similar to take(1)). </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>take</b>(<i>n</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Return an array with the first <i>n</i> elements of the dataset. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>\n",
    "  <td style=\"border-right: none;\"> Return an array with a random sample of <i>num</i> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Return the first <i>n</i> elements of the RDD using either their natural order or a custom comparator. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>saveAsTextFile</b>(<i>path</i>) </td>\n",
    "  <td style=\"border-right: none; border-right: none;\"> Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>countByKey</b>() <a name=\"CountByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>foreach</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Run a function <i>func</i> on each element of the dataset. This is usually done for side effects such as updating an <a href=\"#accumulators\">Accumulator</a> or interacting with external storage systems.\n",
    "  <br><b>Note</b>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href=\"#understanding-closures-a-nameclosureslinka\">Understanding closures </a> for more details.</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> For more info on **actions**, check out the [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Spark programming guide on #actions](http://spark.apache.org/docs/latest/programming-guide.html#actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i> (~1 min)  How might you approach programming with Spark vs Pandas?\n",
    "\n",
    "_Given that operations are \"lazy\"?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dtypes'></a>\n",
    "## Spark data types\n",
    "---\n",
    "\n",
    "### RDD's\n",
    "\n",
    "It's best to think of RDDs as primitive objects that are distributed.  RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.  The RDD type is the oldest type and has been around the Spark codebase since the version 1.0 days.\n",
    "\n",
    "> Neat trick:  Since RDDs can be Python class types, it's possible to do things like run scikit-learn's `GridSearch` over an existing scikit model and paramter search over a massage amount of permutations in a clustered Spark setup.\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "As part of the \"Tungsten Initiative\", which sought to improve the performance of Spark, DataFrames entered the Spark codebase in version 1.3.  The big difference between RDD's and DataFrames, is that DataFrames introduce the idea of a \"schema\" much like Pandas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdds'></a>\n",
    "<span style=\"font-size: 25pt; font-weight: light;\"><strong>R</strong>eslient <strong>D</strong>istributed <strong>D</strong>ataset</span>\n",
    "<hr>\n",
    "\n",
    "\n",
    "Everything in Spark revolves around the **RDD**.\n",
    "\n",
    "> \"..a fault-tolerant collection of elements that can be operated on in parallel...\"\n",
    "> _-Spark Documentation_\n",
    "\n",
    "Most datasets that can be loaded externally (csv, HDFS/hadoop, text files, RDBMS/SQL, etc), become an RDD and thus operated on in parallel within a distruted system (Spark!).  Otherwise, you can `sc.parallelize(my_data)` a dataset through a Spark context (aka a driver program as it's sometimes called)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TBD:  More examples with RDD's.. we will have more fun with DataFrames for now since they are more relatable\n",
    "# RDD's are great for custom jobs and operations that require a more open format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check out this Gridsearch with Spark package on [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Github: spark-sklearn](https://github.com/databricks/spark-sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df'></a>\n",
    "### DataFrames\n",
    "\n",
    "The big plus is that Spark DataFrames serializes its data at a lower level to native Java/Scala, so when it's passed between nodes, it's much more performant, requiring fewer processes to handle computations.  Mainly data can be processed faster when it's optimized to a common format [the schema] that Spark doesn't have to convert to in order to perform tasks on it.\n",
    "\n",
    "Outside of the performance optimizations introduced with a schema-based datastructure, the **DataFrame API** provides a convienient set of selectors for transforming data, much like Pandas.  Lastly, it's possible to create temporary views in which **DataFrames** can be queried with SQL - **SparkSQL**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    path        =   \"./datasets/sentiment_words_simple.csv\",\n",
    "    header      =   True, \n",
    "    mode        =   \"DROPMALFORMED\",   # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    inferSchema =   True               # Not always perfect but works well in most cases as of 2.1+\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- pos_score: double (nullable = true)\n",
      " |-- neg_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common-df'></a>\n",
    "## Common DataFrame operations and characteristics\n",
    "---\n",
    "\n",
    "Let's have a look at some familliar and new functions and properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect variable / column space of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos', 'word', 'pos_score', 'neg_score']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTypes\n",
    "Inspect schema programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pos', 'string'),\n",
       " ('word', 'string'),\n",
       " ('pos_score', 'double'),\n",
       " ('neg_score', 'double')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain DataFrame\n",
    "Show details about DataFrame type, schema, and origin of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation[pos#965,word#966,pos_score#967,neg_score#968] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "pos: string, word: string, pos_score: double, neg_score: double\n",
      "Relation[pos#965,word#966,pos_score#967,neg_score#968] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation[pos#965,word#966,pos_score#967,neg_score#968] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*FileScan csv [pos#965,word#966,pos_score#967,neg_score#968] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/dsi-plus/cloud-intro_to_spark-lesson/datasets/sentiment_words..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pos:string,word:string,pos_score:double,neg_score:double>\n"
     ]
    }
   ],
   "source": [
    "df.explain(extended=True) # Also check out extended stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe\n",
    "Describe will look similar to synonymous Pandas **Describe** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "|summary|   pos|                word|          pos_score|          neg_score|\n",
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "|  count|155287|              155287|             155287|             155287|\n",
      "|   mean|  null| 5.687506389495568E9|0.03865380738372139|0.05048873043068892|\n",
      "| stddev|  null|7.537744261701462E10|0.11118246263109259| 0.1392276319041252|\n",
      "|    min|   adj|               'hood|                0.0|                0.0|\n",
      "|    max|  verb|              zyrian|                1.0|                1.0|\n",
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check Pandas version here!!!\n",
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>155287.000000</td>\n",
       "      <td>155287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.038654</td>\n",
       "      <td>0.050489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.111182</td>\n",
       "      <td>0.139228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pos_score      neg_score\n",
       "count  155287.000000  155287.000000\n",
       "mean        0.038654       0.050489\n",
       "std         0.111182       0.139228\n",
       "min         0.000000       0.000000\n",
       "25%         0.000000       0.000000\n",
       "50%         0.000000       0.000000\n",
       "75%         0.000000       0.000000\n",
       "max         1.000000       1.000000"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema\n",
    "The schema is a very import characteristic of a Spark DataFrame.  It tells us what's possible in terms of transformation.  Also, it's the reason DataFrames are so fast since they are typed to a set number of types that are serialized and optimized in Java/Scala behind the scenes.\n",
    "\n",
    "><i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i> The \"schema\" that we've been so excited to see is finally here to explore.  Feel free to take a screenshot and show your friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- pos_score: double (nullable = true)\n",
      " |-- neg_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count\n",
    "Count with caveat:  This will return the count of all rows, including _non-NaN_ values.  Pandas will omit these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155287"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stats'></a>\n",
    "## Some basic stats in spark\n",
    "---\n",
    "\n",
    "### Covariance\n",
    "<span style=\"font-size: 20pt;\">\n",
    "$\\operatorname{cov}(X,Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}$\n",
    "</span>\n",
    "> \"Covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019032076361625806"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cov(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Score\n",
    "<span style=\"font-size: 20pt;\">\n",
    "$\\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y} $\n",
    "</span>\n",
    "> The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12294884293406051"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='limiting'></a>\n",
    "## Limiting results\n",
    "\n",
    "---\n",
    "\n",
    "### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|\n",
      "|adj|.38_caliber|      0.0|      0.0|\n",
      "|adj|.38_calibre|      0.0|      0.0|\n",
      "|adj|.45-caliber|      0.0|      0.0|\n",
      "|adj|.45-calibre|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show limited results\n",
    "\n",
    "With Pandas we're used to the `df.head()` as a first step in exploring a dataset.  With Spark this isn't exactly the same.  You need to use the `df.show()`, operation in order to explore data as a first step.  Where Pandas formats its DataFrame output for display in nice HTML tables with sensible defaults for output, you have have to be a bit more specific about what you're looking at with `show()` when using Spark.\n",
    "\n",
    "\n",
    "> The paramters `truncate` is helpful for truncating attributes for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---------+\n",
      "|pos|      word|pos_score|neg_score|\n",
      "+---+----------+---------+---------+\n",
      "|adj|.22-cal...|      0.0|      0.0|\n",
      "|adj|.22-cal...|      0.0|      0.0|\n",
      "|adj|.22_cal...|      0.0|      0.0|\n",
      "|adj|.22_cal...|      0.0|      0.0|\n",
      "|adj|.38-cal...|      0.0|      0.0|\n",
      "|adj|.38-cal...|      0.0|      0.0|\n",
      "|adj|.38_cal...|      0.0|      0.0|\n",
      "|adj|.38_cal...|      0.0|      0.0|\n",
      "|adj|.45-cal...|      0.0|      0.0|\n",
      "|adj|.45-cal...|      0.0|      0.0|\n",
      "+---+----------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.show(10, truncate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i> Why should you need to be careful when displaying data in Spark?\n",
    "\n",
    "Hopefully you can see why Pandas is so nice to use for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more-ops'></a>\n",
    "## More DataFrame and Series operations\n",
    "---\n",
    "\n",
    "### Convert from list of Row objects to list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.22-caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.22-calibre'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.22_caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.22_calibre'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.38-caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.38-calibre'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.38_caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.38_calibre'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.45-caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.45-calibre'}]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A: (asDict())\n",
    "[row.asDict() for row in df.take(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting DataFrame Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting variables with Spark **DataFrames API** works very similliarly to Pandas DataFrames.  When selecting variables in Pandas use use a `list` object, passed to a DataFrame object via `[]` brackets like so:\n",
    "\n",
    ">```df[['col1', 'col2']]```\n",
    "\n",
    "The equivalent in spark is using the `.select()` method which takes flat parameters.\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> DataFrames in Pandas are implemented with an overloaded brackets operator so whenever we reference any Pandas DataFrames object with brackets, it get's passed down to a function that handles the column references (as a list), as the input to a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select all features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.select(\"*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.22_caliber</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.22_calibre</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  pos_score\n",
       "0  .22-caliber        0.0\n",
       "1  .22-calibre        0.0\n",
       "2  .22_caliber        0.0\n",
       "3  .22_calibre        0.0\n",
       "4  .38-caliber        0.0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df[[\"word\", \"pos_score\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select specific features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|       word|pos_score|\n",
      "+-----------+---------+\n",
      "|.22-caliber|      0.0|\n",
      "|.22-calibre|      0.0|\n",
      "|.22_caliber|      0.0|\n",
      "|.22_calibre|      0.0|\n",
      "|.38-caliber|      0.0|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.select(\"word\", \"pos_score\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Operations\n",
    "\n",
    "With Pandas, you can easily create a new series that's the sum of every row in **\"col1\"**, to **\"col2\"** like so:\n",
    "\n",
    "> `df['col1'] + df['col2']`\n",
    "\n",
    "In Spark, we have to do this through the select function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|(neg_score + pos_score)|\n",
      "+-----------------------+\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.select(df['neg_score'] + df['pos_score']).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As an \"alias\"\n",
    "As selections are keyed by conditions, they can become hard to read.  We can use an \"alias\" to abstract any selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_score|\n",
      "+-----------+\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A: we \"alias\" the inner result / compound object\n",
    "df.select((df['neg_score'] + df['pos_score']).alias(\"total_score\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+-----------+\n",
      "|pos|       word|pos_score|neg_score|total_score|\n",
      "+---+-----------+---------+---------+-----------+\n",
      "|adj|.22-caliber|      0.0|      0.0|        0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|        0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|        0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|        0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|        0.0|\n",
      "+---+-----------+---------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df = df.withColumn(\"total_score\", (df['neg_score'] + df['pos_score']))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-question-circle\"></i> Have we changed the original DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data\n",
    "In Pandas we use \"masks\" through dataframe object brackets in order to filter data.\n",
    ">`df[df['feature'] > 0]`\n",
    "\n",
    "In Spark, we use the `filter()` method to select different aspects of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------+---------+-----------+\n",
      "|pos|         word|pos_score|neg_score|total_score|\n",
      "+---+-------------+---------+---------+-----------+\n",
      "|adj|2-dimensional|      0.0|    0.625|      0.625|\n",
      "|adj|        a-one|    0.625|      0.0|      0.625|\n",
      "|adj|     abatable|    0.625|      0.0|      0.625|\n",
      "|adj|    abdicable|      0.5|    0.125|      0.625|\n",
      "|adj|     aberrant|      0.5|    0.125|      0.625|\n",
      "+---+-------------+---------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.filter(df['total_score'] > .5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple conditions\n",
    "SAME AS PANDAS!  Thankfully, we don't have to leave our comfort zone with too many oddities here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------+--------------+--------------+\n",
      "| pos|       word|pos_score|     neg_score|   total_score|\n",
      "+----+-----------+---------+--------------+--------------+\n",
      "| adj|  cheapjack|      0.0|           1.0|           1.0|\n",
      "| adj| deplorable|      0.0|0.916666666667|0.916666666667|\n",
      "| adj|distressing|      0.0|        0.9375|        0.9375|\n",
      "| adj|  henpecked|      0.0|           1.0|           1.0|\n",
      "| adj| lamentable|      0.0|           1.0|           1.0|\n",
      "|noun|  angriness|      0.0|           1.0|           1.0|\n",
      "|noun| blackguard|      0.0|           1.0|           1.0|\n",
      "|noun| motormouth|      0.0|           1.0|           1.0|\n",
      "|noun|  scut_work|      0.0|           1.0|           1.0|\n",
      "|noun|   shitwork|      0.0|           1.0|           1.0|\n",
      "+----+-----------+---------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.filter((df['neg_score'] > .9) & (df['pos_score'] == 0)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter as an expression\n",
    "Pandas has a similar function called \"where\".  However, with Spark `filter`, we can filter by shorthand expressions when referencing column sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------+---------+-----------+\n",
      "| pos|           word|pos_score|neg_score|total_score|\n",
      "+----+---------------+---------+---------+-----------+\n",
      "|noun|   admirability|      1.0|      0.0|        1.0|\n",
      "|noun|  admirableness|      1.0|      0.0|        1.0|\n",
      "|noun|          bliss|      1.0|      0.0|        1.0|\n",
      "|noun|   blissfulness|      1.0|      0.0|        1.0|\n",
      "|noun|     cloud_nine|      1.0|      0.0|        1.0|\n",
      "|noun|congratulations|      1.0|      0.0|        1.0|\n",
      "|noun|      extolment|      1.0|      0.0|        1.0|\n",
      "|noun|    first-rater|      1.0|      0.0|        1.0|\n",
      "|noun|          kudos|      1.0|      0.0|        1.0|\n",
      "|noun|research_worker|      1.0|      0.0|        1.0|\n",
      "+----+---------------+---------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.filter(\"pos_score > .9 AND pos = 'noun'\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+---------+-----------+\n",
      "| pos|      word|pos_score|neg_score|total_score|\n",
      "+----+----------+---------+---------+-----------+\n",
      "|noun|blackguard|      0.0|      1.0|        1.0|\n",
      "|noun| angriness|      0.0|      1.0|        1.0|\n",
      "|noun| scut_work|      0.0|      1.0|        1.0|\n",
      "|noun|  shitwork|      0.0|      1.0|        1.0|\n",
      "|noun|motormouth|      0.0|      1.0|        1.0|\n",
      "+----+----------+---------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "df.filter(\"pos = 'noun'\").sort(df['neg_score'].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AS SQL!?\n",
    "\n",
    "\n",
    "Working with DataFrames as SQL is as easy as creating a **temporary view**.\n",
    "\n",
    "> <img src=\"attachment:image.png\" style=\"width: 200px; text-align: left;\"> Can your Pandas do that!?  This is one of Spark's best features that is easily overlooked.  There are performance concerns with this but if you allocate enough memory for this feature in larger Spark setups, this is a very powerful tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "df.createOrReplaceTempView(\"sentiment_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Views select DataFrames\n",
    "So the same transformations can be applied to DataFrames as we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+-----------+\n",
      "|pos|       word|pos_score|neg_score|total_score|\n",
      "+---+-----------+---------+---------+-----------+\n",
      "|adj|.22-caliber|      0.0|      0.0|        0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|        0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|        0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|        0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|        0.0|\n",
      "+---+-----------+---------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "sentiment = spark.sql(\"SELECT * FROM sentiment_view LIMIT 100\")\n",
    "sentiment.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activity'></a>\n",
    "## Activity:  Check out another dataset using Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load up the \"Pokemon\" basic Pokedex dataset\n",
    "First try without infering the schema and without the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check out the dataset with infer schema paramter but without header.\n",
    "How does it work with / without?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Create a tempory view with the Pokedex DataFrame called \"pokemon\"\n",
    "Then \n",
    "```sql SELECT * FROM pokemon LIMIT 10```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a Which is the strongest Pokemon by `Type`?\n",
    "Using Spark DataFrame operations.  Research Sparks \"grouping\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b Which is the strongest Pokemon by Type?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a Which Pokemon has the best combined Attack and Defence?\n",
    "Using Spark DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b Which Pokemon has the best combined Attack and Defence?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create a new feature called \"Pokevalue\" that is the combined Attack, Defence and scaled by .2 of the Pokemon HP.\n",
    "\n",
    "Use any means necessary to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical-tips'></a>\n",
    "# <i class=\"fa fa-thumbs-up\" aria-hidden=\"true\"></i> Practical tips\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from enormous datasets\n",
    "Undoubtably, you may have the need to examine a larger dataset.  A common operation is to take a sample.  To approximate the characteristics of your global distribution, you should try to adjust the size that best matches the metrics of central tendency or consider doing a power analysis to determine sample sizing.\n",
    "\n",
    "> Size of your sample generally depends on your application be it A/B testing, EDA, Machine Learning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "## Additional Resources\n",
    "[Qubole: Apache Spark Use Cases](https://www.qubole.com/blog/big-data/apache-spark-use-cases/)\n",
    "\n",
    "[Spark Examples](http://spark.apache.org/examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Configuration\n",
    "If there's time to go over this, we might connect to an S3 bucket on AWS to pull some data into a dataframe.  In order to do this, we will need to use our AWS credentials in order to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Access Key: \n",
      "AWS Secret Key: \n"
     ]
    }
   ],
   "source": [
    "## Under the hood, we need to include support for S3 so we have to tell the underlying Scala / Java system to \n",
    "## fetch the appriate library and include it into context in PySpark.\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "\n",
    "myAccessKey = input(\"AWS Access Key: \") \n",
    "mySecretKey = input(\"AWS Secret Key: \")\n",
    "\n",
    "hadoopConf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoopConf.set(\"fs.s3.awsAccessKeyId\", myAccessKey)\n",
    "hadoopConf.set(\"fs.s3.awsSecretAccessKey\", mySecretKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "## This can work with general purpose formats outside of CSV as well\n",
    "df = spark.read.load(\"s3://irs-form-990/index_2016.csv\", \n",
    "                              format='com.databricks.spark.csv', \n",
    "                              header='true',\n",
    "                              delimiter=',',\n",
    "                              inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## In Spark 2.x+ we can use this function for loading csv specifically\n",
    "df = spark.read.csv(\"s3://irs-form-990/index_2016.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RETURN_ID: integer (nullable = true)\n",
      " |-- FILING_TYPE: string (nullable = true)\n",
      " |-- EIN: integer (nullable = true)\n",
      " |-- TAX_PERIOD: integer (nullable = true)\n",
      " |-- SUB_DATE: string (nullable = true)\n",
      " |-- TAXPAYER_NAME: string (nullable = true)\n",
      " |-- RETURN_TYPE: string (nullable = true)\n",
      " |-- DLN: long (nullable = true)\n",
      " |-- OBJECT_ID: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "my_stopwords = list(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['always',\n",
       " 'some',\n",
       " 'thick',\n",
       " 'becomes',\n",
       " 'seems',\n",
       " 'yourself',\n",
       " 'top',\n",
       " 'those',\n",
       " 'still',\n",
       " 'co',\n",
       " 'move',\n",
       " 'already',\n",
       " 'only',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'cant',\n",
       " 'behind',\n",
       " 'moreover',\n",
       " 'full',\n",
       " 'somehow',\n",
       " 'whatever',\n",
       " 'ourselves',\n",
       " 'meanwhile',\n",
       " 'few',\n",
       " 'mill',\n",
       " 'five',\n",
       " 'might',\n",
       " 'out',\n",
       " 'yours',\n",
       " 'part',\n",
       " 'fire',\n",
       " 'bill',\n",
       " 'everything',\n",
       " 'last',\n",
       " 'this',\n",
       " 'twelve',\n",
       " 'whereupon',\n",
       " 'you',\n",
       " 'herein',\n",
       " 'call',\n",
       " 'herself',\n",
       " 'less',\n",
       " 'we',\n",
       " 'and',\n",
       " 'between',\n",
       " 'me',\n",
       " 'can',\n",
       " 'hundred',\n",
       " 'our',\n",
       " 'after',\n",
       " 'but',\n",
       " 'am',\n",
       " 'a',\n",
       " 'same',\n",
       " 'someone',\n",
       " 'cannot',\n",
       " 'together',\n",
       " 'with',\n",
       " 'side',\n",
       " 'next',\n",
       " 'sometimes',\n",
       " 'everyone',\n",
       " 'formerly',\n",
       " 'wherein',\n",
       " 'their',\n",
       " 'elsewhere',\n",
       " 'she',\n",
       " 'further',\n",
       " 'themselves',\n",
       " 'first',\n",
       " 'than',\n",
       " 'there',\n",
       " 'while',\n",
       " 'any',\n",
       " 'without',\n",
       " 'around',\n",
       " 'is',\n",
       " 'what',\n",
       " 'latter',\n",
       " 'etc',\n",
       " 'for',\n",
       " 'go',\n",
       " 'noone',\n",
       " 'else',\n",
       " 'except',\n",
       " 'cry',\n",
       " 'either',\n",
       " 'whereafter',\n",
       " 'of',\n",
       " 'whence',\n",
       " 'are',\n",
       " 'fifteen',\n",
       " 'well',\n",
       " 'mostly',\n",
       " 'however',\n",
       " 'un',\n",
       " 'as',\n",
       " 'also',\n",
       " 'nowhere',\n",
       " 'it',\n",
       " 'has',\n",
       " 'then',\n",
       " 'within',\n",
       " 'why',\n",
       " 'former',\n",
       " 'will',\n",
       " 'because',\n",
       " 'seeming',\n",
       " 'others',\n",
       " 'through',\n",
       " 'besides',\n",
       " 'hers',\n",
       " 'along',\n",
       " 'now',\n",
       " 'toward',\n",
       " 'the',\n",
       " 'interest',\n",
       " 'becoming',\n",
       " 'too',\n",
       " 'himself',\n",
       " 'amount',\n",
       " 'could',\n",
       " 'hereafter',\n",
       " 'although',\n",
       " 'must',\n",
       " 'show',\n",
       " 'against',\n",
       " 'rather',\n",
       " 'upon',\n",
       " 'bottom',\n",
       " 'among',\n",
       " 'been',\n",
       " 'at',\n",
       " 'eg',\n",
       " 'least',\n",
       " 'neither',\n",
       " 'latterly',\n",
       " 'how',\n",
       " 're',\n",
       " 'sixty',\n",
       " 'thereupon',\n",
       " 'i',\n",
       " 'they',\n",
       " 'third',\n",
       " 'towards',\n",
       " 'be',\n",
       " 'from',\n",
       " 'perhaps',\n",
       " 'sometime',\n",
       " 'eight',\n",
       " 'four',\n",
       " 'eleven',\n",
       " 'across',\n",
       " 'otherwise',\n",
       " 'system',\n",
       " 'find',\n",
       " 'more',\n",
       " 'an',\n",
       " 'keep',\n",
       " 'onto',\n",
       " 'therefore',\n",
       " 'under',\n",
       " 'whether',\n",
       " 'con',\n",
       " 'put',\n",
       " 'should',\n",
       " 'above',\n",
       " 'that',\n",
       " 'both',\n",
       " 'detail',\n",
       " 'other',\n",
       " 'everywhere',\n",
       " 'thru',\n",
       " 'or',\n",
       " 'amongst',\n",
       " 'each',\n",
       " 'whom',\n",
       " 'yourselves',\n",
       " 'do',\n",
       " 'ie',\n",
       " 'anyway',\n",
       " 'off',\n",
       " 'them',\n",
       " 'though',\n",
       " 'throughout',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'amoungst',\n",
       " 'hence',\n",
       " 'thus',\n",
       " 'have',\n",
       " 'us',\n",
       " 'fifty',\n",
       " 'even',\n",
       " 'front',\n",
       " 'itself',\n",
       " 'somewhere',\n",
       " 'before',\n",
       " 'beside',\n",
       " 'during',\n",
       " 'indeed',\n",
       " 'thence',\n",
       " 'three',\n",
       " 'nor',\n",
       " 'ltd',\n",
       " 'beyond',\n",
       " 'describe',\n",
       " 'hereupon',\n",
       " 'get',\n",
       " 'afterwards',\n",
       " 'anything',\n",
       " 'whereas',\n",
       " 'very',\n",
       " 'again',\n",
       " 'would',\n",
       " 'serious',\n",
       " 'which',\n",
       " 'over',\n",
       " 'back',\n",
       " 'every',\n",
       " 'since',\n",
       " 'these',\n",
       " 'here',\n",
       " 'twenty',\n",
       " 'to',\n",
       " 'namely',\n",
       " 'no',\n",
       " 'hasnt',\n",
       " 'by',\n",
       " 'see',\n",
       " 'thereby',\n",
       " 'up',\n",
       " 'myself',\n",
       " 'become',\n",
       " 'may',\n",
       " 'never',\n",
       " 'beforehand',\n",
       " 'such',\n",
       " 'was',\n",
       " 'your',\n",
       " 'forty',\n",
       " 'ever',\n",
       " 'he',\n",
       " 'almost',\n",
       " 'until',\n",
       " 'none',\n",
       " 'six',\n",
       " 'de',\n",
       " 'per',\n",
       " 'one',\n",
       " 'my',\n",
       " 'once',\n",
       " 'fill',\n",
       " 'something',\n",
       " 'due',\n",
       " 'ten',\n",
       " 'all',\n",
       " 'done',\n",
       " 'so',\n",
       " 'please',\n",
       " 'in',\n",
       " 'nine',\n",
       " 'couldnt',\n",
       " 'many',\n",
       " 'its',\n",
       " 'ours',\n",
       " 'where',\n",
       " 'yet',\n",
       " 'nobody',\n",
       " 'his',\n",
       " 'whoever',\n",
       " 'about',\n",
       " 'nothing',\n",
       " 'name',\n",
       " 'therein',\n",
       " 'when',\n",
       " 'several',\n",
       " 'wherever',\n",
       " 'mine',\n",
       " 'inc',\n",
       " 'him',\n",
       " 'enough',\n",
       " 'own',\n",
       " 'give',\n",
       " 'whenever',\n",
       " 'often',\n",
       " 'into',\n",
       " 'hereby',\n",
       " 'if',\n",
       " 'found',\n",
       " 'not',\n",
       " 'on',\n",
       " 'thereafter',\n",
       " 'made',\n",
       " 'alone',\n",
       " 'anywhere',\n",
       " 'thin',\n",
       " 'seemed',\n",
       " 'via',\n",
       " 'whose',\n",
       " 'seem',\n",
       " 'sincere',\n",
       " 'were',\n",
       " 'down',\n",
       " 'had',\n",
       " 'her',\n",
       " 'most',\n",
       " 'take',\n",
       " 'whereby',\n",
       " 'two',\n",
       " 'nevertheless',\n",
       " 'much',\n",
       " 'below',\n",
       " 'became',\n",
       " 'anyhow',\n",
       " 'whole',\n",
       " 'being',\n",
       " 'empty',\n",
       " 'aaa',\n",
       " 'asca']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
