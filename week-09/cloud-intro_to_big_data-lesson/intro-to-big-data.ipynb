{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> **Jupyter slideshow:** This notebook can be displayed as slides. To view it as a slideshow in your browser type in the console:\n",
    "\n",
    "\n",
    "> `> ipython nbconvert [this_notebook.ipynb] --to slides --post serve`\n",
    "\n",
    "\n",
    "> To toggle off the slideshow cell formatting, click the `CellToolbar` button, then `View --> Cell Toolbar --> None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intro to Big Data\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/SZOEv2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Student Pre-Work\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Run python scripts from the unix shell\n",
    "- Recall how the `cat` and `sort` unix commands work\n",
    "- [Download VM link here](https://www.dropbox.com/s/egzz6129w90okzf/GA%20DSI%20bigdata%200.9.ova?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Recognize big data problems\n",
    "- Explain how the map reduce algorithm works\n",
    "- Understand the difference between high performance computing and cloud computing\n",
    "- Describe the divide and conquer strategy\n",
    "- Perform a map-reduce on a single node using python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction](#intro)\n",
    "- [What is big data?](#big-data)\n",
    "- [High performance computing (HPC)](#hpc)\n",
    "- [Cloud computing](#cloud)\n",
    "- [Parallelism](#parallelism)\n",
    "- [Divide and conquer](#dc)\n",
    "- [MapReduce](#mapreduce)\n",
    "- [MapReduce: key-value pairs](#kv-pairs)\n",
    "- [Guided practice: word count on paper](#guided-practice)\n",
    "    - [Simple MapReduce](#simple)\n",
    "- [Combiners](#combiners)\n",
    "- [MapReduce in python](#python)\n",
    "    - [`mapper.py`](#mapper)\n",
    "    - [`reducer.py`](#reducer)\n",
    "    - [Running the code in terminal](#terminal)\n",
    "- [Independent practice](#ind-practice)\n",
    "- [Conclusion](#conclusion)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "This lesson identifies some major trends in the field of \"big data\" and data infrastructure, including common tools and problems that you may encounter working as a data scientist. \n",
    "\n",
    "It is time to take the tools you've learned to a new level by scaling up the size of datasets you can tackle!\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/mDzP4d.jpg\" style=\"height: 300px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do you think Big Data is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Big data is a hot topic nowadays. It refers to techniques and tools that allow to store, process and analyze large-scale (multi-terabyte) datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can you think of any datasets that would be \"big data\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Facebook social graph\n",
    "- Netflix movie preferences\n",
    "- Large recommender systems\n",
    "- Activity of visitors to a website\n",
    "- Customer activity in a retail store (ie: Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What challenges exist with such large amounts of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Processing time\n",
    "- Cost\n",
    "- Architecture maintenance and setup\n",
    "- Hard to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"big-data\"></a>\n",
    "## What is \"big data\"?\n",
    "---\n",
    "\n",
    "Big data is a term used for data that exceeds the processing capacity of typical databases. We need a big data analytics team when the data is enormous and growing quickly but we need to uncover hidden patterns, unknown correlations, and build models. \n",
    "\n",
    "**There are three main features in big data (the 3 \"V\"s):**\n",
    "- **Volume**: Large amounts of data\n",
    "- **Variety**: Different types of structured, unstructured, and multi-structured data\n",
    "- **Velocity**: Needs to be analyzed quickly\n",
    "\n",
    "**Dave Yerrington's 4th V (unofficial big data tenet):**\n",
    "- **Value**: It's important to assess the value of predictions to business value.  Understanding the underpinnings of cost vs benefit is even more essential in the context of big data.  It's easy to misundersatnd the 3 V's without looking at the bigger picture, connecting the value of the business cases involved.\n",
    "\n",
    "![3v](./assets/images/3vbigdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='hpc'></a>\n",
    "## High performance computing (HPC)\n",
    "---\n",
    "\n",
    "Supercomputers are very expensive, very powerful calculators used by researchers to solve complicated math problems.\n",
    "\n",
    "![supercomputer](./assets/images/supercomputer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can you think of advantages and disadvantages of HPC configurations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROS:**\n",
    "- can perform very complex calculation\n",
    "- centrally controlled\n",
    "- useful for research and defense complicated math problems\n",
    "\n",
    "**CONS:**\n",
    "- expensive\n",
    "- difficult to maintain (self-manged or managed hosting both incur operations overhead)\n",
    "- scalability is bounded (pre-bigdata era:  this would be medium data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='cloud'></a>\n",
    "## Cloud computing\n",
    "---\n",
    "\n",
    "Instead of one huge machine, what if we bought a bunch of (commodity) machines?\n",
    "\n",
    "> *Note: Comodity hardware is a term we used in operations to describe mixed server hardware but it can also refer to basic machines that you would use in an office setting as well.*\n",
    "\n",
    "![commodity hardware](https://snag.gy/fNYgt0.jpg)<center>*Actual AWS Datacenter*</center>\n",
    "\n",
    "**Can you think of advantages and disadvantages of this configuration?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**PROS:**\n",
    "- Relatively cheaper\n",
    "- Easier to maintain (as a user of the cloud system)\n",
    "- Scalability is unbounded (just add more nodes to the cluster)\n",
    "- Variety of turn-key solutions available through cloud providers\n",
    "\n",
    "**CONS:**\n",
    "- Complex infrastructure \n",
    "- Subject matter expertise required to leverage lower-level resources within infrastructure\n",
    "- Mainly tailored for parallelizable problems\n",
    "- Relatively small cpu power at the lowest level\n",
    "- More I/O between machines\n",
    "\n",
    "The term Big Data refers to the cloud computing case, where commodity hardware with unlimited scalability is used to solve highly parallelizable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do you think many computers process data?\n",
    "\n",
    "**How does this contrast to how you perform analysis on your laptop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='parallelism'></a>\n",
    "## Parallelism\n",
    "---\n",
    "\n",
    "The conceptual foundation of Big Data processing is the idea that a problem can be computed by multiple machines in pieces simultaneously. This is many resources being used in \"parallel\".\n",
    "\n",
    "![](https://snag.gy/MknIN6.jpg)\n",
    "\n",
    "- Running multiple instances to process data\n",
    "- Data can be subset and solved iteratively \n",
    "- Sub-solutions can be solved independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='dc'></a>\n",
    "## Divide and conquer\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/xh2mJA.jpg\">\n",
    "\n",
    "The \"Divide and Conquer\" strategy is a fundamental algorithmic technique for solving a task. The steps are:\n",
    "\n",
    "1. Split the task into subtasks\n",
    "- Solve these subtasks independently\n",
    "- Recombine the subtask results into a final result\n",
    "\n",
    "For a problem to be suitable for the divide and conquer approach it must be able to be broken into smaller independent subtasks. Many processes are suitable for this strategy, but there are plenty that do not meet this criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mapreduce'></a>\n",
    "## MapReduce\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/XBgCOs.jpg\">\n",
    "\n",
    "The term **Map Reduce** indicate a two-phase divide and conquer algorithm initially invented and publicized by Google in 2004. It involves splitting a problem into subtasks and processing these subtasks in parallel. It consists of two phases:\n",
    "\n",
    "1. The **mapper** phase\n",
    "- The **reducer** phase\n",
    "\n",
    "In the **mapper phase**, data is split into chunks and the same computation is performed on each chunk, while in the **reducer phase**, data is aggregated back to produce a final result.\n",
    "\n",
    "Map-reduce uses a functional programming paradigm.  The data processing primitives are mappers and reducers.\n",
    "\n",
    "- **Mappers** – filter & transform data\n",
    "- **Reducers** – aggregate results\n",
    "\n",
    "The functional paradigm is good for describing how to solve a problem, but not very good at describing data manipulations (eg, relational joins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kv-pairs'></a>\n",
    "## MapReduce: key-value pairs\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/k2FCar.jpg\">\n",
    "\n",
    "Data is passed through the various phases of a **map-reduce pipeline** as key-value pairs.\n",
    "\n",
    "**What python data structures could be used to implement a key value pair?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- **Dictionary**\n",
    "- **Tuple** of 2 elements\n",
    "- **List** of 2 elements\n",
    "- Named **tuple**\n",
    "\n",
    "To understand map reduce you need to always keep in mind that data is flowing through a pipeline as key-value pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided practice: word count on paper\n",
    "---\n",
    "\n",
    "Let's perform a simple map-reduce in class. Our task is to find the 10 most common words in the paragraph below.\n",
    "\n",
    "    1:  MapReduce is a programming model for large-scale distributed data processing.\n",
    "    3:  It is inspired by the map function and the reduce function of the functional\n",
    "    4:  programming languages such as Lisp, Haskell, or Python. One of the most\n",
    "    5:  important features of MapReduce is that it allows us to hide the low-level\n",
    "    6:  implementation such as message passing or synchronization from users and\n",
    "    7:  allows to split a problem into many partitions. This is a great way to make\n",
    "    8:  trivial parallelization of data processing without any need for\n",
    "    9:  communication between the partitions.\n",
    "    10: MapReduce became main stream because of Apache Hadoop, which is an open\n",
    "    11: source framework that was derived from Google's MapReduce paper.\n",
    "    12: MapReduce allows us to process massive amounts of data in a distributed\n",
    "    13: cluster. In fact, there are many implementations of the MapReduce\n",
    "    14: programming model. Some of them are shown in the following list. It is\n",
    "    15: important to say that MapReduce is not an algorithm; it is just a part\n",
    "    16: of a high-performance infrastructure that provides a lightweight\n",
    "    17: way to run a program in a lot of parallel machines.\n",
    "    18:                from: Practical Data Analysis, Hector Cuesta, 2013\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='simple'></a>\n",
    "### Simple MapReduce\n",
    "\n",
    "**Instructions:**\n",
    "- Students will perform the mapper function.\n",
    "- Instructor will perform the reducer function.\n",
    "\n",
    "Each student will be assigned 1 line of text. You have to produce a list of key value pairs `(word, 1)` and hand those to the instructor. \n",
    "\n",
    "**Check:** what pre-processing should you do your tokens in order to improve the results?\n",
    "\n",
    "Example: the first line will produce this list:\n",
    "\n",
    "    (MapReduce, 1)\n",
    "    (is, 1)\n",
    "    (a, 1)\n",
    "    (programming, 1)\n",
    "    (model, 1)\n",
    "    (for, 1)\n",
    "    (large-scale, 1)\n",
    "    (distributed, 1)\n",
    "    (data, 1)\n",
    "    (processing, 1)\n",
    "\n",
    "The instructor will then sort them, add up the `1`s for each word and produce the counts.\n",
    "\n",
    "**Check:** what additional operation did the instructor perform in order to complete the aggregation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> *Answer: Ignore punctuation and transform all to lower-case.*\n",
    "\n",
    "---\n",
    "\n",
    "> *Instructor notes:*\n",
    "*1. if there are more than 18 students, group the the students to obtain 18 groups.*\n",
    "*- if there are less than 18 students, give each student more than 1 line, so that all lines get processed.*\n",
    "*- make sure that they hand a list of key-value pairs where the key is the word and the value is 1.*\n",
    "*- no need to actually do the count, here are the expected results:*\n",
    ">\n",
    ">        ('of', 10)\n",
    ">        ('a', 9)\n",
    ">        ('is', 8)\n",
    ">        ('the', 8)\n",
    ">        ('mapreduce', 7)\n",
    ">        ('to', 6)\n",
    ">        ('that', 4)\n",
    ">        ('it', 4)\n",
    ">        ('in', 4)\n",
    ">        ('data', 4)\n",
    "\n",
    "---\n",
    "\n",
    "> *Instructor had to shuffle the k-v pairs handed by the students in order to find common key and add up the corresponding values.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='combiners'></a>\n",
    "## Combiners\n",
    "---\n",
    "\n",
    "Combiners are intermediate reducers that are performed at node level in a multi-node architecture.\n",
    "\n",
    "![](https://snag.gy/lFYfoC.jpg)\n",
    "\n",
    "When data is really large we can distribute it to several mappers running on different machines. Sending a long list of `(word, 1)` pairs to the reducer node is not efficient. We can first aggregate at mapper node level and send the result of the aggregation to the reducer. This is possible because aggregations are associative.\n",
    "\n",
    "**Let's repeat the exercise we did before, with a small change:**\n",
    "1.Let's divide the class in 3 groups, in each group one student will be the combiner, the others will be mappers.\n",
    "- Let's split the text in 3 parts and each group gets one part\n",
    "- Mapper students produce the same list of `(word, 1)` for each line they receive and hand the list to the combiner\n",
    "- Combiner students sort the lists and sum the counts for words that appear in each list\n",
    "- Finally combiner students hand their list of counts to the instructor who will combine the intermediate sums and produce the final result\n",
    "\n",
    "**Check:** What changed?\n",
    "\n",
    "Congratulations! you have just performed a map-reduce sum.\n",
    "\n",
    "**Check:** Can you think of other aggregation tasks that can be parallelized in this way?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> *Less message passing to the instructor*\n",
    "\n",
    "---\n",
    "\n",
    "> *Answer:*\n",
    "*- count, sum, average*\n",
    "*- grep, sort, inverted index*\n",
    "*- graph traversals, some ML algorithms*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"python\"></a>\n",
    "## MapReduce in python\n",
    "---\n",
    "\n",
    "Now that we performed map-reduce in person, let's do it in python. Below you can find the code for a simple mapper and reducer that calculate the word count.\n",
    "\n",
    "Let's look at them in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mapper'></a>\n",
    "### `mapper.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# mapper.py\n",
    "import sys\n",
    "\n",
    "# get text from standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** What kind of input does `mapper.py` expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**AND** what kind of output does `mapper.py` produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='reducer'></a>\n",
    "### `reducer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\t0\n"
     ]
    }
   ],
   "source": [
    "# reducer.py\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "    \n",
    "    # try to count, if error continue\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** what kind of input does `reducer.py` expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** what kind of output does `reducer.py` produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='terminal'></a>\n",
    "### Running the code in terminal\n",
    "\n",
    "**You can find `mapper.py`, `reducer.py` and some text input files in the `code` directory.**\n",
    "\n",
    "The code can be run with the following command from terminal:\n",
    "\n",
    "```bash\n",
    "cat <input-file> | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** can you explain what each of the 4 steps in the pipe does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- cat: read the file and streams it line by line\n",
    "- mapper\n",
    "- sort: shuffles the mapper output to sort it by key so that counting is easier\n",
    "- reducer: aggregates by word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** can you find how our previous example *could* be represented in the diagram below?\n",
    "![map reduce word count](./assets/images/word_count_dataflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent practice\n",
    "---\n",
    "\n",
    "Now that you have a basic word counter set up in python, try doing some of the following:\n",
    "\n",
    "1. Process a much larger text file (find one of your choice on the internet)\n",
    "    - For example,  a page from wikipedia or a blog article. If you're really ambitious you can take books from project gutemberg.\n",
    "- Try to see how the execution time scales with file size.\n",
    "- Read [this article](http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html) for some very powerful shell tricks.  Learning to use the shell will save you tons of time munging data on your filesystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion\n",
    "---\n",
    "\n",
    "We have learned about Big Data and the MapReduce process. MapReduce is an algorithm that works really well for aggregations on very large datasets.\n",
    "\n",
    "**Check:** now that you know how it works can you think of some more specific business applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Examples:**\n",
    "\n",
    "- process log files to find security breaches\n",
    "- process medical records to assess spending\n",
    "- process news articles to decide on investments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "### Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Top 500 Supercomputers](http://www.top500.org/lists/)\n",
    "- [Google Map Reduce paper](http://research.google.com/archive/mapreduce.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
