{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab 1\n",
    "\n",
    "In this lab, we will use Spark to further dig into the Bay Area Bike Share data.\n",
    "\n",
    "### Environment Option 1 - BigData VM\n",
    "\n",
    "You will need to run this lab on the VM provided. So, as usual, connect to your VM using\n",
    "\n",
    "    vagrant up\n",
    "    vagrant ssh\n",
    "\n",
    "And then, once inside, run:\n",
    "\n",
    "    spark_local_start.sh\n",
    "\n",
    "**Important:** If your machine is already running and you've started the Hadoop services with `bigdata_start.sh`, you may want to first run `bigdata_stop.sh` to stop all services and free some memory space.\n",
    "\n",
    "Once you've started spark in local mode, you should be able to access Jupyter at this address:\n",
    "\n",
    "http://10.211.55.101:18888\n",
    "\n",
    "We will work in there.\n",
    "\n",
    "\n",
    "### Environment Option 2 - Local Spark Environment\n",
    "\n",
    "In order to work on this lab with your local machine, just start your local jupyter notebook with spark using our previous configuration using the following command alias:\n",
    "\n",
    "```bash\n",
    "jupyter-spark\n",
    "```\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "\n",
    "Our goal is to calculate the average number of trips per hour, using the Caltrain Station as starting point.\n",
    "\n",
    "Check that your spark context is available:\n",
    "\n",
    "> If you are using your BigData VM, create a new notebook on the instance running at http://10.211.55.101:18888, using the content in this notebook as a reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Bay Bay Area Bike Share trip data:\n",
    "\n",
    "Data has been pre-loaded on your BigData VM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = sc.textFile('file:///home/vagrant/data/201408_babs_open_data/201408_trip_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If want to use your local Spark instance (not in the BigData VM), you should be able to setup your trips using the local dataset from the \"bay_area_bike_share\" directory.  The dataset is too large to fit on Github uncompressed so it is in zip format.  You will need to unarchive it and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_file = \"../../../../../datasets/bay_area_bike_share/201408_babs_open_data/201408_trip_data.csv\"\n",
    "trips = sc.textFile(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check:** What kind of object is `trips`?\n",
    "\n",
    "### Exercise 1: split csv lines\n",
    "In spark, we can build complex pipelines that only get executed when we ask to collect them.\n",
    "\n",
    "Remember how we built pipelines in scikit learn as a composition of transformations?\n",
    "\n",
    "The process here is very similar, with one big difference:\n",
    "\n",
    "While in a python pipeline the calculation is immediately executed, with spark the pipeline definition and execution are separate steps.\n",
    "\n",
    "In other words, we can define the pipeline with all its steps, and only when we call `collect` will the data flow through it. In order to get familiar with this new workflow, we will start with small steps to build our pipeline.\n",
    "\n",
    "First step:\n",
    "- apply a map to trips that splits each line at commas and save that to a an RDD\n",
    "\n",
    "**Hint:** if you want to check that you're doing things right, you can collect the result and display the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: filter for Caltrain station\n",
    "In Spark we can also create filters using the `filter` method.\n",
    "Let's select station number 70 by filtering on the 5th column, we will do all the following analysis just on this station, which corresponds to the most popular starting point. Save this to a variable called `station_70`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: trips by day - hour (mapper)\n",
    "Let's analyse the trips by the hour. We can do this by performing a map reduce job in Spark. First we will need to emit tuples with a count of 1 for each (date, hour) key, and then we will sum the counts by key.\n",
    "\n",
    "- Emit tuple of ((date, hour), 1), applying a map to `station_70` that extracts the relevant data from each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Emit tuple of ((date, hour), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:  trips by day - hour (reducer)\n",
    "\n",
    "Use the `reduceByKey` method to obtain the number of trips per (day, hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: trips by hour (mapper)\n",
    "Let's further group the trips by hour. We'll do this with a second Map Reduce job.\n",
    "First we will discard the day and emit tuples of (hour, count). You can achieve this with a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Emit tuple of (hour, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: trips by hour (reducer)\n",
    "Then let's calculate the average number of trips by hour using the `combineByKey` method.\n",
    "\n",
    "You can find a suggestion on how to do it [here](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: collect\n",
    "We can finally collect our result and sort them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "\n",
    "Besides the SparkContext, Spark also exposes a sqlContext that allows us to perform SQL queries on an RDD object.\n",
    "\n",
    "A SQLContext is also already created for you. Do not create another or unspecified behavior may occur. As you can see below, the sqlContext provided is a HiveContext.\n",
    "\n",
    "- Run the same query we performed in Hue/Hive to obtain the average duration of a trip originating from the Caltrain station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tripsSql = sqlContext.read.format('com.databricks.spark.csv').options(header='true',\n",
    "                                                                      inferschema='true').load('file:///home/vagrant/data/201408_babs_open_data/201408_trip_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tripsSql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register this DataFrame as a table.\n",
    "tripsSql.registerTempTable(\"tripsSql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"\n",
    "YOUR QUERY HERE\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
